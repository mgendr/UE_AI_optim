{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Pruning_VGG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OfNv1m3ue4wo",
        "vGjf5LBPhDMB",
        "ZB8o7ZPobSV4",
        "5AIX6fTgL1Ud",
        "XPxManFoh6ra",
        "gu_2JMMS3aB4",
        "xDZHW-ByKKny",
        "RYpN6jtqoT4U"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "342a9aa46b5d4b28b8d854905106b064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4a548902626449da66db37d911d8bbc",
              "IPY_MODEL_86e9bdcbeafe4c50b3ea2294e1e164bb"
            ],
            "layout": "IPY_MODEL_70847251a1064669a8b055c9c753004f"
          }
        },
        "70847251a1064669a8b055c9c753004f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4a548902626449da66db37d911d8bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10f8402e94b74f0eba98448690bc2776",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5271d07c272342b88ea3bd629f26dc73",
            "value": 1
          }
        },
        "86e9bdcbeafe4c50b3ea2294e1e164bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_676faee2a1fe4fb087592d41460bfe15",
            "placeholder": "​",
            "style": "IPY_MODEL_bd96032235d046ef80bf5e86231b56bc",
            "value": " 170500096/? [00:20&lt;00:00, 79446734.25it/s]"
          }
        },
        "5271d07c272342b88ea3bd629f26dc73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "10f8402e94b74f0eba98448690bc2776": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd96032235d046ef80bf5e86231b56bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "676faee2a1fe4fb087592d41460bfe15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8uFjpz6edso"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJSqvb9cejk_"
      },
      "source": [
        "**Via ce projet nous avons cherchés à élaguer au maximum le réseaux de neurones VGG afin de comprendre à quel point nous pouvions le compresser. Ce notebook a été dévellopé via google colab, d'où des échanges upload/download que vous pourrez voir.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdv4sb6Q_1jy"
      },
      "source": [
        "# Fonctions de traitement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfNv1m3ue4wo"
      },
      "source": [
        "## Création des datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "342a9aa46b5d4b28b8d854905106b064",
            "70847251a1064669a8b055c9c753004f",
            "b4a548902626449da66db37d911d8bbc",
            "86e9bdcbeafe4c50b3ea2294e1e164bb",
            "5271d07c272342b88ea3bd629f26dc73",
            "10f8402e94b74f0eba98448690bc2776",
            "bd96032235d046ef80bf5e86231b56bc",
            "676faee2a1fe4fb087592d41460bfe15"
          ]
        },
        "id": "mxi4pzdafJ-A",
        "outputId": "1bebad5c-5d95-4548-8037-e86fdce16f2e"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        " \n",
        "# This script generates the MINICIFAR dataset from CIFAR10\n",
        "# The following parameters can be changed : \n",
        "# n_classes (between 2 and 10) \n",
        "# Reduction factor R (which will result in 10000 /  R examples per class for the train set, and 1000 / R per class for test)\n",
        "# --\n",
        " \n",
        " \n",
        "n_classes_minicifar = 4\n",
        "train_size = 0.8\n",
        "R = 5\n",
        " \n",
        " \n",
        "# Download the entire CIFAR10 dataset\n",
        " \n",
        "from torchvision.datasets import CIFAR10\n",
        "import numpy as np \n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        " \n",
        " \n",
        "import torchvision.transforms as transforms\n",
        " \n",
        "## Normalization is different when training from scratch and when training using an imagenet pretrained backbone\n",
        " \n",
        "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        " \n",
        "normalize_forimagenet = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        " \n",
        "# Data augmentation is needed in order to train from scratch\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize_scratch,\n",
        "])\n",
        " \n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    normalize_scratch,\n",
        "])\n",
        " \n",
        "## No data augmentation when using Transfer Learning \n",
        "## however resize to Imagenet input dimensions is recommended for Transfer learning\n",
        "transform_train_imagenet = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize_forimagenet,\n",
        "])\n",
        " \n",
        "transform_test_imagenet = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "    normalize_forimagenet,\n",
        "])\n",
        " \n",
        " \n",
        "### The data from CIFAR10 will be downloaded in the following dataset\n",
        "rootdir = './data/cifar10'\n",
        " \n",
        "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
        "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
        "\n",
        "\n",
        " \n",
        "c10train_imagenet = CIFAR10(rootdir,train=True,download=True,transform=transform_train_imagenet)\n",
        "c10test_imagenet = CIFAR10(rootdir,train=False,download=True,transform=transform_test_imagenet)\n",
        " \n",
        "# Generating Mini-CIFAR\n",
        "# \n",
        "# CIFAR10 is sufficiently large so that training a model up to the state of the art performance will take approximately 3 hours on the 1060 GPU available on your machine. \n",
        "# As a result, we will create a \"MiniCifar\" dataset, based on CIFAR10, with less classes and exemples. \n",
        " \n",
        "def train_validation_split(train_size, num_train_examples):\n",
        "    # obtain training indices that will be used for validation\n",
        "    indices = list(range(num_train_examples))\n",
        "    np.random.shuffle(indices)\n",
        "    idx_split = int(np.floor(train_size * num_train_examples))\n",
        "    train_index, valid_index = indices[:idx_split], indices[idx_split:]\n",
        " \n",
        "    # define samplers for obtaining training and validation batches\n",
        "    train_sampler = SubsetRandomSampler(train_index)\n",
        "    valid_sampler = SubsetRandomSampler(valid_index)\n",
        " \n",
        "    return train_sampler,valid_sampler\n",
        " \n",
        "def generate_subset(dataset,n_classes,reducefactor,n_ex_class_init):\n",
        " \n",
        "    nb_examples_per_class = int(np.floor(n_ex_class_init / reducefactor))\n",
        "    # Generate the indices. They are the same for each class, could easily be modified to have different ones. But be careful to keep the random seed! \n",
        " \n",
        "    indices_split = np.random.RandomState(seed=42).choice(n_ex_class_init,nb_examples_per_class,replace=False)\n",
        " \n",
        " \n",
        "    all_indices = []\n",
        "    for curclas in range(n_classes):\n",
        "        curtargets = np.where(np.array(dataset.targets) == curclas)\n",
        "        indices_curclas = curtargets[0]\n",
        "        indices_subset = indices_curclas[indices_split]\n",
        "        #print(len(indices_subset))\n",
        "        all_indices.append(indices_subset)\n",
        "    all_indices = np.hstack(all_indices)\n",
        "    \n",
        "    return Subset(dataset,indices=all_indices)\n",
        "    \n",
        " \n",
        " \n",
        "### These dataloader are ready to be used to train for scratch \n",
        "minicifar_train= generate_subset(dataset=c10train,n_classes=n_classes_minicifar,reducefactor=R,n_ex_class_init=5000)\n",
        "num_train_examples=len(minicifar_train)\n",
        "train_sampler,valid_sampler=train_validation_split(train_size, num_train_examples)\n",
        "minicifar_test = generate_subset(dataset=c10test,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000) \n",
        " \n",
        " \n",
        "### These dataloader are ready to be used to train using Transfer Learning \n",
        "### from a backbone pretrained on ImageNet\n",
        "minicifar_train_im= generate_subset(dataset=c10train_imagenet,n_classes=n_classes_minicifar,reducefactor=R,n_ex_class_init=5000)\n",
        "num_train_examples_im=len(minicifar_train_im)\n",
        "train_sampler_im,valid_sampler_im=train_validation_split(train_size, num_train_examples_im)\n",
        "minicifar_test_im= generate_subset(dataset=c10test_imagenet,n_classes=n_classes_minicifar,reducefactor=1,n_ex_class_init=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFGyiqljfLHZ"
      },
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "c10train_sampler,c10valid_sampler=train_validation_split(0.8, len(c10train))\n",
        "\n",
        "c10trainloader = DataLoader(c10train,batch_size=32,sampler=c10train_sampler)\n",
        "c10validloader = DataLoader(c10train,batch_size=32,sampler=c10valid_sampler)\n",
        "c10testloader = DataLoader(c10test,batch_size=32)\n",
        "#from minicifar import minicifar_train,minicifar_test,train_sampler,valid_sampler\n",
        "\n",
        "\n",
        "trainloader = DataLoader(minicifar_train,batch_size=32,sampler=train_sampler)\n",
        "validloader = DataLoader(minicifar_train,batch_size=32,sampler=valid_sampler)\n",
        "testloader = DataLoader(minicifar_test,batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGjf5LBPhDMB"
      },
      "source": [
        "## Architectures de VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3mUvZw5GEKu"
      },
      "source": [
        "'''VGG11/13/16/19 in Pytorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = VGG('VGG11')\n",
        "    x = torch.randn(2,3,32,32)\n",
        "    y = net(x)\n",
        "    print(y.size())\n",
        "\n",
        "# test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olyK1dDLfLEb"
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N85Ik55lfLK6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB8o7ZPobSV4"
      },
      "source": [
        "## Fonctions de visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDhvuWIbgR_0"
      },
      "source": [
        "from torch import nn, optim\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from google.colab import files\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtPUmqYLBYd9"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss(test_loss,training_loss,n_epochs) : \n",
        "  plt.figure()\n",
        "  test_ax=np.arange(0,n_epochs,n_epochs/len(test_loss))\n",
        "  train_ax=np.arange(0,n_epochs,n_epochs/len(training_loss))\n",
        "\n",
        "  plt.plot(test_ax,test_loss,\"r\")\n",
        "  plt.plot(train_ax,training_loss)\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('loss') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LcCe3hz3D-q"
      },
      "source": [
        "def plot_log_loss(test_loss,training_loss,n_epochs) : \n",
        "  plt.figure()\n",
        "  test_ax=np.arange(0,n_epochs,n_epochs/len(test_loss))\n",
        "  train_ax=np.arange(0,n_epochs,n_epochs/len(training_loss))\n",
        "\n",
        "  plt.plot(test_ax,np.log(np.array(test_loss)),\"r\")\n",
        "  plt.plot(train_ax,np.log(np.array(training_loss)))\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('log(loss)')   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR-moKlfBYg7"
      },
      "source": [
        "\n",
        "def print_accuracy(net,testloader ) : \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():  # torch.no_grad for TESTING\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            if data_int :\n",
        "              images=images.half()\n",
        "            images=images.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the test images: %d %%' % (\n",
        "        100 * correct / total))\n",
        "    \n",
        "\n",
        "    class_correct = list(0. for i in range(4))\n",
        "    class_total = list(0. for i in range(4))\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            if data_int :\n",
        "              images=images.half()\n",
        "            images=images.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            c = (predicted == labels.to(device)).squeeze()\n",
        "            for i in range(4):\n",
        "                label = labels[i]\n",
        "                class_correct[label] += c[i].item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "    classes = ('1', '2', '3', '4')\n",
        "    for i in range(4):\n",
        "        print('Accuracy of %5s : %2d %%' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnmt-m_6X0LB"
      },
      "source": [
        "def eval_accuracy(net,testloader ) : \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():  # torch.no_grad for TESTING\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            if data_int :\n",
        "              images=images.half()\n",
        "            images=images.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "\n",
        "    return 100*correct/total\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIOU9gUhY1iA"
      },
      "source": [
        "def plot_accuracy(accuracy,n_epochs):\n",
        "    plt.figure()\n",
        "    test_ax=np.arange(0,n_epochs,n_epochs/len(accuracy))\n",
        "    plt.plot(test_ax,accuracy,\"r\")\n",
        "    plt.ylabel('acurracy %')\n",
        "    plt.xlabel('epoch') \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU--l6UVL1Ko"
      },
      "source": [
        "## Pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AIX6fTgL1Ud"
      },
      "source": [
        "### Fonctions de Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmAhfLUmM-ce"
      },
      "source": [
        "\n",
        "\n",
        "def trainingwithPrunning(trainloader,validloader,testloader,n_epochs,criterion,optimizer,mymodel,valid_losses=[],training_losses=[],test_accuracys=[],scheduler=True) : \n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "      train_loss,valid_loss=0,0\n",
        "      \n",
        "      print(\"epoch \",epoch)\n",
        "\n",
        "      running_loss = 0.0\n",
        "      mymodel.model.train()\n",
        "\n",
        "      for data,label in trainloader:\n",
        "          \n",
        "          if data_int :\n",
        "              data=data.half()\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "\n",
        "          outputs = mymodel.forward(data.to(device))\n",
        "\n",
        "          loss = criterion(outputs, label.to(device))\n",
        "\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        " \n",
        "          # print statistics\n",
        "          train_loss += loss.item()*data.size(0)\n",
        "          \n",
        "      mymodel.model.eval()\n",
        "      for data,label in validloader:\n",
        "          if data_int :\n",
        "              data=data.half()\n",
        "          \n",
        "          outputs = mymodel.forward(data.to(device))\n",
        "\n",
        "          loss = criterion(outputs, label.to(device))\n",
        "\n",
        "          # print statistics\n",
        "          valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "      train_loss /= len(trainloader.sampler)\n",
        "      valid_loss /= len(validloader.sampler)\n",
        "      training_losses.append(train_loss)\n",
        "      if scheduler :\n",
        "        lr_scheduler.step(valid_loss)\n",
        "\n",
        "      #lr_scheduler.step(running_loss/i)        \n",
        "      valid_losses.append(valid_loss)\n",
        "      \n",
        "\n",
        "      #if running_loss/i==min(test_loss) :\n",
        "      print(\"saving weights.... \")\n",
        "\n",
        "      torch.save(mymodel.model.state_dict(), 'checkpoint_bin.pt')\n",
        "      \n",
        "      test_accuracy=eval_accuracy(mymodel.model,testloader )\n",
        "\n",
        "      print(test_accuracy,\" % , \",valid_losses[-1],\" , \",training_losses[-1])\n",
        "      test_accuracys.append(test_accuracy)\n",
        "\n",
        "  #print(training_losses)\n",
        "  print('Finished Training')\n",
        "  return valid_losses,training_losses, test_accuracys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovx2TCqBz4BI"
      },
      "source": [
        "def get_number_param_pruned(model):\n",
        "  total_conv=0\n",
        "  total_fc=0\n",
        "\n",
        "  for name, module in model.named_modules():\n",
        "    \n",
        "\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "\n",
        "      l=list(module.named_buffers())\n",
        "\n",
        "      total_conv+=torch.sum(l[0][1].type(torch.DoubleTensor)).item()\n",
        "      #print(len(l[0][1]))\n",
        "       \n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "\n",
        "      l=list(module.named_buffers())\n",
        "      #print(torch.sum(l[0][1]).item())\n",
        "\n",
        "      total_fc+=torch.sum(l[0][1]).item()\n",
        "\n",
        "  pp=0\n",
        "  for p in list(model.parameters()):\n",
        "      nn=1\n",
        "      for s in list(p.size()):\n",
        "          nn = nn*s\n",
        "      pp += nn\n",
        "  #print(\"params_initiaux : \",pp)\n",
        "  #print(\"nouveaux conv : \",total_conv)\n",
        "  #print(\"nouveaux fc : \",total_fc)\n",
        "  return total_conv,total_fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PXHFEYfHGTz"
      },
      "source": [
        "import torch.nn as nn\n",
        "import numpy\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "\n",
        "class my_network_with_trous():## Classe permettant de pruner un réseaux de neurones facilement\n",
        "    def __init__(self, model):\n",
        "\n",
        "\n",
        "        self.model = model # this contains the model that will be trained and quantified\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        ### This function is used so that the model can be used while training\n",
        "        out = self.model(x)\n",
        "        return out\n",
        "\n",
        "    def prune_all_layers(self,ratio):\n",
        "        #self.save_params()\n",
        "        #for index in range(self.num_of_params):\n",
        "        #    prune.random_unstructured(self.target_modules[index], name=\"weight\", amount=ratio)\n",
        "        print(\"Pruning....\")\n",
        "        \n",
        "        for name, module in self.model.named_modules():\n",
        "            \n",
        "              if isinstance(module, torch.nn.Conv2d):\n",
        "                  prune.ln_structured(module, name='weight', n=2,dim=ratio[\"dim\"],amount=ratio[\"conv\"])\n",
        "                  #prune.remove(module,\"weight\")\n",
        "    \n",
        "              elif isinstance(module, torch.nn.Linear):\n",
        "                  prune.ln_structured(module, name='weight',n=2,dim=1, amount=ratio[\"fc\"])\n",
        "                  #prune.remove(module,\"weight\")\n",
        "\n",
        "        #print(dict(self.model.named_buffers()).keys())  # to verify that all masks exist\n",
        "\n",
        "    def prune_spef_layer(self,name_layer,ratio,dim):\n",
        "        for name, module in self.model.named_modules():\n",
        "              \n",
        "              #print(list(module.named_buffers()))\n",
        "              if str(name)==name_layer :\n",
        "\n",
        "                  print(name)\n",
        "                  prune.ln_structured(module, name='weight', n=2,dim=dim,amount=ratio)\n",
        "\n",
        "                  #print(list(module.named_buffers()))\n",
        "    \n",
        "              #elif isinstance(module, torch.nn.Linear):\n",
        "                  #print(list(module.named_buffers()))\n",
        "              if str(name_layer)==\"a\" and isinstance(module, torch.nn.Conv2d):\n",
        "                  print(name)\n",
        "              elif str(name_layer)==\"a\" and isinstance(module, torch.nn.Linear):\n",
        "                  print(name)\n",
        "      \n",
        "                  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBK3D4Zwo6o8"
      },
      "source": [
        "## La fonction profile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oPhLTI95yCa"
      },
      "source": [
        "**Cette fonction nous a été donnée afin de mesurer la compression de notre réseau.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AVP93EBo5tL"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def count_conv2d(m, x, y):\n",
        "    x = x[0] # remove tuple\n",
        "    fin = m.in_channels\n",
        "    fout = m.out_channels\n",
        "    sh, sw = m.kernel_size\n",
        "    total_params=0\n",
        "    for p in m.parameters():\n",
        "        total_params += torch.Tensor([p.numel()]) / 2  # Division Free quantification\n",
        "\n",
        "    # ops per output element\n",
        "    kernel_mul = sh * sw * fin\n",
        "    kernel_add = sh * sw * fin - 1\n",
        "    bias_ops = 1 if m.bias is not None else 0\n",
        "    kernel_mul = kernel_mul/2 # FP16\n",
        "    ops = kernel_mul + kernel_add + bias_ops\n",
        "    #print(len(y))\n",
        "    # total ops\n",
        "    taux=int(m.total_params.item())/total_params\n",
        "    print(taux)\n",
        "    num_out_elements = y.numel()*taux\n",
        "    total_ops = num_out_elements * ops\n",
        "\n",
        "    print(\"Conv2d: S_c={}, F_in={}, F_out={}, P={}, params={}, operations={}\".format(sh,fin,fout,x.size()[2:].numel(),int(m.total_params.item()),int(total_ops)))\n",
        "    # incase same conv is used multiple times\n",
        "    m.total_ops += torch.Tensor([int(total_ops)])\n",
        "\n",
        "def count_linear(m, x, y):\n",
        "    # per output element\n",
        "    print(m.in_features)\n",
        "    total_mul = m.in_features/2\n",
        "    total_add = m.in_features - 1\n",
        "\n",
        "    total_params=0\n",
        "    for p in m.parameters():\n",
        "        total_params += torch.Tensor([p.numel()]) #/ 2  # Division Free quantification\n",
        "\n",
        "    taux=int(m.total_params.item())/total_params\n",
        "\n",
        "\n",
        "    num_elements = y.numel()\n",
        "    total_ops = (total_mul + total_add) * num_elements*taux\n",
        "    print(\"Linear: F_in={}, F_out={}, params={}, operations={}\".format(m.in_features,m.out_features,int(m.total_params.item()),int(total_ops)))\n",
        "    m.total_ops += torch.Tensor([int(total_ops)])\n",
        "\n",
        "\n",
        "def count_bn2d(m, x, y):\n",
        "    x = x[0] # remove tuple\n",
        "\n",
        "    nelements = x.numel()\n",
        "    total_sub = 2*nelements\n",
        "    total_div = nelements\n",
        "    total_ops = total_sub + total_div\n",
        "\n",
        "    m.total_ops += torch.Tensor([int(total_ops)])\n",
        "    print(\"Batch norm: F_in={} P={}, params={}, operations={}\".format(x.size(1),x.size()[2:].numel(),int(m.total_params.item()),int(total_ops)))\n",
        "\n",
        "\n",
        "def count_relu(m, x, y):\n",
        "    x = x[0]\n",
        "\n",
        "    nelements = x.numel()\n",
        "    total_ops = nelements\n",
        "\n",
        "    m.total_ops += torch.Tensor([int(total_ops)])\n",
        "    print(\"ReLU: F_in={} P={}, params={}, operations={}\".format(x.size(1),x.size()[2:].numel(),0,int(total_ops)))\n",
        "\n",
        "\n",
        "\n",
        "def count_avgpool(m, x, y):\n",
        "    x = x[0]\n",
        "    total_add = torch.prod(torch.Tensor([m.kernel_size])) - 1\n",
        "    total_div = 1\n",
        "    kernel_ops = total_add + total_div\n",
        "    num_elements = y.numel()\n",
        "    total_ops = kernel_ops * num_elements\n",
        "\n",
        "    m.total_ops += torch.Tensor([int(total_ops)])\n",
        "    print(\"AvgPool: S={}, F_in={}, P={}, params={}, operations={}\".format(m.kernel_size,x.size(1),x.size()[2:].numel(),0,int(total_ops)))\n",
        "\n",
        "def count_sequential(m, x, y):\n",
        "    print (\"Sequential: No additional parameters  / op\")\n",
        "\n",
        "# custom ops could be used to pass variable customized ratios for quantization\n",
        "def profile(model, input_size, custom_ops = {}):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    def add_hooks(m):\n",
        "        if len(list(m.children())) > 0: return\n",
        "        m.register_buffer('total_ops', torch.zeros(1))\n",
        "        m.register_buffer('total_params', torch.zeros(1))\n",
        "\n",
        "        for p in m.parameters():\n",
        "            if isinstance(m, torch.nn.Conv2d):\n",
        "                m.total_params += (torch.count_nonzero(torch.cat([param.view(-1) for param in m.parameters()]).detach()))/ 2 # Division Free quantification\n",
        "            elif isinstance(m, torch.nn.Linear):\n",
        "                m.total_params += (torch.count_nonzero(torch.cat([param.view(-1) for param in m.parameters()]).detach()))/ 2 # Division Free quantification\n",
        "            else:\n",
        "                m.total_params += torch.Tensor([p.numel()]) / 2 # Division Free quantification\n",
        "\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            m.register_forward_hook(count_conv2d)\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            m.register_forward_hook(count_bn2d)\n",
        "        elif isinstance(m, nn.ReLU):\n",
        "            m.register_forward_hook(count_relu)\n",
        "        elif isinstance(m, (nn.AvgPool2d)):\n",
        "            m.register_forward_hook(count_avgpool)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            m.register_forward_hook(count_linear)\n",
        "        elif isinstance(m, nn.Sequential):\n",
        "            m.register_forward_hook(count_sequential)\n",
        "        else:\n",
        "            print(\"Not implemented for \", m)\n",
        "\n",
        "    model.apply(add_hooks)\n",
        "\n",
        "    x = torch.zeros(input_size)\n",
        "    if data_int :\n",
        "        x=x.half()\n",
        "    model(x)\n",
        "\n",
        "    total_ops = 0\n",
        "    total_params = 0\n",
        "    for m in model.modules():\n",
        "        if len(list(m.children())) > 0: continue\n",
        "        total_ops += m.total_ops\n",
        "        total_params += m.total_params\n",
        "    return total_ops, total_params\n",
        "\n",
        "def mesure_operation(model,ref=\"VGG16\"):\n",
        "    # Resnet18 - Reference for CIFAR 10\n",
        "    \n",
        "     # Resnet18 - Reference for CIFAR 10\n",
        "\n",
        "    ref_params = 5586981\n",
        "    ref_flops  = 834362880\n",
        "\n",
        "    if ref==\"mobileNet\" :\n",
        "      ref_params = 1118885.0\n",
        "      ref_flops = 9299442688.0\n",
        "\n",
        "    elif ref==\"VGG16\" : \n",
        "      ref_params = 14724042.0\n",
        "      ref_flops  = 940703744.0\n",
        "\n",
        "    #Flops: 470908896.0, Params: 14724041.0\n",
        "    model = model.to(\"cpu\")\n",
        "    # WideResnet-28-10 - Reference for CIFAR 100 \n",
        "    # ref_params = 36500000\n",
        "    # ref_flops  = 10490000000\n",
        "\n",
        "    #model = resnet.ResNet18()\n",
        "    #print(model)\n",
        "    flops, params = profile(model, (1,3,32,32))\n",
        "    flops, params = flops.item(), params.item()\n",
        "\n",
        "    score_flops = flops / ref_flops\n",
        "    score_params = params / ref_params\n",
        "    score = score_flops + score_params\n",
        "    print(\"Flops: {}, Params: {}\".format(flops,params))\n",
        "    print(\"Score flops: {} Score Params: {}\".format(score_flops,score_params))\n",
        "    print(\"Final score: {}\".format(score))\n",
        "    model = model.to(device)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu_2JMMS3aB4"
      },
      "source": [
        "# Etude sur un réseaux quantifié"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW0e7jPZlYS9"
      },
      "source": [
        "**Ces données ont été obtenus via une étude précédente. Nous avons comparé des réseaux 32 bits et 34 bits avec différents taux d'élagages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "fEGpV1oR3ZHs",
        "outputId": "64701e11-3423-48b2-b57b-e39be8d4944d"
      },
      "source": [
        "a= [9.217728,6.445503,4.297002,3.005190]\n",
        "b=[85,85,67,36]\n",
        "c=[9.217728,6.445503,4.297002,3.005190]\n",
        "d=[84,83,51,36]\n",
        "\n",
        "fig=plt.scatter(a,b)\n",
        "\n",
        "plt.scatter(c,d)\n",
        "plt.legend([\"Float32\",\"Float16\"])\n",
        "plt.xlabel(\"millions of parameters\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agEytS9JAze8"
      },
      "source": [
        "# Minicifar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPxManFoh6ra"
      },
      "source": [
        "## Première expérience d'élagage sur minicifar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxTs1BPZDxVX"
      },
      "source": [
        "data_int=False### cette constante permet de définir si oui ou non on fait de la quantification sur nos poids : \n",
        "#True : 32 bits ou False : 64 bits pour chaque poids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTwmYq_CmvUP",
        "outputId": "e62a38d2-69af-4b8a-a7f5-2f87b7bba369"
      },
      "source": [
        "net = VGG('VGG16')\n",
        "net"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (12): ReLU(inplace=True)\n",
              "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (16): ReLU(inplace=True)\n",
              "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (19): ReLU(inplace=True)\n",
              "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (32): ReLU(inplace=True)\n",
              "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (36): ReLU(inplace=True)\n",
              "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (39): ReLU(inplace=True)\n",
              "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (42): ReLU(inplace=True)\n",
              "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
              "  )\n",
              "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QSmpbnOM-e_",
        "outputId": "112677a5-a0fe-43db-aa6c-6a180f22dcc9"
      },
      "source": [
        "\n",
        "net = VGG('VGG16')\n",
        "if data_int :\n",
        "  net.half()\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr=0.001\n",
        "n_epochs=[22,30,20]\n",
        "pruning_coefs=[{\"fc\":0.3 , \"conv\":0.3,\"dim\":0},{\"fc\":0.4 , \"conv\":0.3,\"dim\":2}]\n",
        "\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=lr, momentum=0.9)\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=4)\n",
        "\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(trainloader,validloader,testloader,n_epochs[0],criterion,optimizer,mymodel) \n",
        "\n",
        "mymodel.prune_all_layers(pruning_coefs[0])\n",
        "print(get_number_param_pruned(mymodel.model))\n",
        "\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(trainloader,validloader,testloader,n_epochs[1],criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy) \n",
        "\n",
        "mymodel.prune_all_layers(pruning_coefs[1])\n",
        "get_number_param_pruned(mymodel.model)\n",
        "\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(trainloader,validloader,testloader,n_epochs[2],criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "saving weights.... \n",
            "65.65  % ,  0.9177489304542541  ,  1.0682913821935653\n",
            "epoch  1\n",
            "saving weights.... \n",
            "65.125  % ,  0.8306737053394317  ,  0.8712142884731293\n",
            "Finished Training\n",
            "Pruning....\n",
            "(14710464.0, 2048.0)\n",
            "epoch  0\n",
            "saving weights.... \n",
            "68.55  % ,  0.9009149146080017  ,  0.7548020806908607\n",
            "epoch  1\n",
            "saving weights.... \n",
            "72.825  % ,  0.7062627482414245  ,  0.708474158346653\n",
            "Finished Training\n",
            "Pruning....\n",
            "epoch  0\n",
            "saving weights.... \n",
            "70.45  % ,  0.9119302475452423  ,  0.8206265556812287\n",
            "epoch  1\n",
            "saving weights.... \n",
            "69.8  % ,  0.8463314270973206  ,  0.7093483358621597\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz518rJeMPUV",
        "outputId": "7859f3c0-e457-4f93-fe92-186491ca2b0c"
      },
      "source": [
        "mymodel.prune_spef_layer(\"a\",5,5)\n",
        "# Affichage des layers convolutionnels et du classifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "features.0\n",
            "features.3\n",
            "features.7\n",
            "features.10\n",
            "features.14\n",
            "features.17\n",
            "features.20\n",
            "features.24\n",
            "features.27\n",
            "features.30\n",
            "features.34\n",
            "features.37\n",
            "features.40\n",
            "classifier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "clcAwHe-M-h-",
        "outputId": "d9b40886-cfee-4df2-f777-66b691d7d522"
      },
      "source": [
        "# Affichage de l'accuracy au fur et à mesure des epochs. \n",
        "plot_accuracy(test_accuracy,sum(n_epochs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUOvd_tikW8G"
      },
      "source": [
        "**On peut voir les 2 élagages, aux moments de forte décroissance de l'accuracy. On peut aussi voir qu'après le 2ème élagage le réseau perde beaucoup d'accuracy qu'il ne regagnera probablement pas.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "R3OeAxTySTfY",
        "outputId": "9cae03f4-c89b-499e-ef5c-9a76a3b564a5"
      },
      "source": [
        "plot_loss(valid_loss,training_loss,sum(n_epochs))\n",
        "# On observe des comportements "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jL0AUGZkW_W"
      },
      "source": [
        "**On observe aussi ici les différents élagages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZuyXEV7bv3X",
        "outputId": "0d741356-620d-4167-a23f-c0c726932fd1"
      },
      "source": [
        "print(\"top-accuracy : \",max(test_accuracy))\n",
        "      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top-accuracy :  85.275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8MTQXi8M-nm",
        "outputId": "e3bd399e-decc-4459-9b3c-7d77722fae99"
      },
      "source": [
        "get_number_param_pruned(mymodel.model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "params_initiaux :  9228036\n",
            "nouveaux conv :  3072576.0\n",
            "nouveaux fc :  612.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDZHW-ByKKny"
      },
      "source": [
        "## Pruning specific layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmL1GaJiqAZV"
      },
      "source": [
        "**Ici nous mènerons une étude plus fine en élagagant chaque layers à différents taux afin d'observer l'impact sur l'efficacité du réseaux. Pour cela nous entrainons un model que nous élaguons ( 30% des poids des étages convolutionnels sur la dimension 0 -input features- et 70% des poids des couches full connectés). Puis nous le reentrainons afin de remonter notre effiacité, puis nous le reélaguons à différents taux sur différents layers sur la dimension 2 - output features-.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1dHmeVqKNHE"
      },
      "source": [
        "\n",
        "\n",
        "net = VGG('VGG16')\n",
        "if data_int :\n",
        "  net.half()\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "lr=0.001\n",
        "n_epochs=[25,20,16]\n",
        "pruning_coefs=[{\"fc\":0.7 , \"conv\":0.3,\"dim\":0}]\n",
        "\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=lr, momentum=0.9,weight_decay=5e-4)\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=4)\n",
        "\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(trainloader,validloader,testloader,n_epochs[0],criterion,optimizer,mymodel) \n",
        "\n",
        "mymodel.prune_all_layers(pruning_coefs[0])\n",
        "paramconv1,paramfc1=get_number_param_pruned(mymodel.model)\n",
        "\n",
        "print(paramconv1,paramfc1)\n",
        "\n",
        "valid_loss1,training_loss1,test_accuracy1 = trainingwithPrunning(trainloader,validloader,testloader,n_epochs[1],criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy) \n",
        "\n",
        "#torch.save(mymodel.model.state_dict(), 'checkpoint.pt')\n",
        "state = {\n",
        "            'net': mymodel.model.state_dict(),\n",
        "            'scheduler': lr_scheduler,\n",
        "            'optimizer': optimizer,\n",
        "    }\n",
        "torch.save(state, 'checkpoint.pt')\n",
        "\n",
        "print(\"___________Phase 1 finie_____________\")\n",
        "print(\"Accuracy max : \",max(test_accuracy1))\n",
        "print(\"_____________________________________\")\n",
        "\n",
        "ratio_conv=[0.3,0.6]\n",
        "#list_modules=[\"features.0\",\"features.4\",\"features.8\",\"features.11\",\"features.15\",\"features.18\",\"features.22\",\"features.25\"]    VGG11\n",
        "\n",
        "recap_dic={}\n",
        "list_modules=[ \"features.0\",\n",
        "              \"features.3\",\n",
        "              \"features.7\",\n",
        "              \"features.10\",\n",
        "              \"features.14\",\n",
        "              \"features.17\",\n",
        "              \"features.20\",\n",
        "              \"features.24\",\n",
        "              \"features.27\",\n",
        "              \"features.30\",\n",
        "              \"features.34\",\n",
        "              \"features.37\",\n",
        "              \"features.40\"]\n",
        "\n",
        "\n",
        "\n",
        "for ratios in ratio_conv :  \n",
        "  print(\"________________________________________________________\")\n",
        "  print(\"setting ratio to \",ratios)\n",
        "  for module in list_modules:\n",
        "    valid_loss,training_loss,test_accuracy=[],[],[]\n",
        "\n",
        "    loaded_cpt = torch.load('checkpoint.pt')\n",
        "\n",
        "    mymodel.model.load_state_dict(loaded_cpt[\"net\"])\n",
        "\n",
        "    lr_scheduler=loaded_cpt['scheduler']\n",
        "\n",
        "    optimizer=loaded_cpt['optimizer']\n",
        "\n",
        "\n",
        "    print(\"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \")\n",
        "    print(\"changing module : \", module)\n",
        "    \n",
        "    mymodel.prune_spef_layer(module,ratios,2)\n",
        "    paramconv2,paramfc2=get_number_param_pruned(mymodel.model)\n",
        "    print(paramconv2,paramfc2)\n",
        "\n",
        "    valid_loss,training_loss,test_accuracy = trainingwithPrunning(trainloader,validloader,testloader,n_epochs[2],criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy)\n",
        "\n",
        "    recap_dic[str(ratios)+\" \"+module]={\n",
        "        \"test_acc\" :test_accuracy ,\n",
        "        \"valid_loss\": valid_loss,\n",
        "        \"training_loss\": training_loss ,\n",
        "        \"nbr_param\":[paramfc2,paramconv2] ,\n",
        "    }\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vY7lRijVj6LG",
        "outputId": "dd860de9-48a0-4b23-9cac-37150d938b05"
      },
      "source": [
        "fig, axs = plt.subplots(13,2)\n",
        "fig.suptitle('Accuracies')\n",
        "\n",
        "\n",
        "\n",
        "for key, value in recap_dic.items() :\n",
        "  if key[:3]==str(0.3):\n",
        "    y=0\n",
        "  else :\n",
        "    y=1\n",
        "  test_ax=np.arange(0,n_epochs[-1],n_epochs[-1]/len(value[\"test_acc\"]))\n",
        "\n",
        "  axs[list_modules.index(key[4:]), y].plot(test_ax, value[\"test_acc\"])\n",
        "  axs[list_modules.index(key[4:]), y].set_title(key)\n",
        "\n",
        "  print(key)\n",
        "  print(max(value[\"test_acc\"]))\n",
        "  print(value[\"nbr_param\"])\n",
        "  print()\n",
        "#7,1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.3 features.0\n",
            "77.775\n",
            "[616.0, 10286442.0]\n",
            "\n",
            "0.3 features.3\n",
            "81.2\n",
            "[616.0, 10278207.0]\n",
            "\n",
            "0.3 features.7\n",
            "81.525\n",
            "[616.0, 10269567.0]\n",
            "\n",
            "0.3 features.10\n",
            "81.575\n",
            "[616.0, 10252287.0]\n",
            "\n",
            "0.3 features.14\n",
            "81.15\n",
            "[616.0, 10218111.0]\n",
            "\n",
            "0.3 features.17\n",
            "82.1\n",
            "[616.0, 10149375.0]\n",
            "\n",
            "0.3 features.20\n",
            "83.325\n",
            "[616.0, 10149375.0]\n",
            "\n",
            "0.3 features.24\n",
            "82.525\n",
            "[616.0, 10011903.0]\n",
            "\n",
            "0.3 features.27\n",
            "82.75\n",
            "[616.0, 9736959.0]\n",
            "\n",
            "0.3 features.30\n",
            "83.75\n",
            "[616.0, 9736959.0]\n",
            "\n",
            "0.3 features.34\n",
            "83.35\n",
            "[616.0, 9736959.0]\n",
            "\n",
            "0.3 features.37\n",
            "83.45\n",
            "[616.0, 9736959.0]\n",
            "\n",
            "0.3 features.40\n",
            "83.925\n",
            "[616.0, 9736959.0]\n",
            "\n",
            "0.6 features.0\n",
            "62.7\n",
            "[616.0, 10286037.0]\n",
            "\n",
            "0.6 features.3\n",
            "74.4\n",
            "[616.0, 10269567.0]\n",
            "\n",
            "0.6 features.7\n",
            "73.1\n",
            "[616.0, 10252287.0]\n",
            "\n",
            "0.6 features.10\n",
            "77.125\n",
            "[616.0, 10217727.0]\n",
            "\n",
            "0.6 features.14\n",
            "70.2\n",
            "[616.0, 10149375.0]\n",
            "\n",
            "0.6 features.17\n",
            "81.0\n",
            "[616.0, 10011903.0]\n",
            "\n",
            "0.6 features.20\n",
            "78.775\n",
            "[616.0, 10011903.0]\n",
            "\n",
            "0.6 features.24\n",
            "80.95\n",
            "[616.0, 9736959.0]\n",
            "\n",
            "0.6 features.27\n",
            "82.4\n",
            "[616.0, 9187071.0]\n",
            "\n",
            "0.6 features.30\n",
            "83.1\n",
            "[616.0, 9187071.0]\n",
            "\n",
            "0.6 features.34\n",
            "83.65\n",
            "[616.0, 9187071.0]\n",
            "\n",
            "0.6 features.37\n",
            "83.325\n",
            "[616.0, 9187071.0]\n",
            "\n",
            "0.6 features.40\n",
            "83.6\n",
            "[616.0, 9187071.0]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 26 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtmbI4woKNPZ"
      },
      "source": [
        "\n",
        "figure=plt.figure(figsize=(10,10))\n",
        "\n",
        "for key, value in recap_dic.items() :\n",
        "  #if key[:3]==str(0.3):\n",
        "\n",
        "  print(key,\" : \",max(value[\"test_acc\"]),\" ---- \",sum(value[\"nbr_param\"]))\n",
        "  plt.scatter(sum(value[\"nbr_param\"]),max(value[\"test_acc\"]))\n",
        "\n",
        "plt.legend(recap_dic.keys())\n",
        "plt.xlabel(\"millions of parameters\")\n",
        "plt.ylabel(\"Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cj_dUV6r0w_"
      },
      "source": [
        "**L'image n'est plus disponible, mais nous retenons de cette expérience qu'il est possible de pruner à haut taux les derniers layers, mais qu'il est plus compliqué de pruner les layers de bas niveau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYpN6jtqoT4U"
      },
      "source": [
        "## Application à Cifar 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZNRgEvOoQD9"
      },
      "source": [
        "\n",
        "\n",
        "net = VGG('VGG16')\n",
        "if data_int :\n",
        "  net.half()\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "lr=0.1\n",
        "n_epochs=[100,20,16]\n",
        "pruning_coefs=[{\"fc\":0.7 , \"conv\":0.3,\"dim\":0}]\n",
        "\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=lr, momentum=0.9,weight_decay=5e-4)\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True,factor=0.125, patience=5)\n",
        "\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,n_epochs[0],criterion,optimizer,mymodel) \n",
        "\n",
        "mymodel.prune_all_layers(pruning_coefs[0])\n",
        "paramconv1,paramfc1=get_number_param_pruned(mymodel.model)\n",
        "\n",
        "print(paramconv1,paramfc1)\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "\n",
        "valid_loss1,training_loss1,test_accuracy1 = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,n_epochs[1],criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy) \n",
        "\n",
        "\n",
        "\n",
        "print(\"___________Phase 1 finie_____________\")\n",
        "print(\"Accuracy max : \",max(test_accuracy1))\n",
        "print(\"_____________________________________\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pruned_liste=[[0.6,\"features.40\"],[0.6,\"features.34\"],[0.3,\"features.30\"],[0.6,\"features.37\"]]\n",
        "\n",
        "\n",
        "\n",
        "for element in pruned_liste :\n",
        "\n",
        "    mymodel.prune_spef_layer(element[1],element[0],2)\n",
        "    paramconv2,paramfc2=get_number_param_pruned(mymodel.model)\n",
        "    print(paramconv2,paramfc2)\n",
        "\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True,factor=0.125, patience=5)\n",
        "valid_loss2,training_loss2,test_accuracy2 = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,n_epochs[2],criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy)\n",
        "\n",
        "    \n",
        "print(max(test_accuracy2))\n",
        "print(paramconv2,paramfc2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TKDnEvroQGn"
      },
      "source": [
        "\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True,factor=0.125, patience=5)\n",
        "valid_loss2,training_loss2,test_accuracy2 = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,n_epochs[2],criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sjj2rNVoQTW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZwsCt1CZr55"
      },
      "source": [
        "## Test avec 3 prunage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7d2NLV9174L"
      },
      "source": [
        "**Ici nous allons travailler sur cifar 10. Nous allons pruner divers étages de VGG à différents taux. En raison du temps de calculs nécessaires à cela, des sauvegardes de résultats sont effectuées.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuhHjq3llupB"
      },
      "source": [
        "### 1er entrainement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XoW557i2OfA"
      },
      "source": [
        "Nous entrainons un premier modèle :\n",
        "- premier entrainement (from scratch) de 100 epoch avec lr = 0.1 (, avec weight  decay, momentum et lr scheduler)\n",
        "\n",
        "- Pruning de touts les layers convolutionnels à 30% sur la dim 0 ( input features) et 90% sur les full connected\n",
        "- Fine tuning : reentrainement sur 20 epochs, lr = 0.01(, avec weight  decay, momentum)\n",
        "- Fine tuning : reentrainement sur 10 epochs, lr = 0.001(, avec weight  decay, momentum)\n",
        "\n",
        "- Pruning de touts les layers à 30% sur la dim 1 ( kernel)\n",
        "- Fine tuning : reentrainement sur 20 epochs, lr = 0.01(, avec weight  decay, momentum)\n",
        "- Fine tuning : reentrainement sur 10 epochs, lr = 0.001(, avec weight  decay, momentum)\n",
        "\n",
        "\n",
        "Après expériences, nous avons réalisés que réaliser 2 réentrainement avec un lr qui se réduisait, permettaient de grandement améliorer notre fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q_viZM9MoQWs",
        "outputId": "4aefa0c6-5aca-4176-d139-29956d82053a"
      },
      "source": [
        "\n",
        "data_int=False\n",
        "net = VGG('VGG16')\n",
        "if data_int :\n",
        "  net.half()\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "lr=0.1\n",
        "n_epochs=[100,20,10,20,10,20,10]\n",
        "pruning_coefs=[{\"fc\":0.9 , \"conv\":0.3,\"dim\":0},{\"fc\":0 , \"conv\":0.3,\"dim\":1}]\n",
        "\n",
        "#premier entrainement\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.1, momentum=0.9,weight_decay=5e-4)\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True,factor=0.125, patience=5)\n",
        "valid_loss0,training_loss0,test_accuracy0 = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,n_epochs[0],criterion,optimizer,mymodel) \n",
        "\n",
        "\n",
        "#1er pruning\n",
        "mymodel.prune_all_layers(pruning_coefs[0])\n",
        "paramconv0,paramfc0=get_number_param_pruned(mymodel.model)\n",
        "print(paramconv0,paramfc0)\n",
        "\n",
        "#2ème entrainement\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss0,training_loss0,test_accuracy0 = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,n_epochs[1],criterion,optimizer,mymodel,valid_loss0,training_loss0,test_accuracy0,scheduler=False )\n",
        "\n",
        "#2ème entrainement bis\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss0,training_loss0,test_accuracy0 = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,n_epochs[2],criterion,optimizer,mymodel,valid_loss0,training_loss0,test_accuracy0,scheduler=False ) \n",
        "\n",
        "#2ème pruning\n",
        "mymodel.prune_all_layers(pruning_coefs[1])\n",
        "paramconv1,paramfc1=get_number_param_pruned(mymodel.model)\n",
        "print(paramconv1,paramfc1)\n",
        "\n",
        "\n",
        "#3ème entrainement \n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss0,training_loss0,test_accuracy0 = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,n_epochs[3],criterion,optimizer,mymodel,valid_loss0,training_loss0,test_accuracy0,scheduler=False ) \n",
        "\n",
        "#3ème entrainement bis\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss0,training_loss0,test_accuracy0 = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,n_epochs[4],criterion,optimizer,mymodel,valid_loss0,training_loss0,test_accuracy0,scheduler=False ) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#torch.save(mymodel.model.state_dict(), 'checkpoint.pt')\n",
        "state = {\n",
        "            'net': mymodel.model.state_dict(),\n",
        "            'scheduler': lr_scheduler,\n",
        "            'optimizer': optimizer,\n",
        "    }\n",
        "torch.save(state, 'checkpoint.pt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"___________Phase 1 finie_____________\")\n",
        "print(\"Accuracy max : \",max(test_accuracy))\n",
        "print(\"_____________________________________\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "saving weights.... \n",
            "11.67  % ,  2.3020303283691406  ,  2.417413781929016\n",
            "epoch  1\n",
            "saving weights.... \n",
            "20.72  % ,  2.066643391418457  ,  2.1081053373336793\n",
            "epoch  2\n",
            "saving weights.... \n",
            "34.58  % ,  1.73147279586792  ,  1.7918777463912965\n",
            "epoch  3\n",
            "saving weights.... \n",
            "32.19  % ,  2.4348307025909426  ,  1.5905733932495116\n",
            "epoch  4\n",
            "saving weights.... \n",
            "32.84  % ,  2.684371448135376  ,  1.443699486351013\n",
            "epoch  5\n",
            "saving weights.... \n",
            "54.11  % ,  1.3757770265579223  ,  1.2983197905540467\n",
            "epoch  6\n",
            "saving weights.... \n",
            "54.47  % ,  1.540200781059265  ,  1.184232015800476\n",
            "epoch  7\n",
            "saving weights.... \n",
            "58.96  % ,  1.3641583070755006  ,  1.1295040771007538\n",
            "epoch  8\n",
            "saving weights.... \n",
            "54.38  % ,  1.3603452348709106  ,  1.076919110441208\n",
            "epoch  9\n",
            "saving weights.... \n",
            "60.29  % ,  1.1539476640701294  ,  1.056083007645607\n",
            "epoch  10\n",
            "saving weights.... \n",
            "67.85  % ,  0.9873358587265014  ,  1.0466141822338104\n",
            "epoch  11\n",
            "saving weights.... \n",
            "53.51  % ,  1.5343587635040283  ,  1.03401013982296\n",
            "epoch  12\n",
            "saving weights.... \n",
            "59.01  % ,  1.2558701667785646  ,  1.027432073020935\n",
            "epoch  13\n",
            "saving weights.... \n",
            "55.63  % ,  1.742766997909546  ,  1.0117052640676498\n",
            "epoch  14\n",
            "saving weights.... \n",
            "63.86  % ,  1.107673205757141  ,  1.0006779901742935\n",
            "epoch  15\n",
            "saving weights.... \n",
            "62.27  % ,  1.2477540365219115  ,  0.9994595697641373\n",
            "epoch  16\n",
            "Epoch    17: reducing learning rate of group 0 to 1.2500e-02.\n",
            "saving weights.... \n",
            "65.22  % ,  1.079909609222412  ,  0.9836529437065125\n",
            "epoch  17\n",
            "saving weights.... \n",
            "82.47  % ,  0.5488995859146119  ,  0.6278569725632668\n",
            "epoch  18\n",
            "saving weights.... \n",
            "83.06  % ,  0.5230136713981628  ,  0.5512760835707188\n",
            "epoch  19\n",
            "saving weights.... \n",
            "83.23  % ,  0.5268640187978745  ,  0.5201296313762664\n",
            "epoch  20\n",
            "saving weights.... \n",
            "83.24  % ,  0.5153795557498931  ,  0.49391228057146075\n",
            "epoch  21\n",
            "saving weights.... \n",
            "82.51  % ,  0.5407895516395569  ,  0.4772192007482052\n",
            "epoch  22\n",
            "saving weights.... \n",
            "83.05  % ,  0.5192469216108322  ,  0.47152038807868957\n",
            "epoch  23\n",
            "saving weights.... \n",
            "82.57  % ,  0.5358894493818283  ,  0.46235366578698156\n",
            "epoch  24\n",
            "saving weights.... \n",
            "81.18  % ,  0.568852701330185  ,  0.45535792359709737\n",
            "epoch  25\n",
            "saving weights.... \n",
            "82.93  % ,  0.5261790685176849  ,  0.44518737993240354\n",
            "epoch  26\n",
            "Epoch    27: reducing learning rate of group 0 to 1.5625e-03.\n",
            "saving weights.... \n",
            "82.1  % ,  0.550914296245575  ,  0.4434565069556236\n",
            "epoch  27\n",
            "saving weights.... \n",
            "88.1  % ,  0.3833277987003326  ,  0.3000860840916634\n",
            "epoch  28\n",
            "saving weights.... \n",
            "88.29  % ,  0.37371886537075044  ,  0.2554480500087142\n",
            "epoch  29\n",
            "saving weights.... \n",
            "88.16  % ,  0.3774108278512955  ,  0.23423733055740595\n",
            "epoch  30\n",
            "saving weights.... \n",
            "88.35  % ,  0.37471715995669364  ,  0.22376299384608864\n",
            "epoch  31\n",
            "saving weights.... \n",
            "88.24  % ,  0.3774086757659912  ,  0.20974931845963002\n",
            "epoch  32\n",
            "saving weights.... \n",
            "88.78  % ,  0.3539587100625038  ,  0.20014114781022072\n",
            "epoch  33\n",
            "saving weights.... \n",
            "88.61  % ,  0.37030838384628295  ,  0.1912137828655541\n",
            "epoch  34\n",
            "saving weights.... \n",
            "88.78  % ,  0.3758167558670044  ,  0.18359255600422622\n",
            "epoch  35\n",
            "saving weights.... \n",
            "88.83  % ,  0.37097576482594014  ,  0.1714328805282712\n",
            "epoch  36\n",
            "saving weights.... \n",
            "88.86  % ,  0.38292558805942534  ,  0.16728458694815634\n",
            "epoch  37\n",
            "saving weights.... \n",
            "88.72  % ,  0.38256981055140493  ,  0.16389295557737352\n",
            "epoch  38\n",
            "Epoch    39: reducing learning rate of group 0 to 1.9531e-04.\n",
            "saving weights.... \n",
            "88.89  % ,  0.37957434527873996  ,  0.15479989875108002\n",
            "epoch  39\n",
            "saving weights.... \n",
            "89.46  % ,  0.36872845175266267  ,  0.12238503238260746\n",
            "epoch  40\n",
            "saving weights.... \n",
            "89.44  % ,  0.3589270159959793  ,  0.11343987673036754\n",
            "epoch  41\n",
            "saving weights.... \n",
            "89.53  % ,  0.3733252852499485  ,  0.10791944822892546\n",
            "epoch  42\n",
            "saving weights.... \n",
            "89.52  % ,  0.36418450221419335  ,  0.1055736208818853\n",
            "epoch  43\n",
            "saving weights.... \n",
            "89.48  % ,  0.3675555806308985  ,  0.10206770761981607\n",
            "epoch  44\n",
            "Epoch    45: reducing learning rate of group 0 to 2.4414e-05.\n",
            "saving weights.... \n",
            "89.7  % ,  0.36957138311862947  ,  0.0997672894783318\n",
            "epoch  45\n",
            "saving weights.... \n",
            "89.58  % ,  0.3745081982553005  ,  0.09648132153525948\n",
            "epoch  46\n",
            "saving weights.... \n",
            "89.47  % ,  0.3728187191188335  ,  0.09325560500770808\n",
            "epoch  47\n",
            "saving weights.... \n",
            "89.54  % ,  0.3762701651334763  ,  0.09241372288390994\n",
            "epoch  48\n",
            "saving weights.... \n",
            "89.57  % ,  0.3732919434279203  ,  0.09699507030025124\n",
            "epoch  49\n",
            "saving weights.... \n",
            "89.52  % ,  0.3642566065102816  ,  0.09316237828917802\n",
            "epoch  50\n",
            "Epoch    51: reducing learning rate of group 0 to 3.0518e-06.\n",
            "saving weights.... \n",
            "89.57  % ,  0.3812022388458252  ,  0.09311661867536604\n",
            "epoch  51\n",
            "saving weights.... \n",
            "89.54  % ,  0.36688602935671805  ,  0.09504935253560542\n",
            "epoch  52\n",
            "saving weights.... \n",
            "89.58  % ,  0.3719777122616768  ,  0.09340891186445952\n",
            "epoch  53\n",
            "saving weights.... \n",
            "89.59  % ,  0.3676685450255871  ,  0.0936456647887826\n",
            "epoch  54\n",
            "saving weights.... \n",
            "89.56  % ,  0.3589105906009674  ,  0.09410241490378976\n",
            "epoch  55\n",
            "saving weights.... \n",
            "89.53  % ,  0.37763314894139766  ,  0.09448723642993719\n",
            "epoch  56\n",
            "Epoch    57: reducing learning rate of group 0 to 3.8147e-07.\n",
            "saving weights.... \n",
            "89.43  % ,  0.3743018726825714  ,  0.0929805553946644\n",
            "epoch  57\n",
            "saving weights.... \n",
            "89.66  % ,  0.3664599239170551  ,  0.09282733952403069\n",
            "epoch  58\n",
            "saving weights.... \n",
            "89.64  % ,  0.3752812718987465  ,  0.09697720702551306\n",
            "epoch  59\n",
            "saving weights.... \n",
            "89.58  % ,  0.3684004069924355  ,  0.09414514120779932\n",
            "epoch  60\n",
            "saving weights.... \n",
            "89.48  % ,  0.3763148213982582  ,  0.09242788287214934\n",
            "epoch  61\n",
            "saving weights.... \n",
            "89.59  % ,  0.37409498401880265  ,  0.09248446319065988\n",
            "epoch  62\n",
            "Epoch    63: reducing learning rate of group 0 to 4.7684e-08.\n",
            "saving weights.... \n",
            "89.69  % ,  0.3755095350861549  ,  0.09197015557549894\n",
            "epoch  63\n",
            "saving weights.... \n",
            "89.58  % ,  0.3700080324232578  ,  0.09315418026857078\n",
            "epoch  64\n",
            "saving weights.... \n",
            "89.64  % ,  0.3671286351710558  ,  0.09209903924632817\n",
            "epoch  65\n",
            "saving weights.... \n",
            "89.58  % ,  0.37065474313497543  ,  0.09418897079341114\n",
            "epoch  66\n",
            "saving weights.... \n",
            "89.56  % ,  0.3626444832921028  ,  0.09173762956857681\n",
            "epoch  67\n",
            "saving weights.... \n",
            "89.55  % ,  0.37787334629893304  ,  0.09297983642444015\n",
            "epoch  68\n",
            "Epoch    69: reducing learning rate of group 0 to 5.9605e-09.\n",
            "saving weights.... \n",
            "89.62  % ,  0.3759508341193199  ,  0.09089234627857805\n",
            "epoch  69\n",
            "saving weights.... \n",
            "89.35  % ,  0.3685145484983921  ,  0.09398187811039388\n",
            "epoch  70\n",
            "saving weights.... \n",
            "89.59  % ,  0.3821151529610157  ,  0.09208251990228891\n",
            "epoch  71\n",
            "saving weights.... \n",
            "89.59  % ,  0.37596478788256643  ,  0.09210607695169747\n",
            "epoch  72\n",
            "saving weights.... \n",
            "89.65  % ,  0.36906644086241724  ,  0.092994559654966\n",
            "epoch  73\n",
            "saving weights.... \n",
            "89.52  % ,  0.37426490671038626  ,  0.09313778205439448\n",
            "epoch  74\n",
            "saving weights.... \n",
            "89.65  % ,  0.3690250712633133  ,  0.09036091858036815\n",
            "epoch  75\n",
            "saving weights.... \n",
            "89.57  % ,  0.37159501811265944  ,  0.09060247026607395\n",
            "epoch  76\n",
            "saving weights.... \n",
            "89.55  % ,  0.3723872014760971  ,  0.09386438751332461\n",
            "epoch  77\n",
            "saving weights.... \n",
            "89.55  % ,  0.36968602390289307  ,  0.09317235793285072\n",
            "epoch  78\n",
            "saving weights.... \n",
            "89.49  % ,  0.3650114567577839  ,  0.09549892504177987\n",
            "epoch  79\n",
            "saving weights.... \n",
            "89.42  % ,  0.3778181895673275  ,  0.09313822543490678\n",
            "epoch  80\n",
            "saving weights.... \n",
            "89.6  % ,  0.3706963669806719  ,  0.09377261420711874\n",
            "epoch  81\n",
            "saving weights.... \n",
            "89.59  % ,  0.36660486810207366  ,  0.09112022848576307\n",
            "epoch  82\n",
            "saving weights.... \n",
            "89.54  % ,  0.3637263784915209  ,  0.09312397752143442\n",
            "epoch  83\n",
            "saving weights.... \n",
            "89.58  % ,  0.36958114493489264  ,  0.09073525677584111\n",
            "epoch  84\n",
            "saving weights.... \n",
            "89.65  % ,  0.36077159188240765  ,  0.09305420763380826\n",
            "epoch  85\n",
            "saving weights.... \n",
            "89.51  % ,  0.3733453572332859  ,  0.09211023069806397\n",
            "epoch  86\n",
            "saving weights.... \n",
            "89.56  % ,  0.37377186697125436  ,  0.0924114108076319\n",
            "epoch  87\n",
            "saving weights.... \n",
            "89.54  % ,  0.3686994249165058  ,  0.09278076465763152\n",
            "epoch  88\n",
            "saving weights.... \n",
            "89.59  % ,  0.37617474278509616  ,  0.0921571606207639\n",
            "epoch  89\n",
            "saving weights.... \n",
            "89.55  % ,  0.37123578150868414  ,  0.09485311852395535\n",
            "epoch  90\n",
            "saving weights.... \n",
            "89.57  % ,  0.37082732361257076  ,  0.09141548810228706\n",
            "epoch  91\n",
            "saving weights.... \n",
            "89.49  % ,  0.37753736719787123  ,  0.09214914880804717\n",
            "epoch  92\n",
            "saving weights.... \n",
            "89.61  % ,  0.37697616447508336  ,  0.09337491270862519\n",
            "epoch  93\n",
            "saving weights.... \n",
            "89.65  % ,  0.364629502171278  ,  0.09125193781219423\n",
            "epoch  94\n",
            "saving weights.... \n",
            "89.59  % ,  0.3690165502548218  ,  0.09101326310560107\n",
            "epoch  95\n",
            "saving weights.... \n",
            "89.48  % ,  0.3674890054017305  ,  0.09095142091736197\n",
            "epoch  96\n",
            "saving weights.... \n",
            "89.58  % ,  0.3673253970295191  ,  0.09096200352422892\n",
            "epoch  97\n",
            "saving weights.... \n",
            "89.57  % ,  0.37336855450868606  ,  0.09193984320946037\n",
            "epoch  98\n",
            "saving weights.... \n",
            "89.63  % ,  0.36366496005654336  ,  0.09362751943543553\n",
            "epoch  99\n",
            "saving weights.... \n",
            "89.6  % ,  0.37437075972259043  ,  0.09375980184935033\n",
            "Finished Training\n",
            "Pruning....\n",
            "10286847.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "83.65  % ,  0.5221410145044327  ,  0.4714464868605137\n",
            "epoch  1\n",
            "saving weights.... \n",
            "84.37  % ,  0.5152353544712067  ,  0.4314799740970135\n",
            "epoch  2\n",
            "saving weights.... \n",
            "82.98  % ,  0.5163485506296158  ,  0.42243181087374687\n",
            "epoch  3\n",
            "saving weights.... \n",
            "83.06  % ,  0.5460205240726471  ,  0.3955083531349897\n",
            "epoch  4\n",
            "saving weights.... \n",
            "82.39  % ,  0.5420370716571807  ,  0.39075385716557504\n",
            "epoch  5\n",
            "saving weights.... \n",
            "83.35  % ,  0.49611043637990954  ,  0.3868023108005524\n",
            "epoch  6\n",
            "saving weights.... \n",
            "83.76  % ,  0.5139239819526672  ,  0.3741577806711197\n",
            "epoch  7\n",
            "saving weights.... \n",
            "84.57  % ,  0.48873352551460264  ,  0.3619259416460991\n",
            "epoch  8\n",
            "saving weights.... \n",
            "84.56  % ,  0.5066027806520462  ,  0.36245851413309577\n",
            "epoch  9\n",
            "saving weights.... \n",
            "85.08  % ,  0.48782283272743227  ,  0.3563381720840931\n",
            "epoch  10\n",
            "saving weights.... \n",
            "85.17  % ,  0.4667084740161896  ,  0.34694795441031456\n",
            "epoch  11\n",
            "saving weights.... \n",
            "82.92  % ,  0.5441156292438507  ,  0.34842971118986604\n",
            "epoch  12\n",
            "saving weights.... \n",
            "85.42  % ,  0.460188448369503  ,  0.33998007444143297\n",
            "epoch  13\n",
            "saving weights.... \n",
            "83.55  % ,  0.5594184343099594  ,  0.33469861417412755\n",
            "epoch  14\n",
            "saving weights.... \n",
            "83.48  % ,  0.5341177065849304  ,  0.3324211207777262\n",
            "epoch  15\n",
            "saving weights.... \n",
            "83.83  % ,  0.522014312338829  ,  0.3304660686969757\n",
            "epoch  16\n",
            "saving weights.... \n",
            "84.9  % ,  0.4718740864276886  ,  0.3282374457508326\n",
            "epoch  17\n",
            "saving weights.... \n",
            "84.5  % ,  0.4800481384992599  ,  0.324031405121088\n",
            "epoch  18\n",
            "saving weights.... \n",
            "85.41  % ,  0.47365801706314087  ,  0.3195135005354881\n",
            "epoch  19\n",
            "saving weights.... \n",
            "83.84  % ,  0.49312546308040617  ,  0.31843284825384616\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "89.99  % ,  0.32618296114206313  ,  0.19188263104856015\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.01  % ,  0.328309613776207  ,  0.14769073228538038\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.1  % ,  0.33435251667499544  ,  0.13309207183234392\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.19  % ,  0.3337646927714348  ,  0.1189482294075191\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.31  % ,  0.3378022805452347  ,  0.1092327516477555\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.28  % ,  0.32529391811788083  ,  0.10333820139281452\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.55  % ,  0.33722608537375925  ,  0.09732152100168169\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.55  % ,  0.34084541712254285  ,  0.08926268819384277\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.65  % ,  0.33843429578840734  ,  0.08690344084072858\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.47  % ,  0.3408967581257224  ,  0.08095910558942705\n",
            "Finished Training\n",
            "Pruning....\n",
            "7194231.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "82.85  % ,  0.5772366307020187  ,  0.39671731663644316\n",
            "epoch  1\n",
            "saving weights.... \n",
            "84.29  % ,  0.4931754376411438  ,  0.36312275176644326\n",
            "epoch  2\n",
            "saving weights.... \n",
            "80.94  % ,  0.62733051943779  ,  0.3581112200200558\n",
            "epoch  3\n",
            "saving weights.... \n",
            "82.59  % ,  0.5458648070812225  ,  0.3450205471724272\n",
            "epoch  4\n",
            "saving weights.... \n",
            "85.67  % ,  0.48227920422554016  ,  0.3378283184081316\n",
            "epoch  5\n",
            "saving weights.... \n",
            "83.0  % ,  0.576415773487091  ,  0.33664451165497306\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.15  % ,  0.4790411346435547  ,  0.3281017841041088\n",
            "epoch  7\n",
            "saving weights.... \n",
            "85.43  % ,  0.4969365208864212  ,  0.3251492828249931\n",
            "epoch  8\n",
            "saving weights.... \n",
            "84.92  % ,  0.4888304271697998  ,  0.3234482729077339\n",
            "epoch  9\n",
            "saving weights.... \n",
            "85.43  % ,  0.460977321267128  ,  0.3179836963146925\n",
            "epoch  10\n",
            "saving weights.... \n",
            "85.74  % ,  0.466532265496254  ,  0.316142246940732\n",
            "epoch  11\n",
            "saving weights.... \n",
            "85.58  % ,  0.4805399189710617  ,  0.314176420545578\n",
            "epoch  12\n",
            "saving weights.... \n",
            "84.18  % ,  0.5114793315887451  ,  0.31356075191646815\n",
            "epoch  13\n",
            "saving weights.... \n",
            "85.14  % ,  0.4667842866420746  ,  0.3053950723618269\n",
            "epoch  14\n",
            "saving weights.... \n",
            "86.63  % ,  0.470282453417778  ,  0.3021239306628704\n",
            "epoch  15\n",
            "saving weights.... \n",
            "86.03  % ,  0.4761088403701782  ,  0.3049250571042299\n",
            "epoch  16\n",
            "saving weights.... \n",
            "84.65  % ,  0.5126411559581756  ,  0.3003882993221283\n",
            "epoch  17\n",
            "saving weights.... \n",
            "86.22  % ,  0.46160928158760073  ,  0.2975499641418457\n",
            "epoch  18\n",
            "saving weights.... \n",
            "85.39  % ,  0.4847922438621521  ,  0.2995170045033097\n",
            "epoch  19\n",
            "saving weights.... \n",
            "85.62  % ,  0.46884900035858157  ,  0.29727912406921386\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "89.59  % ,  0.3355635328292847  ,  0.17010595620796085\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.08  % ,  0.3276761618375778  ,  0.12842831336520613\n",
            "epoch  2\n",
            "saving weights.... \n",
            "89.99  % ,  0.3379191252857447  ,  0.10975525309704244\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.12  % ,  0.3445344701230526  ,  0.10058669382072985\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.1  % ,  0.3424496056199074  ,  0.09119220506325364\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.59  % ,  0.35121267728209493  ,  0.08540473322886974\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.5  % ,  0.3587662658691406  ,  0.07861045600660145\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.46  % ,  0.35123559695780276  ,  0.07385647239908576\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.6  % ,  0.3467195249184966  ,  0.0668434476936236\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.56  % ,  0.34947664111703636  ,  0.06669759066374972\n",
            "Finished Training\n",
            "___________Phase 1 finie_____________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4ec45647db25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"___________Phase 1 finie_____________\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy max : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracy1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_____________________________________\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_accuracy1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RCInvC9onrk"
      },
      "source": [
        "#from google.colab import drive\n",
        "from google.colab import files\n",
        "#drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "#files.upload()\n",
        "\n",
        "files.download('checkpoint.pt')# Permet de récupérer notre avancement"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es45-mK9sOP1"
      },
      "source": [
        "### Prune sur les features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8bKq3WH4J15"
      },
      "source": [
        "Ici on prune un étage spécifique à un taux spécifique, puis on réentraine afin de remonter notre accuracy.\n",
        "\n",
        "Le pruning se fait sur la dim 2\n",
        "Le réentrainement se fait sur 20 epochs, avec lr = 0.01, puis sur 10 epochs avec lr =0.001\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozklXSeDqTzv"
      },
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "import io\n",
        "data_int=False\n",
        "def prune_fine_tune(ind_module,ratio_conv=[0.3,0.6]):\n",
        "  try: \n",
        "    loaded_cpt = torch.load('checkpoint.pt')\n",
        "\n",
        "  except :\n",
        "\n",
        "\n",
        "    print(\"Chargez les poids\")\n",
        "    files.upload()\n",
        "    loaded_cpt = torch.load('checkpoint.pt')\n",
        "\n",
        "  \n",
        "  net = VGG('VGG16')\n",
        "  \n",
        "  if data_int :\n",
        "    net.half()\n",
        "  mymodel = my_network_with_trous(net)\n",
        "  mymodel.model = mymodel.model.to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  \n",
        "  recap_dic={}\n",
        "  dic_save={}\n",
        "  list_modules=[ \"features.0\",\n",
        "              \"features.3\",\n",
        "              \"features.7\",\n",
        "              \"features.10\",\n",
        "              \"features.14\",\n",
        "              \"features.17\",\n",
        "              \"features.20\",\n",
        "              \"features.24\",\n",
        "              \"features.27\",\n",
        "              \"features.30\",\n",
        "              \"features.34\",\n",
        "              \"features.37\",\n",
        "              \"features.40\"]\n",
        "  module=list_modules[ind_module]\n",
        "  for ratios in ratio_conv :  \n",
        "    print(\"________________________________________________________\")\n",
        "    print(\"setting ratio to \",ratios)\n",
        "    \n",
        "    valid_loss,training_loss,test_accuracy=[],[],[]\n",
        "    \n",
        "\n",
        "    mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})# nécessaire car sinon le réseau bug, ici on élague 0 paramètres\n",
        "\n",
        "    mymodel.model.load_state_dict(loaded_cpt[\"net\"])\n",
        "    #mymodel.model.load_state_dict(io.BytesIO(loaded_cpt[\"net\"]))\n",
        "\n",
        "    #lr_scheduler=loaded_cpt['scheduler']\n",
        "\n",
        "    #optimizer=loaded_cpt['optimizer']\n",
        "\n",
        "\n",
        "    print(\"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \")\n",
        "    print(\"changing module : \", module)\n",
        "    \n",
        "    mymodel.prune_spef_layer(module,ratios,2)\n",
        "    paramconv2,paramfc2=get_number_param_pruned(mymodel.model)\n",
        "    print(paramconv2,paramfc2)\n",
        "    # entrainement a\n",
        "\n",
        "    optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "    valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,20,criterion,optimizer,mymodel,scheduler=False ) \n",
        "\n",
        "    #entrainement b\n",
        "\n",
        "    optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "    valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,10,criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy,scheduler=False ) \n",
        "\n",
        "\n",
        "    recap_dic[str(ratios)+\" \"+module]={\n",
        "        \"test_acc\" :test_accuracy ,\n",
        "        \"valid_loss\": valid_loss,\n",
        "        \"training_loss\": training_loss ,\n",
        "        \"nbr_param\":[paramfc2,paramconv2] \n",
        "    }\n",
        "    \n",
        "    dic_save[str(ratios)+\" \"+module]={\n",
        "        \"test_acc\" :max(test_accuracy) ,\n",
        "        \"nbr_param\":[paramfc2,paramconv2] \n",
        "    }\n",
        "  \n",
        "  try: \n",
        "    current_feature=np.load(i+'.npy', allow_pickle='TRUE')\n",
        "    \n",
        "    current_feature=current_feature.tolist()\n",
        "\n",
        "    for key, value in current_feature.items() :\n",
        "      dic_save[key]=value\n",
        "\n",
        "\n",
        "  except :\n",
        "    print(\"chargez \",i,\".npy, si inexistant annuler\")\n",
        "    files.upload()\n",
        "    try :\n",
        "\n",
        "      current_feature=np.load(i+'.npy', allow_pickle='TRUE')\n",
        "      current_feature=current_feature.tolist()\n",
        "\n",
        "      for key, value in current_feature.items() :\n",
        "        dic_save[key]=value\n",
        "      \n",
        "\n",
        "    except :\n",
        "      np.save(module+'.npy', dic_save)\n",
        "      files.download(module+'.npy')\n",
        "\n",
        "\n",
        "  return recap_dic\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1KjFeQ6l59s"
      },
      "source": [
        "### Prune sur \"features.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KvS0bSF57Xe"
      },
      "source": [
        "prune_fine_tune(0,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qyfuDXVmqE7"
      },
      "source": [
        "### Prune sur \"features.3\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nVvAFx1npkyk",
        "outputId": "8978d51f-32fa-4be3-8045-d5cf64fbb97c"
      },
      "source": [
        "prune_fine_tune(1,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.3\n",
            "features.3\n",
            "7188156.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "83.32  % ,  0.4187791236877441  ,  0.4079273605465889\n",
            "epoch  1\n",
            "saving weights.... \n",
            "83.88  % ,  0.4018106766104698  ,  0.3540658381551504\n",
            "epoch  2\n",
            "saving weights.... \n",
            "82.56  % ,  0.4652509907484055  ,  0.33332614375948905\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.09  % ,  0.40118082023859025  ,  0.32534253038167954\n",
            "epoch  4\n",
            "saving weights.... \n",
            "84.33  % ,  0.46513295035362245  ,  0.32141239524483683\n",
            "epoch  5\n",
            "saving weights.... \n",
            "84.74  % ,  0.4175166788101196  ,  0.3130440088123083\n",
            "epoch  6\n",
            "saving weights.... \n",
            "83.42  % ,  0.4576169460296631  ,  0.31074335777163503\n",
            "epoch  7\n",
            "saving weights.... \n",
            "82.12  % ,  0.540731411576271  ,  0.30931447079479696\n",
            "epoch  8\n",
            "saving weights.... \n",
            "86.11  % ,  0.4185573099374771  ,  0.3087220321804285\n",
            "epoch  9\n",
            "saving weights.... \n",
            "86.25  % ,  0.3991020019769669  ,  0.3069173827171326\n",
            "epoch  10\n",
            "saving weights.... \n",
            "85.22  % ,  0.4241870040178299  ,  0.3015768424719572\n",
            "epoch  11\n",
            "saving weights.... \n",
            "85.43  % ,  0.4432670653104782  ,  0.2932341114550829\n",
            "epoch  12\n",
            "saving weights.... \n",
            "86.09  % ,  0.41662420592308047  ,  0.3041641746670008\n",
            "epoch  13\n",
            "saving weights.... \n",
            "85.35  % ,  0.4335766587972641  ,  0.2909928483307362\n",
            "epoch  14\n",
            "saving weights.... \n",
            "83.97  % ,  0.5080727669239045  ,  0.2908400336116552\n",
            "epoch  15\n",
            "saving weights.... \n",
            "83.45  % ,  0.484168142414093  ,  0.29654359705001115\n",
            "epoch  16\n",
            "saving weights.... \n",
            "85.72  % ,  0.4433573377370834  ,  0.29154753260314464\n",
            "epoch  17\n",
            "saving weights.... \n",
            "85.83  % ,  0.432013666844368  ,  0.2888963930875063\n",
            "epoch  18\n",
            "saving weights.... \n",
            "86.71  % ,  0.4236209304332733  ,  0.28773582755327226\n",
            "epoch  19\n",
            "saving weights.... \n",
            "85.95  % ,  0.4350401842713356  ,  0.2837288407325745\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "89.82  % ,  0.29148730807304385  ,  0.16135014606267215\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.36  % ,  0.28856611856222153  ,  0.11978951680883765\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.54  % ,  0.28308883257508277  ,  0.10250736006181688\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.51  % ,  0.29095828539133073  ,  0.09077563301697374\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.54  % ,  0.28445813535749914  ,  0.08144782977364957\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.52  % ,  0.29199858210980895  ,  0.07824713298194111\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.56  % ,  0.2953148814052343  ,  0.07122663903646172\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.5  % ,  0.2968919693544507  ,  0.06521718141715974\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.7  % ,  0.2979113065838814  ,  0.0637800672323443\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.67  % ,  0.29900536466240885  ,  0.05701077757114544\n",
            "Finished Training\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.3\n",
            "features.3\n",
            "7182081.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "86.12  % ,  0.3585671618461609  ,  0.4604839712917805\n",
            "epoch  1\n",
            "saving weights.... \n",
            "84.75  % ,  0.40215390717983246  ,  0.36821296733021736\n",
            "epoch  2\n",
            "saving weights.... \n",
            "83.8  % ,  0.4420592936515808  ,  0.3521891108751297\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.22  % ,  0.38922653814554214  ,  0.3480148832499981\n",
            "epoch  4\n",
            "saving weights.... \n",
            "85.53  % ,  0.4037159222960472  ,  0.33442504706382753\n",
            "epoch  5\n",
            "saving weights.... \n",
            "85.53  % ,  0.4089932821273804  ,  0.3257275182902813\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.6  % ,  0.4173490993618965  ,  0.3231349487930536\n",
            "epoch  7\n",
            "saving weights.... \n",
            "83.95  % ,  0.46846406474113467  ,  0.32497233298122885\n",
            "epoch  8\n",
            "saving weights.... \n",
            "85.63  % ,  0.4042750144958496  ,  0.31234465238898995\n",
            "epoch  9\n",
            "saving weights.... \n",
            "84.39  % ,  0.4439492600917816  ,  0.3140811449632049\n",
            "epoch  10\n",
            "saving weights.... \n",
            "83.64  % ,  0.4893664236783981  ,  0.30756306006610395\n",
            "epoch  11\n",
            "saving weights.... \n",
            "83.96  % ,  0.4862129561662674  ,  0.30973936368227006\n",
            "epoch  12\n",
            "saving weights.... \n",
            "85.76  % ,  0.4133458930969238  ,  0.31035704487264154\n",
            "epoch  13\n",
            "saving weights.... \n",
            "83.33  % ,  0.5020191180944443  ,  0.29961748715937137\n",
            "epoch  14\n",
            "saving weights.... \n",
            "85.91  % ,  0.4316810423851013  ,  0.30401615715920927\n",
            "epoch  15\n",
            "saving weights.... \n",
            "86.67  % ,  0.38711990263462065  ,  0.2944527658075094\n",
            "epoch  16\n",
            "saving weights.... \n",
            "84.33  % ,  0.47905206220149993  ,  0.29180890017151834\n",
            "epoch  17\n",
            "saving weights.... \n",
            "85.82  % ,  0.4117352591276169  ,  0.2993567759245634\n",
            "epoch  18\n",
            "saving weights.... \n",
            "84.94  % ,  0.44760724906921384  ,  0.29215417718887327\n",
            "epoch  19\n",
            "saving weights.... \n",
            "86.4  % ,  0.40987945516109464  ,  0.28927055866271256\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "89.74  % ,  0.2855820753633976  ,  0.16083216299787165\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.11  % ,  0.290034493970871  ,  0.12251639947630465\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.09  % ,  0.2861618744432926  ,  0.10589267364665866\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.31  % ,  0.2966159295350313  ,  0.09528515082336962\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.14  % ,  0.30668378766775134  ,  0.08652428435087205\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.53  % ,  0.29086014902740714  ,  0.08206707478072495\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.35  % ,  0.30237287242114547  ,  0.07237067394275218\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.61  % ,  0.29378735272362827  ,  0.06901212050095201\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.47  % ,  0.31184847749471667  ,  0.06361850416399538\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.63  % ,  0.3096024141073227  ,  0.06038223183192313\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5dd217e7-caa3-4987-9bf0-8c29fd4525bb\", \"features.3.npy\", 433)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.3 features.3': {'nbr_param': [510.0, 7188156.0],\n",
              "  'test_acc': [83.98,\n",
              "   85.75,\n",
              "   85.77,\n",
              "   85.73,\n",
              "   86.59,\n",
              "   84.29,\n",
              "   85.47,\n",
              "   86.27,\n",
              "   85.07,\n",
              "   86.61,\n",
              "   85.96,\n",
              "   85.99,\n",
              "   85.72,\n",
              "   86.05,\n",
              "   84.93,\n",
              "   86.52,\n",
              "   85.63,\n",
              "   85.14,\n",
              "   85.41,\n",
              "   86.03,\n",
              "   90.2,\n",
              "   90.57,\n",
              "   90.81,\n",
              "   90.9,\n",
              "   91.01,\n",
              "   90.97,\n",
              "   90.62,\n",
              "   90.68,\n",
              "   90.92,\n",
              "   90.9,\n",
              "   83.63,\n",
              "   84.01,\n",
              "   82.57,\n",
              "   85.63,\n",
              "   81.29,\n",
              "   85.27,\n",
              "   84.91,\n",
              "   85.81,\n",
              "   83.41,\n",
              "   81.7,\n",
              "   85.41,\n",
              "   86.15,\n",
              "   83.99,\n",
              "   83.5,\n",
              "   86.16,\n",
              "   85.99,\n",
              "   86.5,\n",
              "   84.18,\n",
              "   84.8,\n",
              "   85.8,\n",
              "   85.96,\n",
              "   85.72,\n",
              "   85.13,\n",
              "   85.66,\n",
              "   89.83,\n",
              "   90.09,\n",
              "   90.38,\n",
              "   90.38,\n",
              "   90.43,\n",
              "   90.56,\n",
              "   90.61,\n",
              "   90.53,\n",
              "   90.64,\n",
              "   90.58,\n",
              "   83.77,\n",
              "   82.95,\n",
              "   85.76,\n",
              "   84.82,\n",
              "   83.32,\n",
              "   85.02,\n",
              "   85.55,\n",
              "   86.23,\n",
              "   85.49,\n",
              "   85.14,\n",
              "   84.13,\n",
              "   84.93,\n",
              "   84.85,\n",
              "   85.88,\n",
              "   85.66,\n",
              "   85.14,\n",
              "   85.73,\n",
              "   85.74,\n",
              "   84.58,\n",
              "   85.94,\n",
              "   89.86,\n",
              "   90.43,\n",
              "   90.48,\n",
              "   90.55,\n",
              "   90.41,\n",
              "   90.74,\n",
              "   90.74,\n",
              "   90.41,\n",
              "   90.3,\n",
              "   90.49,\n",
              "   83.32,\n",
              "   83.88,\n",
              "   82.56,\n",
              "   85.09,\n",
              "   84.33,\n",
              "   84.74,\n",
              "   83.42,\n",
              "   82.12,\n",
              "   86.11,\n",
              "   86.25,\n",
              "   85.22,\n",
              "   85.43,\n",
              "   86.09,\n",
              "   85.35,\n",
              "   83.97,\n",
              "   83.45,\n",
              "   85.72,\n",
              "   85.83,\n",
              "   86.71,\n",
              "   85.95,\n",
              "   89.82,\n",
              "   90.36,\n",
              "   90.54,\n",
              "   90.51,\n",
              "   90.54,\n",
              "   90.52,\n",
              "   90.56,\n",
              "   90.5,\n",
              "   90.7,\n",
              "   90.67,\n",
              "   86.12,\n",
              "   84.75,\n",
              "   83.8,\n",
              "   85.22,\n",
              "   85.53,\n",
              "   85.53,\n",
              "   85.6,\n",
              "   83.95,\n",
              "   85.63,\n",
              "   84.39,\n",
              "   83.64,\n",
              "   83.96,\n",
              "   85.76,\n",
              "   83.33,\n",
              "   85.91,\n",
              "   86.67,\n",
              "   84.33,\n",
              "   85.82,\n",
              "   84.94,\n",
              "   86.4,\n",
              "   89.74,\n",
              "   90.11,\n",
              "   90.09,\n",
              "   90.31,\n",
              "   90.14,\n",
              "   90.53,\n",
              "   90.35,\n",
              "   90.61,\n",
              "   90.47,\n",
              "   90.63],\n",
              "  'training_loss': [0.38155910065174103,\n",
              "   0.33946473263204097,\n",
              "   0.33206032343804837,\n",
              "   0.31462940281629564,\n",
              "   0.32070257615447045,\n",
              "   0.31364221625328065,\n",
              "   0.30362944741249087,\n",
              "   0.30698678484410047,\n",
              "   0.2948304178386927,\n",
              "   0.2963203844085336,\n",
              "   0.294711550411582,\n",
              "   0.2935970535814762,\n",
              "   0.2875599007427692,\n",
              "   0.29316070586144927,\n",
              "   0.2794798585519195,\n",
              "   0.2841258435308933,\n",
              "   0.286539588958025,\n",
              "   0.2818945085734129,\n",
              "   0.28125950447320935,\n",
              "   0.2813090688481927,\n",
              "   0.15378602032661437,\n",
              "   0.11635532239489257,\n",
              "   0.09643237129040062,\n",
              "   0.08723186704218387,\n",
              "   0.07978326659817249,\n",
              "   0.0724008918164298,\n",
              "   0.0662930525271222,\n",
              "   0.061833755770139394,\n",
              "   0.0576664484555833,\n",
              "   0.05402257683547214,\n",
              "   0.3852047494977713,\n",
              "   0.34484966853260995,\n",
              "   0.334251786544919,\n",
              "   0.327029796230793,\n",
              "   0.45676373807787896,\n",
              "   0.36379065352678297,\n",
              "   0.34889577090740204,\n",
              "   0.33322349429428577,\n",
              "   0.33113222465515135,\n",
              "   0.32421154239773753,\n",
              "   0.32265920989215374,\n",
              "   0.31403127353489396,\n",
              "   0.31403420201539994,\n",
              "   0.308655017220974,\n",
              "   0.30307760660350325,\n",
              "   0.3035681668132544,\n",
              "   0.31059137663543224,\n",
              "   0.30375398650318386,\n",
              "   0.29288498572409155,\n",
              "   0.29997970818579195,\n",
              "   0.2850610995456576,\n",
              "   0.295754800593853,\n",
              "   0.2945524783551693,\n",
              "   0.2863565190374851,\n",
              "   0.16231757047325374,\n",
              "   0.11865447539947926,\n",
              "   0.10516013918593525,\n",
              "   0.09137856136057526,\n",
              "   0.08227975916303694,\n",
              "   0.07678717026859522,\n",
              "   0.07325530195776374,\n",
              "   0.0643363418508321,\n",
              "   0.06035936718769371,\n",
              "   0.05618587753158063,\n",
              "   0.5580356254041194,\n",
              "   0.3991969026386738,\n",
              "   0.3713805489808321,\n",
              "   0.3618467540025711,\n",
              "   0.3487018126994371,\n",
              "   0.3377587229937315,\n",
              "   0.33649390374720095,\n",
              "   0.331301762726903,\n",
              "   0.32513891161084174,\n",
              "   0.31870904984176157,\n",
              "   0.31612393091022967,\n",
              "   0.3136995284408331,\n",
              "   0.31568758649230005,\n",
              "   0.31074572643637655,\n",
              "   0.3052586601167917,\n",
              "   0.30674112515598534,\n",
              "   0.3092631254658103,\n",
              "   0.3030758010327816,\n",
              "   0.2984077451497316,\n",
              "   0.2947553270429373,\n",
              "   0.1677758205898106,\n",
              "   0.1277018541149795,\n",
              "   0.10837703995034098,\n",
              "   0.0977226862937212,\n",
              "   0.08901226596925407,\n",
              "   0.08268288986869156,\n",
              "   0.07796748800482602,\n",
              "   0.07178604435827583,\n",
              "   0.0685623920686543,\n",
              "   0.06428193959193304,\n",
              "   0.4079273605465889,\n",
              "   0.3540658381551504,\n",
              "   0.33332614375948905,\n",
              "   0.32534253038167954,\n",
              "   0.32141239524483683,\n",
              "   0.3130440088123083,\n",
              "   0.31074335777163503,\n",
              "   0.30931447079479696,\n",
              "   0.3087220321804285,\n",
              "   0.3069173827171326,\n",
              "   0.3015768424719572,\n",
              "   0.2932341114550829,\n",
              "   0.3041641746670008,\n",
              "   0.2909928483307362,\n",
              "   0.2908400336116552,\n",
              "   0.29654359705001115,\n",
              "   0.29154753260314464,\n",
              "   0.2888963930875063,\n",
              "   0.28773582755327226,\n",
              "   0.2837288407325745,\n",
              "   0.16135014606267215,\n",
              "   0.11978951680883765,\n",
              "   0.10250736006181688,\n",
              "   0.09077563301697374,\n",
              "   0.08144782977364957,\n",
              "   0.07824713298194111,\n",
              "   0.07122663903646172,\n",
              "   0.06521718141715974,\n",
              "   0.0637800672323443,\n",
              "   0.05701077757114544,\n",
              "   0.4604839712917805,\n",
              "   0.36821296733021736,\n",
              "   0.3521891108751297,\n",
              "   0.3480148832499981,\n",
              "   0.33442504706382753,\n",
              "   0.3257275182902813,\n",
              "   0.3231349487930536,\n",
              "   0.32497233298122885,\n",
              "   0.31234465238898995,\n",
              "   0.3140811449632049,\n",
              "   0.30756306006610395,\n",
              "   0.30973936368227006,\n",
              "   0.31035704487264154,\n",
              "   0.29961748715937137,\n",
              "   0.30401615715920927,\n",
              "   0.2944527658075094,\n",
              "   0.29180890017151834,\n",
              "   0.2993567759245634,\n",
              "   0.29215417718887327,\n",
              "   0.28927055866271256,\n",
              "   0.16083216299787165,\n",
              "   0.12251639947630465,\n",
              "   0.10589267364665866,\n",
              "   0.09528515082336962,\n",
              "   0.08652428435087205,\n",
              "   0.08206707478072495,\n",
              "   0.07237067394275218,\n",
              "   0.06901212050095201,\n",
              "   0.06361850416399538,\n",
              "   0.06038223183192313],\n",
              "  'valid_loss': [0.3902129514217377,\n",
              "   0.3745125361442566,\n",
              "   0.3691382401943207,\n",
              "   0.3867180294752121,\n",
              "   0.3596814860343933,\n",
              "   0.44843302898406984,\n",
              "   0.4195019269704819,\n",
              "   0.3959871680498123,\n",
              "   0.4079048013210297,\n",
              "   0.36777907435894014,\n",
              "   0.4405428479671478,\n",
              "   0.4010564106345177,\n",
              "   0.41002829401493074,\n",
              "   0.3869992198586464,\n",
              "   0.4208167046666145,\n",
              "   0.40034088065624235,\n",
              "   0.4387858188867569,\n",
              "   0.4541357951879501,\n",
              "   0.436925417971611,\n",
              "   0.42878594484329224,\n",
              "   0.28801884111166,\n",
              "   0.2827026910245419,\n",
              "   0.2713789153546095,\n",
              "   0.2749424152076244,\n",
              "   0.2897830685585737,\n",
              "   0.2857977539226413,\n",
              "   0.29150515076965094,\n",
              "   0.2864712070666254,\n",
              "   0.2969366750508547,\n",
              "   0.29037387744337323,\n",
              "   0.4423168306827545,\n",
              "   0.437982451069355,\n",
              "   0.44118181369304654,\n",
              "   0.37071692160367964,\n",
              "   0.4647326499462128,\n",
              "   0.377533415722847,\n",
              "   0.41474953594207764,\n",
              "   0.3770085090637207,\n",
              "   0.46206880307197573,\n",
              "   0.5135058022499085,\n",
              "   0.4312784754395485,\n",
              "   0.3775988023281097,\n",
              "   0.4885145141839981,\n",
              "   0.46643731937408445,\n",
              "   0.4043769103884697,\n",
              "   0.41200980584621427,\n",
              "   0.41479329483509064,\n",
              "   0.4682110832452774,\n",
              "   0.4529590367794037,\n",
              "   0.44030285118818285,\n",
              "   0.4116955370306969,\n",
              "   0.4445895838975906,\n",
              "   0.460952856862545,\n",
              "   0.42719808411598204,\n",
              "   0.2954306924164295,\n",
              "   0.2914474868178368,\n",
              "   0.28853924567699435,\n",
              "   0.2867657709747553,\n",
              "   0.2926275720745325,\n",
              "   0.2887160607755184,\n",
              "   0.29779333323538304,\n",
              "   0.2930128229893744,\n",
              "   0.2975225138902664,\n",
              "   0.31421767672747375,\n",
              "   0.4383423241853714,\n",
              "   0.461743790602684,\n",
              "   0.3844044865369797,\n",
              "   0.4332940185070038,\n",
              "   0.47880877890586854,\n",
              "   0.4469291090965271,\n",
              "   0.4051117587327957,\n",
              "   0.4076347698688507,\n",
              "   0.426863675403595,\n",
              "   0.4408632058620453,\n",
              "   0.48027661538124083,\n",
              "   0.45948316886425017,\n",
              "   0.44446558842658995,\n",
              "   0.4245792115688324,\n",
              "   0.4464360282540321,\n",
              "   0.43920491453409194,\n",
              "   0.43653895380496976,\n",
              "   0.4156890111327171,\n",
              "   0.47253578729629514,\n",
              "   0.43037066625356674,\n",
              "   0.2852649106025696,\n",
              "   0.2856877024635673,\n",
              "   0.2896685786098242,\n",
              "   0.28693220753967763,\n",
              "   0.28614758661985396,\n",
              "   0.29220747233629224,\n",
              "   0.2929576855868101,\n",
              "   0.2876923825129867,\n",
              "   0.30032676868885755,\n",
              "   0.30974006225913764,\n",
              "   0.4187791236877441,\n",
              "   0.4018106766104698,\n",
              "   0.4652509907484055,\n",
              "   0.40118082023859025,\n",
              "   0.46513295035362245,\n",
              "   0.4175166788101196,\n",
              "   0.4576169460296631,\n",
              "   0.540731411576271,\n",
              "   0.4185573099374771,\n",
              "   0.3991020019769669,\n",
              "   0.4241870040178299,\n",
              "   0.4432670653104782,\n",
              "   0.41662420592308047,\n",
              "   0.4335766587972641,\n",
              "   0.5080727669239045,\n",
              "   0.484168142414093,\n",
              "   0.4433573377370834,\n",
              "   0.432013666844368,\n",
              "   0.4236209304332733,\n",
              "   0.4350401842713356,\n",
              "   0.29148730807304385,\n",
              "   0.28856611856222153,\n",
              "   0.28308883257508277,\n",
              "   0.29095828539133073,\n",
              "   0.28445813535749914,\n",
              "   0.29199858210980895,\n",
              "   0.2953148814052343,\n",
              "   0.2968919693544507,\n",
              "   0.2979113065838814,\n",
              "   0.29900536466240885,\n",
              "   0.3585671618461609,\n",
              "   0.40215390717983246,\n",
              "   0.4420592936515808,\n",
              "   0.38922653814554214,\n",
              "   0.4037159222960472,\n",
              "   0.4089932821273804,\n",
              "   0.4173490993618965,\n",
              "   0.46846406474113467,\n",
              "   0.4042750144958496,\n",
              "   0.4439492600917816,\n",
              "   0.4893664236783981,\n",
              "   0.4862129561662674,\n",
              "   0.4133458930969238,\n",
              "   0.5020191180944443,\n",
              "   0.4316810423851013,\n",
              "   0.38711990263462065,\n",
              "   0.47905206220149993,\n",
              "   0.4117352591276169,\n",
              "   0.44760724906921384,\n",
              "   0.40987945516109464,\n",
              "   0.2855820753633976,\n",
              "   0.290034493970871,\n",
              "   0.2861618744432926,\n",
              "   0.2966159295350313,\n",
              "   0.30668378766775134,\n",
              "   0.29086014902740714,\n",
              "   0.30237287242114547,\n",
              "   0.29378735272362827,\n",
              "   0.31184847749471667,\n",
              "   0.3096024141073227]},\n",
              " '0.6 features.3': {'nbr_param': [510.0, 7182081.0],\n",
              "  'test_acc': [83.98,\n",
              "   85.75,\n",
              "   85.77,\n",
              "   85.73,\n",
              "   86.59,\n",
              "   84.29,\n",
              "   85.47,\n",
              "   86.27,\n",
              "   85.07,\n",
              "   86.61,\n",
              "   85.96,\n",
              "   85.99,\n",
              "   85.72,\n",
              "   86.05,\n",
              "   84.93,\n",
              "   86.52,\n",
              "   85.63,\n",
              "   85.14,\n",
              "   85.41,\n",
              "   86.03,\n",
              "   90.2,\n",
              "   90.57,\n",
              "   90.81,\n",
              "   90.9,\n",
              "   91.01,\n",
              "   90.97,\n",
              "   90.62,\n",
              "   90.68,\n",
              "   90.92,\n",
              "   90.9,\n",
              "   83.63,\n",
              "   84.01,\n",
              "   82.57,\n",
              "   85.63,\n",
              "   81.29,\n",
              "   85.27,\n",
              "   84.91,\n",
              "   85.81,\n",
              "   83.41,\n",
              "   81.7,\n",
              "   85.41,\n",
              "   86.15,\n",
              "   83.99,\n",
              "   83.5,\n",
              "   86.16,\n",
              "   85.99,\n",
              "   86.5,\n",
              "   84.18,\n",
              "   84.8,\n",
              "   85.8,\n",
              "   85.96,\n",
              "   85.72,\n",
              "   85.13,\n",
              "   85.66,\n",
              "   89.83,\n",
              "   90.09,\n",
              "   90.38,\n",
              "   90.38,\n",
              "   90.43,\n",
              "   90.56,\n",
              "   90.61,\n",
              "   90.53,\n",
              "   90.64,\n",
              "   90.58,\n",
              "   83.77,\n",
              "   82.95,\n",
              "   85.76,\n",
              "   84.82,\n",
              "   83.32,\n",
              "   85.02,\n",
              "   85.55,\n",
              "   86.23,\n",
              "   85.49,\n",
              "   85.14,\n",
              "   84.13,\n",
              "   84.93,\n",
              "   84.85,\n",
              "   85.88,\n",
              "   85.66,\n",
              "   85.14,\n",
              "   85.73,\n",
              "   85.74,\n",
              "   84.58,\n",
              "   85.94,\n",
              "   89.86,\n",
              "   90.43,\n",
              "   90.48,\n",
              "   90.55,\n",
              "   90.41,\n",
              "   90.74,\n",
              "   90.74,\n",
              "   90.41,\n",
              "   90.3,\n",
              "   90.49,\n",
              "   83.32,\n",
              "   83.88,\n",
              "   82.56,\n",
              "   85.09,\n",
              "   84.33,\n",
              "   84.74,\n",
              "   83.42,\n",
              "   82.12,\n",
              "   86.11,\n",
              "   86.25,\n",
              "   85.22,\n",
              "   85.43,\n",
              "   86.09,\n",
              "   85.35,\n",
              "   83.97,\n",
              "   83.45,\n",
              "   85.72,\n",
              "   85.83,\n",
              "   86.71,\n",
              "   85.95,\n",
              "   89.82,\n",
              "   90.36,\n",
              "   90.54,\n",
              "   90.51,\n",
              "   90.54,\n",
              "   90.52,\n",
              "   90.56,\n",
              "   90.5,\n",
              "   90.7,\n",
              "   90.67,\n",
              "   86.12,\n",
              "   84.75,\n",
              "   83.8,\n",
              "   85.22,\n",
              "   85.53,\n",
              "   85.53,\n",
              "   85.6,\n",
              "   83.95,\n",
              "   85.63,\n",
              "   84.39,\n",
              "   83.64,\n",
              "   83.96,\n",
              "   85.76,\n",
              "   83.33,\n",
              "   85.91,\n",
              "   86.67,\n",
              "   84.33,\n",
              "   85.82,\n",
              "   84.94,\n",
              "   86.4,\n",
              "   89.74,\n",
              "   90.11,\n",
              "   90.09,\n",
              "   90.31,\n",
              "   90.14,\n",
              "   90.53,\n",
              "   90.35,\n",
              "   90.61,\n",
              "   90.47,\n",
              "   90.63],\n",
              "  'training_loss': [0.38155910065174103,\n",
              "   0.33946473263204097,\n",
              "   0.33206032343804837,\n",
              "   0.31462940281629564,\n",
              "   0.32070257615447045,\n",
              "   0.31364221625328065,\n",
              "   0.30362944741249087,\n",
              "   0.30698678484410047,\n",
              "   0.2948304178386927,\n",
              "   0.2963203844085336,\n",
              "   0.294711550411582,\n",
              "   0.2935970535814762,\n",
              "   0.2875599007427692,\n",
              "   0.29316070586144927,\n",
              "   0.2794798585519195,\n",
              "   0.2841258435308933,\n",
              "   0.286539588958025,\n",
              "   0.2818945085734129,\n",
              "   0.28125950447320935,\n",
              "   0.2813090688481927,\n",
              "   0.15378602032661437,\n",
              "   0.11635532239489257,\n",
              "   0.09643237129040062,\n",
              "   0.08723186704218387,\n",
              "   0.07978326659817249,\n",
              "   0.0724008918164298,\n",
              "   0.0662930525271222,\n",
              "   0.061833755770139394,\n",
              "   0.0576664484555833,\n",
              "   0.05402257683547214,\n",
              "   0.3852047494977713,\n",
              "   0.34484966853260995,\n",
              "   0.334251786544919,\n",
              "   0.327029796230793,\n",
              "   0.45676373807787896,\n",
              "   0.36379065352678297,\n",
              "   0.34889577090740204,\n",
              "   0.33322349429428577,\n",
              "   0.33113222465515135,\n",
              "   0.32421154239773753,\n",
              "   0.32265920989215374,\n",
              "   0.31403127353489396,\n",
              "   0.31403420201539994,\n",
              "   0.308655017220974,\n",
              "   0.30307760660350325,\n",
              "   0.3035681668132544,\n",
              "   0.31059137663543224,\n",
              "   0.30375398650318386,\n",
              "   0.29288498572409155,\n",
              "   0.29997970818579195,\n",
              "   0.2850610995456576,\n",
              "   0.295754800593853,\n",
              "   0.2945524783551693,\n",
              "   0.2863565190374851,\n",
              "   0.16231757047325374,\n",
              "   0.11865447539947926,\n",
              "   0.10516013918593525,\n",
              "   0.09137856136057526,\n",
              "   0.08227975916303694,\n",
              "   0.07678717026859522,\n",
              "   0.07325530195776374,\n",
              "   0.0643363418508321,\n",
              "   0.06035936718769371,\n",
              "   0.05618587753158063,\n",
              "   0.5580356254041194,\n",
              "   0.3991969026386738,\n",
              "   0.3713805489808321,\n",
              "   0.3618467540025711,\n",
              "   0.3487018126994371,\n",
              "   0.3377587229937315,\n",
              "   0.33649390374720095,\n",
              "   0.331301762726903,\n",
              "   0.32513891161084174,\n",
              "   0.31870904984176157,\n",
              "   0.31612393091022967,\n",
              "   0.3136995284408331,\n",
              "   0.31568758649230005,\n",
              "   0.31074572643637655,\n",
              "   0.3052586601167917,\n",
              "   0.30674112515598534,\n",
              "   0.3092631254658103,\n",
              "   0.3030758010327816,\n",
              "   0.2984077451497316,\n",
              "   0.2947553270429373,\n",
              "   0.1677758205898106,\n",
              "   0.1277018541149795,\n",
              "   0.10837703995034098,\n",
              "   0.0977226862937212,\n",
              "   0.08901226596925407,\n",
              "   0.08268288986869156,\n",
              "   0.07796748800482602,\n",
              "   0.07178604435827583,\n",
              "   0.0685623920686543,\n",
              "   0.06428193959193304,\n",
              "   0.4079273605465889,\n",
              "   0.3540658381551504,\n",
              "   0.33332614375948905,\n",
              "   0.32534253038167954,\n",
              "   0.32141239524483683,\n",
              "   0.3130440088123083,\n",
              "   0.31074335777163503,\n",
              "   0.30931447079479696,\n",
              "   0.3087220321804285,\n",
              "   0.3069173827171326,\n",
              "   0.3015768424719572,\n",
              "   0.2932341114550829,\n",
              "   0.3041641746670008,\n",
              "   0.2909928483307362,\n",
              "   0.2908400336116552,\n",
              "   0.29654359705001115,\n",
              "   0.29154753260314464,\n",
              "   0.2888963930875063,\n",
              "   0.28773582755327226,\n",
              "   0.2837288407325745,\n",
              "   0.16135014606267215,\n",
              "   0.11978951680883765,\n",
              "   0.10250736006181688,\n",
              "   0.09077563301697374,\n",
              "   0.08144782977364957,\n",
              "   0.07824713298194111,\n",
              "   0.07122663903646172,\n",
              "   0.06521718141715974,\n",
              "   0.0637800672323443,\n",
              "   0.05701077757114544,\n",
              "   0.4604839712917805,\n",
              "   0.36821296733021736,\n",
              "   0.3521891108751297,\n",
              "   0.3480148832499981,\n",
              "   0.33442504706382753,\n",
              "   0.3257275182902813,\n",
              "   0.3231349487930536,\n",
              "   0.32497233298122885,\n",
              "   0.31234465238898995,\n",
              "   0.3140811449632049,\n",
              "   0.30756306006610395,\n",
              "   0.30973936368227006,\n",
              "   0.31035704487264154,\n",
              "   0.29961748715937137,\n",
              "   0.30401615715920927,\n",
              "   0.2944527658075094,\n",
              "   0.29180890017151834,\n",
              "   0.2993567759245634,\n",
              "   0.29215417718887327,\n",
              "   0.28927055866271256,\n",
              "   0.16083216299787165,\n",
              "   0.12251639947630465,\n",
              "   0.10589267364665866,\n",
              "   0.09528515082336962,\n",
              "   0.08652428435087205,\n",
              "   0.08206707478072495,\n",
              "   0.07237067394275218,\n",
              "   0.06901212050095201,\n",
              "   0.06361850416399538,\n",
              "   0.06038223183192313],\n",
              "  'valid_loss': [0.3902129514217377,\n",
              "   0.3745125361442566,\n",
              "   0.3691382401943207,\n",
              "   0.3867180294752121,\n",
              "   0.3596814860343933,\n",
              "   0.44843302898406984,\n",
              "   0.4195019269704819,\n",
              "   0.3959871680498123,\n",
              "   0.4079048013210297,\n",
              "   0.36777907435894014,\n",
              "   0.4405428479671478,\n",
              "   0.4010564106345177,\n",
              "   0.41002829401493074,\n",
              "   0.3869992198586464,\n",
              "   0.4208167046666145,\n",
              "   0.40034088065624235,\n",
              "   0.4387858188867569,\n",
              "   0.4541357951879501,\n",
              "   0.436925417971611,\n",
              "   0.42878594484329224,\n",
              "   0.28801884111166,\n",
              "   0.2827026910245419,\n",
              "   0.2713789153546095,\n",
              "   0.2749424152076244,\n",
              "   0.2897830685585737,\n",
              "   0.2857977539226413,\n",
              "   0.29150515076965094,\n",
              "   0.2864712070666254,\n",
              "   0.2969366750508547,\n",
              "   0.29037387744337323,\n",
              "   0.4423168306827545,\n",
              "   0.437982451069355,\n",
              "   0.44118181369304654,\n",
              "   0.37071692160367964,\n",
              "   0.4647326499462128,\n",
              "   0.377533415722847,\n",
              "   0.41474953594207764,\n",
              "   0.3770085090637207,\n",
              "   0.46206880307197573,\n",
              "   0.5135058022499085,\n",
              "   0.4312784754395485,\n",
              "   0.3775988023281097,\n",
              "   0.4885145141839981,\n",
              "   0.46643731937408445,\n",
              "   0.4043769103884697,\n",
              "   0.41200980584621427,\n",
              "   0.41479329483509064,\n",
              "   0.4682110832452774,\n",
              "   0.4529590367794037,\n",
              "   0.44030285118818285,\n",
              "   0.4116955370306969,\n",
              "   0.4445895838975906,\n",
              "   0.460952856862545,\n",
              "   0.42719808411598204,\n",
              "   0.2954306924164295,\n",
              "   0.2914474868178368,\n",
              "   0.28853924567699435,\n",
              "   0.2867657709747553,\n",
              "   0.2926275720745325,\n",
              "   0.2887160607755184,\n",
              "   0.29779333323538304,\n",
              "   0.2930128229893744,\n",
              "   0.2975225138902664,\n",
              "   0.31421767672747375,\n",
              "   0.4383423241853714,\n",
              "   0.461743790602684,\n",
              "   0.3844044865369797,\n",
              "   0.4332940185070038,\n",
              "   0.47880877890586854,\n",
              "   0.4469291090965271,\n",
              "   0.4051117587327957,\n",
              "   0.4076347698688507,\n",
              "   0.426863675403595,\n",
              "   0.4408632058620453,\n",
              "   0.48027661538124083,\n",
              "   0.45948316886425017,\n",
              "   0.44446558842658995,\n",
              "   0.4245792115688324,\n",
              "   0.4464360282540321,\n",
              "   0.43920491453409194,\n",
              "   0.43653895380496976,\n",
              "   0.4156890111327171,\n",
              "   0.47253578729629514,\n",
              "   0.43037066625356674,\n",
              "   0.2852649106025696,\n",
              "   0.2856877024635673,\n",
              "   0.2896685786098242,\n",
              "   0.28693220753967763,\n",
              "   0.28614758661985396,\n",
              "   0.29220747233629224,\n",
              "   0.2929576855868101,\n",
              "   0.2876923825129867,\n",
              "   0.30032676868885755,\n",
              "   0.30974006225913764,\n",
              "   0.4187791236877441,\n",
              "   0.4018106766104698,\n",
              "   0.4652509907484055,\n",
              "   0.40118082023859025,\n",
              "   0.46513295035362245,\n",
              "   0.4175166788101196,\n",
              "   0.4576169460296631,\n",
              "   0.540731411576271,\n",
              "   0.4185573099374771,\n",
              "   0.3991020019769669,\n",
              "   0.4241870040178299,\n",
              "   0.4432670653104782,\n",
              "   0.41662420592308047,\n",
              "   0.4335766587972641,\n",
              "   0.5080727669239045,\n",
              "   0.484168142414093,\n",
              "   0.4433573377370834,\n",
              "   0.432013666844368,\n",
              "   0.4236209304332733,\n",
              "   0.4350401842713356,\n",
              "   0.29148730807304385,\n",
              "   0.28856611856222153,\n",
              "   0.28308883257508277,\n",
              "   0.29095828539133073,\n",
              "   0.28445813535749914,\n",
              "   0.29199858210980895,\n",
              "   0.2953148814052343,\n",
              "   0.2968919693544507,\n",
              "   0.2979113065838814,\n",
              "   0.29900536466240885,\n",
              "   0.3585671618461609,\n",
              "   0.40215390717983246,\n",
              "   0.4420592936515808,\n",
              "   0.38922653814554214,\n",
              "   0.4037159222960472,\n",
              "   0.4089932821273804,\n",
              "   0.4173490993618965,\n",
              "   0.46846406474113467,\n",
              "   0.4042750144958496,\n",
              "   0.4439492600917816,\n",
              "   0.4893664236783981,\n",
              "   0.4862129561662674,\n",
              "   0.4133458930969238,\n",
              "   0.5020191180944443,\n",
              "   0.4316810423851013,\n",
              "   0.38711990263462065,\n",
              "   0.47905206220149993,\n",
              "   0.4117352591276169,\n",
              "   0.44760724906921384,\n",
              "   0.40987945516109464,\n",
              "   0.2855820753633976,\n",
              "   0.290034493970871,\n",
              "   0.2861618744432926,\n",
              "   0.2966159295350313,\n",
              "   0.30668378766775134,\n",
              "   0.29086014902740714,\n",
              "   0.30237287242114547,\n",
              "   0.29378735272362827,\n",
              "   0.31184847749471667,\n",
              "   0.3096024141073227]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zNts9frmqIm"
      },
      "source": [
        "### Prune sur \"features.7\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsgvS-0DplWV"
      },
      "source": [
        "prune_fine_tune(2,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCBnPUPlm9nq"
      },
      "source": [
        "### Prune sur \"features.10\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KqBaLkqRpl3q",
        "outputId": "7352877f-f761-4308-ebf9-e7a526052616"
      },
      "source": [
        "prune_fine_tune(3,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.10\n",
            "features.10\n",
            "7169931.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "84.85  % ,  0.37885906960964205  ,  0.3915827317610383\n",
            "epoch  1\n",
            "saving weights.... \n",
            "85.14  % ,  0.3813879222869873  ,  0.36015125019550326\n",
            "epoch  2\n",
            "saving weights.... \n",
            "85.97  % ,  0.36162704025506975  ,  0.3394492732614279\n",
            "epoch  3\n",
            "saving weights.... \n",
            "86.47  % ,  0.36307315928936007  ,  0.33276546038389204\n",
            "epoch  4\n",
            "saving weights.... \n",
            "85.85  % ,  0.373699124789238  ,  0.321261684307456\n",
            "epoch  5\n",
            "saving weights.... \n",
            "84.84  % ,  0.4199017300724983  ,  0.317406006988883\n",
            "epoch  6\n",
            "saving weights.... \n",
            "84.26  % ,  0.4386263092637062  ,  0.31472248481065035\n",
            "epoch  7\n",
            "saving weights.... \n",
            "86.4  % ,  0.37494810302257536  ,  0.31368526110053063\n",
            "epoch  8\n",
            "saving weights.... \n",
            "86.52  % ,  0.38244445865154264  ,  0.30694957921504973\n",
            "epoch  9\n",
            "saving weights.... \n",
            "86.11  % ,  0.39212416303157804  ,  0.30771915841698644\n",
            "epoch  10\n",
            "saving weights.... \n",
            "84.97  % ,  0.42861008958816527  ,  0.30400373374074696\n",
            "epoch  11\n",
            "saving weights.... \n",
            "85.45  % ,  0.40980375032424926  ,  0.3019208874642849\n",
            "epoch  12\n",
            "saving weights.... \n",
            "85.45  % ,  0.39386013095378875  ,  0.2940466021746397\n",
            "epoch  13\n",
            "saving weights.... \n",
            "85.88  % ,  0.39420974303483963  ,  0.29928265756517647\n",
            "epoch  14\n",
            "saving weights.... \n",
            "86.5  % ,  0.3733188946247101  ,  0.2930287833362818\n",
            "epoch  15\n",
            "saving weights.... \n",
            "86.6  % ,  0.39367760735750196  ,  0.2921968050956726\n",
            "epoch  16\n",
            "saving weights.... \n",
            "85.16  % ,  0.4251572137236595  ,  0.28996033757179973\n",
            "epoch  17\n",
            "saving weights.... \n",
            "86.44  % ,  0.387861485004425  ,  0.29818379101753234\n",
            "epoch  18\n",
            "saving weights.... \n",
            "84.81  % ,  0.4270662139892578  ,  0.28905609567165375\n",
            "epoch  19\n",
            "saving weights.... \n",
            "86.56  % ,  0.39478837317228316  ,  0.2829679045215249\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.24  % ,  0.2804021812051535  ,  0.1546595427915454\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.62  % ,  0.2687478130489588  ,  0.12215680264309049\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.87  % ,  0.2640162861764431  ,  0.10269908316135407\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.99  % ,  0.2640109902530909  ,  0.09301462987661362\n",
            "epoch  4\n",
            "saving weights.... \n",
            "91.08  % ,  0.2667842405796051  ,  0.08477825286388398\n",
            "epoch  5\n",
            "saving weights.... \n",
            "91.25  % ,  0.2732843278825283  ,  0.07879231826141476\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.89  % ,  0.2812460358470678  ,  0.07056481614094227\n",
            "epoch  7\n",
            "saving weights.... \n",
            "91.03  % ,  0.2759890424236655  ,  0.06797104831896722\n",
            "epoch  8\n",
            "saving weights.... \n",
            "91.26  % ,  0.2865777604825795  ,  0.0619820798965171\n",
            "epoch  9\n",
            "saving weights.... \n",
            "91.05  % ,  0.27957971667200326  ,  0.05927455047667027\n",
            "Finished Training\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.10\n",
            "features.10\n",
            "7145631.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "83.79  % ,  0.4035875594854355  ,  0.43319469038397074\n",
            "epoch  1\n",
            "saving weights.... \n",
            "86.59  % ,  0.34408994076251986  ,  0.3730352936834097\n",
            "epoch  2\n",
            "saving weights.... \n",
            "83.8  % ,  0.4150781086206436  ,  0.3572012962371111\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.63  % ,  0.3763122297525406  ,  0.3487765677303076\n",
            "epoch  4\n",
            "saving weights.... \n",
            "84.77  % ,  0.41687004276514056  ,  0.3396714374542236\n",
            "epoch  5\n",
            "saving weights.... \n",
            "85.62  % ,  0.3772286298036575  ,  0.3310719349384308\n",
            "epoch  6\n",
            "saving weights.... \n",
            "86.5  % ,  0.37435594547986983  ,  0.32411790349185465\n",
            "epoch  7\n",
            "saving weights.... \n",
            "84.33  % ,  0.436659460735321  ,  0.31850334607064723\n",
            "epoch  8\n",
            "saving weights.... \n",
            "84.8  % ,  0.4233722271308303  ,  0.3203605338037014\n",
            "epoch  9\n",
            "saving weights.... \n",
            "85.55  % ,  0.39898086166381835  ,  0.3191365311205387\n",
            "epoch  10\n",
            "saving weights.... \n",
            "85.46  % ,  0.4139880071401596  ,  0.3103590044379234\n",
            "epoch  11\n",
            "saving weights.... \n",
            "85.83  % ,  0.3978924362182617  ,  0.31409982268214226\n",
            "epoch  12\n",
            "saving weights.... \n",
            "83.2  % ,  0.5051281242370605  ,  0.3018649290859699\n",
            "epoch  13\n",
            "saving weights.... \n",
            "84.77  % ,  0.43509574440717697  ,  0.30651151714622976\n",
            "epoch  14\n",
            "saving weights.... \n",
            "83.36  % ,  0.47109321502447127  ,  0.30179595678150656\n",
            "epoch  15\n",
            "saving weights.... \n",
            "86.32  % ,  0.4048124081730843  ,  0.30144626151025294\n",
            "epoch  16\n",
            "saving weights.... \n",
            "85.44  % ,  0.42337273358106614  ,  0.30031773758530617\n",
            "epoch  17\n",
            "saving weights.... \n",
            "85.26  % ,  0.4663681983053684  ,  0.29617451620399954\n",
            "epoch  18\n",
            "saving weights.... \n",
            "86.57  % ,  0.4163111712932587  ,  0.2962040895164013\n",
            "epoch  19\n",
            "saving weights.... \n",
            "85.36  % ,  0.43114149227142334  ,  0.2949030253380537\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.06  % ,  0.27357843561172485  ,  0.16728212798833847\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.39  % ,  0.28219130519777536  ,  0.12332898807525634\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.44  % ,  0.2825168494999409  ,  0.10870779219828546\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.47  % ,  0.27767424177229405  ,  0.0953997129637748\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.95  % ,  0.2851933906078339  ,  0.08869926975779235\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.83  % ,  0.28464450816810133  ,  0.08311862417384983\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.93  % ,  0.2854661216765642  ,  0.07547869409192354\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.64  % ,  0.2778184846512973  ,  0.0719722601454705\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.81  % ,  0.28704436569809916  ,  0.0650316122174263\n",
            "epoch  9\n",
            "saving weights.... \n",
            "91.12  % ,  0.2970047707542777  ,  0.06117693085847423\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d3ab086b-0869-477e-a0c7-91d7b8145874\", \"features.10.npy\", 435)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.3 features.10': {'nbr_param': [510.0, 7169931.0],\n",
              "  'test_acc': [82.52,\n",
              "   85.01,\n",
              "   83.73,\n",
              "   85.0,\n",
              "   84.73,\n",
              "   85.54,\n",
              "   83.11,\n",
              "   86.34,\n",
              "   85.08,\n",
              "   84.93,\n",
              "   82.78,\n",
              "   84.63,\n",
              "   84.43,\n",
              "   84.6,\n",
              "   86.08,\n",
              "   85.62,\n",
              "   85.56,\n",
              "   85.69,\n",
              "   85.79,\n",
              "   83.26,\n",
              "   89.8,\n",
              "   89.89,\n",
              "   90.32,\n",
              "   90.63,\n",
              "   90.6,\n",
              "   90.71,\n",
              "   90.71,\n",
              "   90.78,\n",
              "   90.7,\n",
              "   90.6,\n",
              "   79.68,\n",
              "   81.94,\n",
              "   83.57,\n",
              "   82.51,\n",
              "   84.51,\n",
              "   83.98,\n",
              "   85.53,\n",
              "   84.25,\n",
              "   83.95,\n",
              "   84.22,\n",
              "   83.95,\n",
              "   85.34,\n",
              "   85.96,\n",
              "   84.3,\n",
              "   85.46,\n",
              "   86.59,\n",
              "   85.51,\n",
              "   85.05,\n",
              "   86.36,\n",
              "   84.16,\n",
              "   90.02,\n",
              "   90.22,\n",
              "   90.7,\n",
              "   90.59,\n",
              "   90.58,\n",
              "   90.48,\n",
              "   90.64,\n",
              "   90.22,\n",
              "   90.63,\n",
              "   90.55,\n",
              "   84.85,\n",
              "   85.14,\n",
              "   85.97,\n",
              "   86.47,\n",
              "   85.85,\n",
              "   84.84,\n",
              "   84.26,\n",
              "   86.4,\n",
              "   86.52,\n",
              "   86.11,\n",
              "   84.97,\n",
              "   85.45,\n",
              "   85.45,\n",
              "   85.88,\n",
              "   86.5,\n",
              "   86.6,\n",
              "   85.16,\n",
              "   86.44,\n",
              "   84.81,\n",
              "   86.56,\n",
              "   90.24,\n",
              "   90.62,\n",
              "   90.87,\n",
              "   90.99,\n",
              "   91.08,\n",
              "   91.25,\n",
              "   90.89,\n",
              "   91.03,\n",
              "   91.26,\n",
              "   91.05,\n",
              "   83.79,\n",
              "   86.59,\n",
              "   83.8,\n",
              "   85.63,\n",
              "   84.77,\n",
              "   85.62,\n",
              "   86.5,\n",
              "   84.33,\n",
              "   84.8,\n",
              "   85.55,\n",
              "   85.46,\n",
              "   85.83,\n",
              "   83.2,\n",
              "   84.77,\n",
              "   83.36,\n",
              "   86.32,\n",
              "   85.44,\n",
              "   85.26,\n",
              "   86.57,\n",
              "   85.36,\n",
              "   90.06,\n",
              "   90.39,\n",
              "   90.44,\n",
              "   90.47,\n",
              "   90.95,\n",
              "   90.83,\n",
              "   90.93,\n",
              "   90.64,\n",
              "   90.81,\n",
              "   91.12],\n",
              "  'training_loss': [0.40907566645145416,\n",
              "   0.360950541138649,\n",
              "   0.35040272166132924,\n",
              "   0.3473049049317837,\n",
              "   0.3275092122018337,\n",
              "   0.3349805044710636,\n",
              "   0.31958931329548357,\n",
              "   0.31514615920186045,\n",
              "   0.3107901745721698,\n",
              "   0.3090474818378687,\n",
              "   0.30886315927356484,\n",
              "   0.3099342880159616,\n",
              "   0.31027158932387827,\n",
              "   0.3020517555832863,\n",
              "   0.30246584529578685,\n",
              "   0.298972766559571,\n",
              "   0.2908178831547499,\n",
              "   0.30048629485815764,\n",
              "   0.2928548130407929,\n",
              "   0.2921842183619738,\n",
              "   0.16048257313892245,\n",
              "   0.12430203691832721,\n",
              "   0.10577896826770157,\n",
              "   0.09972059777788818,\n",
              "   0.09043372996971011,\n",
              "   0.08116793152783067,\n",
              "   0.07712968648150563,\n",
              "   0.0711976630795747,\n",
              "   0.0640951121110469,\n",
              "   0.06066714383419603,\n",
              "   0.6032445247888565,\n",
              "   0.4600371181309223,\n",
              "   0.42056222738623616,\n",
              "   0.40091566311120985,\n",
              "   0.39350112627148626,\n",
              "   0.3724687097400427,\n",
              "   0.37358303456008435,\n",
              "   0.36664005237817765,\n",
              "   0.3556244697600603,\n",
              "   0.3554153149843216,\n",
              "   0.35183879979252813,\n",
              "   0.3447562878936529,\n",
              "   0.3385072982698679,\n",
              "   0.3340740914642811,\n",
              "   0.3333474228441715,\n",
              "   0.3252881019204855,\n",
              "   0.3247955938056111,\n",
              "   0.324323251709342,\n",
              "   0.3257295282006264,\n",
              "   0.3136143458515406,\n",
              "   0.18703131733089687,\n",
              "   0.14455435569658875,\n",
              "   0.12962016634270548,\n",
              "   0.11906005847752094,\n",
              "   0.1089113010328263,\n",
              "   0.1044217544157058,\n",
              "   0.09653020181171595,\n",
              "   0.09167640583980828,\n",
              "   0.08709070926774293,\n",
              "   0.07952612738590688,\n",
              "   0.3915827317610383,\n",
              "   0.36015125019550326,\n",
              "   0.3394492732614279,\n",
              "   0.33276546038389204,\n",
              "   0.321261684307456,\n",
              "   0.317406006988883,\n",
              "   0.31472248481065035,\n",
              "   0.31368526110053063,\n",
              "   0.30694957921504973,\n",
              "   0.30771915841698644,\n",
              "   0.30400373374074696,\n",
              "   0.3019208874642849,\n",
              "   0.2940466021746397,\n",
              "   0.29928265756517647,\n",
              "   0.2930287833362818,\n",
              "   0.2921968050956726,\n",
              "   0.28996033757179973,\n",
              "   0.29818379101753234,\n",
              "   0.28905609567165375,\n",
              "   0.2829679045215249,\n",
              "   0.1546595427915454,\n",
              "   0.12215680264309049,\n",
              "   0.10269908316135407,\n",
              "   0.09301462987661362,\n",
              "   0.08477825286388398,\n",
              "   0.07879231826141476,\n",
              "   0.07056481614094227,\n",
              "   0.06797104831896722,\n",
              "   0.0619820798965171,\n",
              "   0.05927455047667027,\n",
              "   0.43319469038397074,\n",
              "   0.3730352936834097,\n",
              "   0.3572012962371111,\n",
              "   0.3487765677303076,\n",
              "   0.3396714374542236,\n",
              "   0.3310719349384308,\n",
              "   0.32411790349185465,\n",
              "   0.31850334607064723,\n",
              "   0.3203605338037014,\n",
              "   0.3191365311205387,\n",
              "   0.3103590044379234,\n",
              "   0.31409982268214226,\n",
              "   0.3018649290859699,\n",
              "   0.30651151714622976,\n",
              "   0.30179595678150656,\n",
              "   0.30144626151025294,\n",
              "   0.30031773758530617,\n",
              "   0.29617451620399954,\n",
              "   0.2962040895164013,\n",
              "   0.2949030253380537,\n",
              "   0.16728212798833847,\n",
              "   0.12332898807525634,\n",
              "   0.10870779219828546,\n",
              "   0.0953997129637748,\n",
              "   0.08869926975779235,\n",
              "   0.08311862417384983,\n",
              "   0.07547869409192354,\n",
              "   0.0719722601454705,\n",
              "   0.0650316122174263,\n",
              "   0.06117693085847423],\n",
              "  'valid_loss': [0.42210225051641465,\n",
              "   0.35383942351341247,\n",
              "   0.4297317171573639,\n",
              "   0.4053603218317032,\n",
              "   0.4291050954580307,\n",
              "   0.3971800770521164,\n",
              "   0.4493438600540161,\n",
              "   0.3755212651491165,\n",
              "   0.4161437240600586,\n",
              "   0.40946616864204405,\n",
              "   0.4856018295288086,\n",
              "   0.43796557857990265,\n",
              "   0.46040558100938794,\n",
              "   0.4405988154411316,\n",
              "   0.4151842958331108,\n",
              "   0.44308626794815065,\n",
              "   0.422457066488266,\n",
              "   0.40483203476667406,\n",
              "   0.40512289390563966,\n",
              "   0.49783689572811124,\n",
              "   0.28154569994807244,\n",
              "   0.27318392246067524,\n",
              "   0.2721190889239311,\n",
              "   0.268596945810318,\n",
              "   0.27571865207254886,\n",
              "   0.2710410477638245,\n",
              "   0.2686416128218174,\n",
              "   0.2839177329212427,\n",
              "   0.28436483749076724,\n",
              "   0.2866449775144458,\n",
              "   0.6017296772003173,\n",
              "   0.5101664099693298,\n",
              "   0.44742883722782134,\n",
              "   0.5044509061813355,\n",
              "   0.4397493969917297,\n",
              "   0.4753770266056061,\n",
              "   0.4013705028295517,\n",
              "   0.44403475792407987,\n",
              "   0.4547236580371857,\n",
              "   0.44203361523151397,\n",
              "   0.4756121888399124,\n",
              "   0.439184329867363,\n",
              "   0.4061865078806877,\n",
              "   0.508562934923172,\n",
              "   0.44241422488689425,\n",
              "   0.3968075229883194,\n",
              "   0.4381755865812302,\n",
              "   0.4360711445331574,\n",
              "   0.43369366626739503,\n",
              "   0.4615786414384842,\n",
              "   0.2850886409282684,\n",
              "   0.2834207551896572,\n",
              "   0.2861843703627586,\n",
              "   0.28229024144113063,\n",
              "   0.2847004294723272,\n",
              "   0.2934260986864567,\n",
              "   0.2896845830738545,\n",
              "   0.28826696096211674,\n",
              "   0.2827771405771375,\n",
              "   0.29946278535425663,\n",
              "   0.37885906960964205,\n",
              "   0.3813879222869873,\n",
              "   0.36162704025506975,\n",
              "   0.36307315928936007,\n",
              "   0.373699124789238,\n",
              "   0.4199017300724983,\n",
              "   0.4386263092637062,\n",
              "   0.37494810302257536,\n",
              "   0.38244445865154264,\n",
              "   0.39212416303157804,\n",
              "   0.42861008958816527,\n",
              "   0.40980375032424926,\n",
              "   0.39386013095378875,\n",
              "   0.39420974303483963,\n",
              "   0.3733188946247101,\n",
              "   0.39367760735750196,\n",
              "   0.4251572137236595,\n",
              "   0.387861485004425,\n",
              "   0.4270662139892578,\n",
              "   0.39478837317228316,\n",
              "   0.2804021812051535,\n",
              "   0.2687478130489588,\n",
              "   0.2640162861764431,\n",
              "   0.2640109902530909,\n",
              "   0.2667842405796051,\n",
              "   0.2732843278825283,\n",
              "   0.2812460358470678,\n",
              "   0.2759890424236655,\n",
              "   0.2865777604825795,\n",
              "   0.27957971667200326,\n",
              "   0.4035875594854355,\n",
              "   0.34408994076251986,\n",
              "   0.4150781086206436,\n",
              "   0.3763122297525406,\n",
              "   0.41687004276514056,\n",
              "   0.3772286298036575,\n",
              "   0.37435594547986983,\n",
              "   0.436659460735321,\n",
              "   0.4233722271308303,\n",
              "   0.39898086166381835,\n",
              "   0.4139880071401596,\n",
              "   0.3978924362182617,\n",
              "   0.5051281242370605,\n",
              "   0.43509574440717697,\n",
              "   0.47109321502447127,\n",
              "   0.4048124081730843,\n",
              "   0.42337273358106614,\n",
              "   0.4663681983053684,\n",
              "   0.4163111712932587,\n",
              "   0.43114149227142334,\n",
              "   0.27357843561172485,\n",
              "   0.28219130519777536,\n",
              "   0.2825168494999409,\n",
              "   0.27767424177229405,\n",
              "   0.2851933906078339,\n",
              "   0.28464450816810133,\n",
              "   0.2854661216765642,\n",
              "   0.2778184846512973,\n",
              "   0.28704436569809916,\n",
              "   0.2970047707542777]},\n",
              " '0.6 features.10': {'nbr_param': [510.0, 7145631.0],\n",
              "  'test_acc': [82.52,\n",
              "   85.01,\n",
              "   83.73,\n",
              "   85.0,\n",
              "   84.73,\n",
              "   85.54,\n",
              "   83.11,\n",
              "   86.34,\n",
              "   85.08,\n",
              "   84.93,\n",
              "   82.78,\n",
              "   84.63,\n",
              "   84.43,\n",
              "   84.6,\n",
              "   86.08,\n",
              "   85.62,\n",
              "   85.56,\n",
              "   85.69,\n",
              "   85.79,\n",
              "   83.26,\n",
              "   89.8,\n",
              "   89.89,\n",
              "   90.32,\n",
              "   90.63,\n",
              "   90.6,\n",
              "   90.71,\n",
              "   90.71,\n",
              "   90.78,\n",
              "   90.7,\n",
              "   90.6,\n",
              "   79.68,\n",
              "   81.94,\n",
              "   83.57,\n",
              "   82.51,\n",
              "   84.51,\n",
              "   83.98,\n",
              "   85.53,\n",
              "   84.25,\n",
              "   83.95,\n",
              "   84.22,\n",
              "   83.95,\n",
              "   85.34,\n",
              "   85.96,\n",
              "   84.3,\n",
              "   85.46,\n",
              "   86.59,\n",
              "   85.51,\n",
              "   85.05,\n",
              "   86.36,\n",
              "   84.16,\n",
              "   90.02,\n",
              "   90.22,\n",
              "   90.7,\n",
              "   90.59,\n",
              "   90.58,\n",
              "   90.48,\n",
              "   90.64,\n",
              "   90.22,\n",
              "   90.63,\n",
              "   90.55,\n",
              "   84.85,\n",
              "   85.14,\n",
              "   85.97,\n",
              "   86.47,\n",
              "   85.85,\n",
              "   84.84,\n",
              "   84.26,\n",
              "   86.4,\n",
              "   86.52,\n",
              "   86.11,\n",
              "   84.97,\n",
              "   85.45,\n",
              "   85.45,\n",
              "   85.88,\n",
              "   86.5,\n",
              "   86.6,\n",
              "   85.16,\n",
              "   86.44,\n",
              "   84.81,\n",
              "   86.56,\n",
              "   90.24,\n",
              "   90.62,\n",
              "   90.87,\n",
              "   90.99,\n",
              "   91.08,\n",
              "   91.25,\n",
              "   90.89,\n",
              "   91.03,\n",
              "   91.26,\n",
              "   91.05,\n",
              "   83.79,\n",
              "   86.59,\n",
              "   83.8,\n",
              "   85.63,\n",
              "   84.77,\n",
              "   85.62,\n",
              "   86.5,\n",
              "   84.33,\n",
              "   84.8,\n",
              "   85.55,\n",
              "   85.46,\n",
              "   85.83,\n",
              "   83.2,\n",
              "   84.77,\n",
              "   83.36,\n",
              "   86.32,\n",
              "   85.44,\n",
              "   85.26,\n",
              "   86.57,\n",
              "   85.36,\n",
              "   90.06,\n",
              "   90.39,\n",
              "   90.44,\n",
              "   90.47,\n",
              "   90.95,\n",
              "   90.83,\n",
              "   90.93,\n",
              "   90.64,\n",
              "   90.81,\n",
              "   91.12],\n",
              "  'training_loss': [0.40907566645145416,\n",
              "   0.360950541138649,\n",
              "   0.35040272166132924,\n",
              "   0.3473049049317837,\n",
              "   0.3275092122018337,\n",
              "   0.3349805044710636,\n",
              "   0.31958931329548357,\n",
              "   0.31514615920186045,\n",
              "   0.3107901745721698,\n",
              "   0.3090474818378687,\n",
              "   0.30886315927356484,\n",
              "   0.3099342880159616,\n",
              "   0.31027158932387827,\n",
              "   0.3020517555832863,\n",
              "   0.30246584529578685,\n",
              "   0.298972766559571,\n",
              "   0.2908178831547499,\n",
              "   0.30048629485815764,\n",
              "   0.2928548130407929,\n",
              "   0.2921842183619738,\n",
              "   0.16048257313892245,\n",
              "   0.12430203691832721,\n",
              "   0.10577896826770157,\n",
              "   0.09972059777788818,\n",
              "   0.09043372996971011,\n",
              "   0.08116793152783067,\n",
              "   0.07712968648150563,\n",
              "   0.0711976630795747,\n",
              "   0.0640951121110469,\n",
              "   0.06066714383419603,\n",
              "   0.6032445247888565,\n",
              "   0.4600371181309223,\n",
              "   0.42056222738623616,\n",
              "   0.40091566311120985,\n",
              "   0.39350112627148626,\n",
              "   0.3724687097400427,\n",
              "   0.37358303456008435,\n",
              "   0.36664005237817765,\n",
              "   0.3556244697600603,\n",
              "   0.3554153149843216,\n",
              "   0.35183879979252813,\n",
              "   0.3447562878936529,\n",
              "   0.3385072982698679,\n",
              "   0.3340740914642811,\n",
              "   0.3333474228441715,\n",
              "   0.3252881019204855,\n",
              "   0.3247955938056111,\n",
              "   0.324323251709342,\n",
              "   0.3257295282006264,\n",
              "   0.3136143458515406,\n",
              "   0.18703131733089687,\n",
              "   0.14455435569658875,\n",
              "   0.12962016634270548,\n",
              "   0.11906005847752094,\n",
              "   0.1089113010328263,\n",
              "   0.1044217544157058,\n",
              "   0.09653020181171595,\n",
              "   0.09167640583980828,\n",
              "   0.08709070926774293,\n",
              "   0.07952612738590688,\n",
              "   0.3915827317610383,\n",
              "   0.36015125019550326,\n",
              "   0.3394492732614279,\n",
              "   0.33276546038389204,\n",
              "   0.321261684307456,\n",
              "   0.317406006988883,\n",
              "   0.31472248481065035,\n",
              "   0.31368526110053063,\n",
              "   0.30694957921504973,\n",
              "   0.30771915841698644,\n",
              "   0.30400373374074696,\n",
              "   0.3019208874642849,\n",
              "   0.2940466021746397,\n",
              "   0.29928265756517647,\n",
              "   0.2930287833362818,\n",
              "   0.2921968050956726,\n",
              "   0.28996033757179973,\n",
              "   0.29818379101753234,\n",
              "   0.28905609567165375,\n",
              "   0.2829679045215249,\n",
              "   0.1546595427915454,\n",
              "   0.12215680264309049,\n",
              "   0.10269908316135407,\n",
              "   0.09301462987661362,\n",
              "   0.08477825286388398,\n",
              "   0.07879231826141476,\n",
              "   0.07056481614094227,\n",
              "   0.06797104831896722,\n",
              "   0.0619820798965171,\n",
              "   0.05927455047667027,\n",
              "   0.43319469038397074,\n",
              "   0.3730352936834097,\n",
              "   0.3572012962371111,\n",
              "   0.3487765677303076,\n",
              "   0.3396714374542236,\n",
              "   0.3310719349384308,\n",
              "   0.32411790349185465,\n",
              "   0.31850334607064723,\n",
              "   0.3203605338037014,\n",
              "   0.3191365311205387,\n",
              "   0.3103590044379234,\n",
              "   0.31409982268214226,\n",
              "   0.3018649290859699,\n",
              "   0.30651151714622976,\n",
              "   0.30179595678150656,\n",
              "   0.30144626151025294,\n",
              "   0.30031773758530617,\n",
              "   0.29617451620399954,\n",
              "   0.2962040895164013,\n",
              "   0.2949030253380537,\n",
              "   0.16728212798833847,\n",
              "   0.12332898807525634,\n",
              "   0.10870779219828546,\n",
              "   0.0953997129637748,\n",
              "   0.08869926975779235,\n",
              "   0.08311862417384983,\n",
              "   0.07547869409192354,\n",
              "   0.0719722601454705,\n",
              "   0.0650316122174263,\n",
              "   0.06117693085847423],\n",
              "  'valid_loss': [0.42210225051641465,\n",
              "   0.35383942351341247,\n",
              "   0.4297317171573639,\n",
              "   0.4053603218317032,\n",
              "   0.4291050954580307,\n",
              "   0.3971800770521164,\n",
              "   0.4493438600540161,\n",
              "   0.3755212651491165,\n",
              "   0.4161437240600586,\n",
              "   0.40946616864204405,\n",
              "   0.4856018295288086,\n",
              "   0.43796557857990265,\n",
              "   0.46040558100938794,\n",
              "   0.4405988154411316,\n",
              "   0.4151842958331108,\n",
              "   0.44308626794815065,\n",
              "   0.422457066488266,\n",
              "   0.40483203476667406,\n",
              "   0.40512289390563966,\n",
              "   0.49783689572811124,\n",
              "   0.28154569994807244,\n",
              "   0.27318392246067524,\n",
              "   0.2721190889239311,\n",
              "   0.268596945810318,\n",
              "   0.27571865207254886,\n",
              "   0.2710410477638245,\n",
              "   0.2686416128218174,\n",
              "   0.2839177329212427,\n",
              "   0.28436483749076724,\n",
              "   0.2866449775144458,\n",
              "   0.6017296772003173,\n",
              "   0.5101664099693298,\n",
              "   0.44742883722782134,\n",
              "   0.5044509061813355,\n",
              "   0.4397493969917297,\n",
              "   0.4753770266056061,\n",
              "   0.4013705028295517,\n",
              "   0.44403475792407987,\n",
              "   0.4547236580371857,\n",
              "   0.44203361523151397,\n",
              "   0.4756121888399124,\n",
              "   0.439184329867363,\n",
              "   0.4061865078806877,\n",
              "   0.508562934923172,\n",
              "   0.44241422488689425,\n",
              "   0.3968075229883194,\n",
              "   0.4381755865812302,\n",
              "   0.4360711445331574,\n",
              "   0.43369366626739503,\n",
              "   0.4615786414384842,\n",
              "   0.2850886409282684,\n",
              "   0.2834207551896572,\n",
              "   0.2861843703627586,\n",
              "   0.28229024144113063,\n",
              "   0.2847004294723272,\n",
              "   0.2934260986864567,\n",
              "   0.2896845830738545,\n",
              "   0.28826696096211674,\n",
              "   0.2827771405771375,\n",
              "   0.29946278535425663,\n",
              "   0.37885906960964205,\n",
              "   0.3813879222869873,\n",
              "   0.36162704025506975,\n",
              "   0.36307315928936007,\n",
              "   0.373699124789238,\n",
              "   0.4199017300724983,\n",
              "   0.4386263092637062,\n",
              "   0.37494810302257536,\n",
              "   0.38244445865154264,\n",
              "   0.39212416303157804,\n",
              "   0.42861008958816527,\n",
              "   0.40980375032424926,\n",
              "   0.39386013095378875,\n",
              "   0.39420974303483963,\n",
              "   0.3733188946247101,\n",
              "   0.39367760735750196,\n",
              "   0.4251572137236595,\n",
              "   0.387861485004425,\n",
              "   0.4270662139892578,\n",
              "   0.39478837317228316,\n",
              "   0.2804021812051535,\n",
              "   0.2687478130489588,\n",
              "   0.2640162861764431,\n",
              "   0.2640109902530909,\n",
              "   0.2667842405796051,\n",
              "   0.2732843278825283,\n",
              "   0.2812460358470678,\n",
              "   0.2759890424236655,\n",
              "   0.2865777604825795,\n",
              "   0.27957971667200326,\n",
              "   0.4035875594854355,\n",
              "   0.34408994076251986,\n",
              "   0.4150781086206436,\n",
              "   0.3763122297525406,\n",
              "   0.41687004276514056,\n",
              "   0.3772286298036575,\n",
              "   0.37435594547986983,\n",
              "   0.436659460735321,\n",
              "   0.4233722271308303,\n",
              "   0.39898086166381835,\n",
              "   0.4139880071401596,\n",
              "   0.3978924362182617,\n",
              "   0.5051281242370605,\n",
              "   0.43509574440717697,\n",
              "   0.47109321502447127,\n",
              "   0.4048124081730843,\n",
              "   0.42337273358106614,\n",
              "   0.4663681983053684,\n",
              "   0.4163111712932587,\n",
              "   0.43114149227142334,\n",
              "   0.27357843561172485,\n",
              "   0.28219130519777536,\n",
              "   0.2825168494999409,\n",
              "   0.27767424177229405,\n",
              "   0.2851933906078339,\n",
              "   0.28464450816810133,\n",
              "   0.2854661216765642,\n",
              "   0.2778184846512973,\n",
              "   0.28704436569809916,\n",
              "   0.2970047707542777]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-l782Lfm9rG"
      },
      "source": [
        "### Prune sur \"features.14\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SFO3JzdfpmUm",
        "outputId": "3686668c-5492-4e9e-a733-c2ce6faffe57"
      },
      "source": [
        "prune_fine_tune(4,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chargez les poids\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-597569b9-ff58-4622-a6e0-e33fdad60b8a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-597569b9-ff58-4622-a6e0-e33fdad60b8a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving checkpoint.pt to checkpoint.pt\n",
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.14\n",
            "features.14\n",
            "7145901.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "82.52  % ,  0.42210225051641465  ,  0.40907566645145416\n",
            "epoch  1\n",
            "saving weights.... \n",
            "85.01  % ,  0.35383942351341247  ,  0.360950541138649\n",
            "epoch  2\n",
            "saving weights.... \n",
            "83.73  % ,  0.4297317171573639  ,  0.35040272166132924\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.0  % ,  0.4053603218317032  ,  0.3473049049317837\n",
            "epoch  4\n",
            "saving weights.... \n",
            "84.73  % ,  0.4291050954580307  ,  0.3275092122018337\n",
            "epoch  5\n",
            "saving weights.... \n",
            "85.54  % ,  0.3971800770521164  ,  0.3349805044710636\n",
            "epoch  6\n",
            "saving weights.... \n",
            "83.11  % ,  0.4493438600540161  ,  0.31958931329548357\n",
            "epoch  7\n",
            "saving weights.... \n",
            "86.34  % ,  0.3755212651491165  ,  0.31514615920186045\n",
            "epoch  8\n",
            "saving weights.... \n",
            "85.08  % ,  0.4161437240600586  ,  0.3107901745721698\n",
            "epoch  9\n",
            "saving weights.... \n",
            "84.93  % ,  0.40946616864204405  ,  0.3090474818378687\n",
            "epoch  10\n",
            "saving weights.... \n",
            "82.78  % ,  0.4856018295288086  ,  0.30886315927356484\n",
            "epoch  11\n",
            "saving weights.... \n",
            "84.63  % ,  0.43796557857990265  ,  0.3099342880159616\n",
            "epoch  12\n",
            "saving weights.... \n",
            "84.43  % ,  0.46040558100938794  ,  0.31027158932387827\n",
            "epoch  13\n",
            "saving weights.... \n",
            "84.6  % ,  0.4405988154411316  ,  0.3020517555832863\n",
            "epoch  14\n",
            "saving weights.... \n",
            "86.08  % ,  0.4151842958331108  ,  0.30246584529578685\n",
            "epoch  15\n",
            "saving weights.... \n",
            "85.62  % ,  0.44308626794815065  ,  0.298972766559571\n",
            "epoch  16\n",
            "saving weights.... \n",
            "85.56  % ,  0.422457066488266  ,  0.2908178831547499\n",
            "epoch  17\n",
            "saving weights.... \n",
            "85.69  % ,  0.40483203476667406  ,  0.30048629485815764\n",
            "epoch  18\n",
            "saving weights.... \n",
            "85.79  % ,  0.40512289390563966  ,  0.2928548130407929\n",
            "epoch  19\n",
            "saving weights.... \n",
            "83.26  % ,  0.49783689572811124  ,  0.2921842183619738\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "89.8  % ,  0.28154569994807244  ,  0.16048257313892245\n",
            "epoch  1\n",
            "saving weights.... \n",
            "89.89  % ,  0.27318392246067524  ,  0.12430203691832721\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.32  % ,  0.2721190889239311  ,  0.10577896826770157\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.63  % ,  0.268596945810318  ,  0.09972059777788818\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.6  % ,  0.27571865207254886  ,  0.09043372996971011\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.71  % ,  0.2710410477638245  ,  0.08116793152783067\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.71  % ,  0.2686416128218174  ,  0.07712968648150563\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.78  % ,  0.2839177329212427  ,  0.0711976630795747\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.7  % ,  0.28436483749076724  ,  0.0640951121110469\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.6  % ,  0.2866449775144458  ,  0.06066714383419603\n",
            "Finished Training\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.14\n",
            "features.14\n",
            "7097571.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "79.68  % ,  0.6017296772003173  ,  0.6032445247888565\n",
            "epoch  1\n",
            "saving weights.... \n",
            "81.94  % ,  0.5101664099693298  ,  0.4600371181309223\n",
            "epoch  2\n",
            "saving weights.... \n",
            "83.57  % ,  0.44742883722782134  ,  0.42056222738623616\n",
            "epoch  3\n",
            "saving weights.... \n",
            "82.51  % ,  0.5044509061813355  ,  0.40091566311120985\n",
            "epoch  4\n",
            "saving weights.... \n",
            "84.51  % ,  0.4397493969917297  ,  0.39350112627148626\n",
            "epoch  5\n",
            "saving weights.... \n",
            "83.98  % ,  0.4753770266056061  ,  0.3724687097400427\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.53  % ,  0.4013705028295517  ,  0.37358303456008435\n",
            "epoch  7\n",
            "saving weights.... \n",
            "84.25  % ,  0.44403475792407987  ,  0.36664005237817765\n",
            "epoch  8\n",
            "saving weights.... \n",
            "83.95  % ,  0.4547236580371857  ,  0.3556244697600603\n",
            "epoch  9\n",
            "saving weights.... \n",
            "84.22  % ,  0.44203361523151397  ,  0.3554153149843216\n",
            "epoch  10\n",
            "saving weights.... \n",
            "83.95  % ,  0.4756121888399124  ,  0.35183879979252813\n",
            "epoch  11\n",
            "saving weights.... \n",
            "85.34  % ,  0.439184329867363  ,  0.3447562878936529\n",
            "epoch  12\n",
            "saving weights.... \n",
            "85.96  % ,  0.4061865078806877  ,  0.3385072982698679\n",
            "epoch  13\n",
            "saving weights.... \n",
            "84.3  % ,  0.508562934923172  ,  0.3340740914642811\n",
            "epoch  14\n",
            "saving weights.... \n",
            "85.46  % ,  0.44241422488689425  ,  0.3333474228441715\n",
            "epoch  15\n",
            "saving weights.... \n",
            "86.59  % ,  0.3968075229883194  ,  0.3252881019204855\n",
            "epoch  16\n",
            "saving weights.... \n",
            "85.51  % ,  0.4381755865812302  ,  0.3247955938056111\n",
            "epoch  17\n",
            "saving weights.... \n",
            "85.05  % ,  0.4360711445331574  ,  0.324323251709342\n",
            "epoch  18\n",
            "saving weights.... \n",
            "86.36  % ,  0.43369366626739503  ,  0.3257295282006264\n",
            "epoch  19\n",
            "saving weights.... \n",
            "84.16  % ,  0.4615786414384842  ,  0.3136143458515406\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.02  % ,  0.2850886409282684  ,  0.18703131733089687\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.22  % ,  0.2834207551896572  ,  0.14455435569658875\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.7  % ,  0.2861843703627586  ,  0.12962016634270548\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.59  % ,  0.28229024144113063  ,  0.11906005847752094\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.58  % ,  0.2847004294723272  ,  0.1089113010328263\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.48  % ,  0.2934260986864567  ,  0.1044217544157058\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.64  % ,  0.2896845830738545  ,  0.09653020181171595\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.22  % ,  0.28826696096211674  ,  0.09167640583980828\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.63  % ,  0.2827771405771375  ,  0.08709070926774293\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.55  % ,  0.29946278535425663  ,  0.07952612738590688\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_cd5c8301-f201-4880-944c-7f1d9a81ba90\", \"features.14.npy\", 435)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.3 features.14': {'nbr_param': [510.0, 7145901.0],\n",
              "  'test_acc': [82.52,\n",
              "   85.01,\n",
              "   83.73,\n",
              "   85.0,\n",
              "   84.73,\n",
              "   85.54,\n",
              "   83.11,\n",
              "   86.34,\n",
              "   85.08,\n",
              "   84.93,\n",
              "   82.78,\n",
              "   84.63,\n",
              "   84.43,\n",
              "   84.6,\n",
              "   86.08,\n",
              "   85.62,\n",
              "   85.56,\n",
              "   85.69,\n",
              "   85.79,\n",
              "   83.26,\n",
              "   89.8,\n",
              "   89.89,\n",
              "   90.32,\n",
              "   90.63,\n",
              "   90.6,\n",
              "   90.71,\n",
              "   90.71,\n",
              "   90.78,\n",
              "   90.7,\n",
              "   90.6,\n",
              "   79.68,\n",
              "   81.94,\n",
              "   83.57,\n",
              "   82.51,\n",
              "   84.51,\n",
              "   83.98,\n",
              "   85.53,\n",
              "   84.25,\n",
              "   83.95,\n",
              "   84.22,\n",
              "   83.95,\n",
              "   85.34,\n",
              "   85.96,\n",
              "   84.3,\n",
              "   85.46,\n",
              "   86.59,\n",
              "   85.51,\n",
              "   85.05,\n",
              "   86.36,\n",
              "   84.16,\n",
              "   90.02,\n",
              "   90.22,\n",
              "   90.7,\n",
              "   90.59,\n",
              "   90.58,\n",
              "   90.48,\n",
              "   90.64,\n",
              "   90.22,\n",
              "   90.63,\n",
              "   90.55],\n",
              "  'training_loss': [0.40907566645145416,\n",
              "   0.360950541138649,\n",
              "   0.35040272166132924,\n",
              "   0.3473049049317837,\n",
              "   0.3275092122018337,\n",
              "   0.3349805044710636,\n",
              "   0.31958931329548357,\n",
              "   0.31514615920186045,\n",
              "   0.3107901745721698,\n",
              "   0.3090474818378687,\n",
              "   0.30886315927356484,\n",
              "   0.3099342880159616,\n",
              "   0.31027158932387827,\n",
              "   0.3020517555832863,\n",
              "   0.30246584529578685,\n",
              "   0.298972766559571,\n",
              "   0.2908178831547499,\n",
              "   0.30048629485815764,\n",
              "   0.2928548130407929,\n",
              "   0.2921842183619738,\n",
              "   0.16048257313892245,\n",
              "   0.12430203691832721,\n",
              "   0.10577896826770157,\n",
              "   0.09972059777788818,\n",
              "   0.09043372996971011,\n",
              "   0.08116793152783067,\n",
              "   0.07712968648150563,\n",
              "   0.0711976630795747,\n",
              "   0.0640951121110469,\n",
              "   0.06066714383419603,\n",
              "   0.6032445247888565,\n",
              "   0.4600371181309223,\n",
              "   0.42056222738623616,\n",
              "   0.40091566311120985,\n",
              "   0.39350112627148626,\n",
              "   0.3724687097400427,\n",
              "   0.37358303456008435,\n",
              "   0.36664005237817765,\n",
              "   0.3556244697600603,\n",
              "   0.3554153149843216,\n",
              "   0.35183879979252813,\n",
              "   0.3447562878936529,\n",
              "   0.3385072982698679,\n",
              "   0.3340740914642811,\n",
              "   0.3333474228441715,\n",
              "   0.3252881019204855,\n",
              "   0.3247955938056111,\n",
              "   0.324323251709342,\n",
              "   0.3257295282006264,\n",
              "   0.3136143458515406,\n",
              "   0.18703131733089687,\n",
              "   0.14455435569658875,\n",
              "   0.12962016634270548,\n",
              "   0.11906005847752094,\n",
              "   0.1089113010328263,\n",
              "   0.1044217544157058,\n",
              "   0.09653020181171595,\n",
              "   0.09167640583980828,\n",
              "   0.08709070926774293,\n",
              "   0.07952612738590688],\n",
              "  'valid_loss': [0.42210225051641465,\n",
              "   0.35383942351341247,\n",
              "   0.4297317171573639,\n",
              "   0.4053603218317032,\n",
              "   0.4291050954580307,\n",
              "   0.3971800770521164,\n",
              "   0.4493438600540161,\n",
              "   0.3755212651491165,\n",
              "   0.4161437240600586,\n",
              "   0.40946616864204405,\n",
              "   0.4856018295288086,\n",
              "   0.43796557857990265,\n",
              "   0.46040558100938794,\n",
              "   0.4405988154411316,\n",
              "   0.4151842958331108,\n",
              "   0.44308626794815065,\n",
              "   0.422457066488266,\n",
              "   0.40483203476667406,\n",
              "   0.40512289390563966,\n",
              "   0.49783689572811124,\n",
              "   0.28154569994807244,\n",
              "   0.27318392246067524,\n",
              "   0.2721190889239311,\n",
              "   0.268596945810318,\n",
              "   0.27571865207254886,\n",
              "   0.2710410477638245,\n",
              "   0.2686416128218174,\n",
              "   0.2839177329212427,\n",
              "   0.28436483749076724,\n",
              "   0.2866449775144458,\n",
              "   0.6017296772003173,\n",
              "   0.5101664099693298,\n",
              "   0.44742883722782134,\n",
              "   0.5044509061813355,\n",
              "   0.4397493969917297,\n",
              "   0.4753770266056061,\n",
              "   0.4013705028295517,\n",
              "   0.44403475792407987,\n",
              "   0.4547236580371857,\n",
              "   0.44203361523151397,\n",
              "   0.4756121888399124,\n",
              "   0.439184329867363,\n",
              "   0.4061865078806877,\n",
              "   0.508562934923172,\n",
              "   0.44241422488689425,\n",
              "   0.3968075229883194,\n",
              "   0.4381755865812302,\n",
              "   0.4360711445331574,\n",
              "   0.43369366626739503,\n",
              "   0.4615786414384842,\n",
              "   0.2850886409282684,\n",
              "   0.2834207551896572,\n",
              "   0.2861843703627586,\n",
              "   0.28229024144113063,\n",
              "   0.2847004294723272,\n",
              "   0.2934260986864567,\n",
              "   0.2896845830738545,\n",
              "   0.28826696096211674,\n",
              "   0.2827771405771375,\n",
              "   0.29946278535425663]},\n",
              " '0.6 features.14': {'nbr_param': [510.0, 7097571.0],\n",
              "  'test_acc': [82.52,\n",
              "   85.01,\n",
              "   83.73,\n",
              "   85.0,\n",
              "   84.73,\n",
              "   85.54,\n",
              "   83.11,\n",
              "   86.34,\n",
              "   85.08,\n",
              "   84.93,\n",
              "   82.78,\n",
              "   84.63,\n",
              "   84.43,\n",
              "   84.6,\n",
              "   86.08,\n",
              "   85.62,\n",
              "   85.56,\n",
              "   85.69,\n",
              "   85.79,\n",
              "   83.26,\n",
              "   89.8,\n",
              "   89.89,\n",
              "   90.32,\n",
              "   90.63,\n",
              "   90.6,\n",
              "   90.71,\n",
              "   90.71,\n",
              "   90.78,\n",
              "   90.7,\n",
              "   90.6,\n",
              "   79.68,\n",
              "   81.94,\n",
              "   83.57,\n",
              "   82.51,\n",
              "   84.51,\n",
              "   83.98,\n",
              "   85.53,\n",
              "   84.25,\n",
              "   83.95,\n",
              "   84.22,\n",
              "   83.95,\n",
              "   85.34,\n",
              "   85.96,\n",
              "   84.3,\n",
              "   85.46,\n",
              "   86.59,\n",
              "   85.51,\n",
              "   85.05,\n",
              "   86.36,\n",
              "   84.16,\n",
              "   90.02,\n",
              "   90.22,\n",
              "   90.7,\n",
              "   90.59,\n",
              "   90.58,\n",
              "   90.48,\n",
              "   90.64,\n",
              "   90.22,\n",
              "   90.63,\n",
              "   90.55],\n",
              "  'training_loss': [0.40907566645145416,\n",
              "   0.360950541138649,\n",
              "   0.35040272166132924,\n",
              "   0.3473049049317837,\n",
              "   0.3275092122018337,\n",
              "   0.3349805044710636,\n",
              "   0.31958931329548357,\n",
              "   0.31514615920186045,\n",
              "   0.3107901745721698,\n",
              "   0.3090474818378687,\n",
              "   0.30886315927356484,\n",
              "   0.3099342880159616,\n",
              "   0.31027158932387827,\n",
              "   0.3020517555832863,\n",
              "   0.30246584529578685,\n",
              "   0.298972766559571,\n",
              "   0.2908178831547499,\n",
              "   0.30048629485815764,\n",
              "   0.2928548130407929,\n",
              "   0.2921842183619738,\n",
              "   0.16048257313892245,\n",
              "   0.12430203691832721,\n",
              "   0.10577896826770157,\n",
              "   0.09972059777788818,\n",
              "   0.09043372996971011,\n",
              "   0.08116793152783067,\n",
              "   0.07712968648150563,\n",
              "   0.0711976630795747,\n",
              "   0.0640951121110469,\n",
              "   0.06066714383419603,\n",
              "   0.6032445247888565,\n",
              "   0.4600371181309223,\n",
              "   0.42056222738623616,\n",
              "   0.40091566311120985,\n",
              "   0.39350112627148626,\n",
              "   0.3724687097400427,\n",
              "   0.37358303456008435,\n",
              "   0.36664005237817765,\n",
              "   0.3556244697600603,\n",
              "   0.3554153149843216,\n",
              "   0.35183879979252813,\n",
              "   0.3447562878936529,\n",
              "   0.3385072982698679,\n",
              "   0.3340740914642811,\n",
              "   0.3333474228441715,\n",
              "   0.3252881019204855,\n",
              "   0.3247955938056111,\n",
              "   0.324323251709342,\n",
              "   0.3257295282006264,\n",
              "   0.3136143458515406,\n",
              "   0.18703131733089687,\n",
              "   0.14455435569658875,\n",
              "   0.12962016634270548,\n",
              "   0.11906005847752094,\n",
              "   0.1089113010328263,\n",
              "   0.1044217544157058,\n",
              "   0.09653020181171595,\n",
              "   0.09167640583980828,\n",
              "   0.08709070926774293,\n",
              "   0.07952612738590688],\n",
              "  'valid_loss': [0.42210225051641465,\n",
              "   0.35383942351341247,\n",
              "   0.4297317171573639,\n",
              "   0.4053603218317032,\n",
              "   0.4291050954580307,\n",
              "   0.3971800770521164,\n",
              "   0.4493438600540161,\n",
              "   0.3755212651491165,\n",
              "   0.4161437240600586,\n",
              "   0.40946616864204405,\n",
              "   0.4856018295288086,\n",
              "   0.43796557857990265,\n",
              "   0.46040558100938794,\n",
              "   0.4405988154411316,\n",
              "   0.4151842958331108,\n",
              "   0.44308626794815065,\n",
              "   0.422457066488266,\n",
              "   0.40483203476667406,\n",
              "   0.40512289390563966,\n",
              "   0.49783689572811124,\n",
              "   0.28154569994807244,\n",
              "   0.27318392246067524,\n",
              "   0.2721190889239311,\n",
              "   0.268596945810318,\n",
              "   0.27571865207254886,\n",
              "   0.2710410477638245,\n",
              "   0.2686416128218174,\n",
              "   0.2839177329212427,\n",
              "   0.28436483749076724,\n",
              "   0.2866449775144458,\n",
              "   0.6017296772003173,\n",
              "   0.5101664099693298,\n",
              "   0.44742883722782134,\n",
              "   0.5044509061813355,\n",
              "   0.4397493969917297,\n",
              "   0.4753770266056061,\n",
              "   0.4013705028295517,\n",
              "   0.44403475792407987,\n",
              "   0.4547236580371857,\n",
              "   0.44203361523151397,\n",
              "   0.4756121888399124,\n",
              "   0.439184329867363,\n",
              "   0.4061865078806877,\n",
              "   0.508562934923172,\n",
              "   0.44241422488689425,\n",
              "   0.3968075229883194,\n",
              "   0.4381755865812302,\n",
              "   0.4360711445331574,\n",
              "   0.43369366626739503,\n",
              "   0.4615786414384842,\n",
              "   0.2850886409282684,\n",
              "   0.2834207551896572,\n",
              "   0.2861843703627586,\n",
              "   0.28229024144113063,\n",
              "   0.2847004294723272,\n",
              "   0.2934260986864567,\n",
              "   0.2896845830738545,\n",
              "   0.28826696096211674,\n",
              "   0.2827771405771375,\n",
              "   0.29946278535425663]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wn4qCPzm9u3"
      },
      "source": [
        "### Prune sur \"features.17\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gSoJxJBqpnFW",
        "outputId": "ce17b688-9b1b-42d7-99ed-ec157024d35b"
      },
      "source": [
        "prune_fine_tune(5,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.17\n",
            "features.17\n",
            "7098108.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "85.68  % ,  0.3673545283079147  ,  0.4165575761258602\n",
            "epoch  1\n",
            "saving weights.... \n",
            "84.31  % ,  0.391107876932621  ,  0.36688573741316793\n",
            "epoch  2\n",
            "saving weights.... \n",
            "84.46  % ,  0.424245005774498  ,  0.3547136522620916\n",
            "epoch  3\n",
            "saving weights.... \n",
            "86.06  % ,  0.38230340807437896  ,  0.3405995216906071\n",
            "epoch  4\n",
            "saving weights.... \n",
            "83.46  % ,  0.4369804766893387  ,  0.3373549881637096\n",
            "epoch  5\n",
            "saving weights.... \n",
            "83.73  % ,  0.4667327652692795  ,  0.32738397109508516\n",
            "epoch  6\n",
            "saving weights.... \n",
            "84.62  % ,  0.42256137087345125  ,  0.32126084246337416\n",
            "epoch  7\n",
            "saving weights.... \n",
            "87.1  % ,  0.3763585899591446  ,  0.3222502499461174\n",
            "epoch  8\n",
            "saving weights.... \n",
            "85.3  % ,  0.43210205738544466  ,  0.32008050666749477\n",
            "epoch  9\n",
            "saving weights.... \n",
            "84.62  % ,  0.42068634634017943  ,  0.31754820965826513\n",
            "epoch  10\n",
            "saving weights.... \n",
            "86.08  % ,  0.3826380405664444  ,  0.3089464570879936\n",
            "epoch  11\n",
            "saving weights.... \n",
            "84.97  % ,  0.45791608271598816  ,  0.3091197503954172\n",
            "epoch  12\n",
            "saving weights.... \n",
            "87.0  % ,  0.3638025357246399  ,  0.3076738418698311\n",
            "epoch  13\n",
            "saving weights.... \n",
            "86.06  % ,  0.40323217886686324  ,  0.304758866456151\n",
            "epoch  14\n",
            "saving weights.... \n",
            "85.3  % ,  0.43324824776649473  ,  0.2998647786572576\n",
            "epoch  15\n",
            "saving weights.... \n",
            "85.98  % ,  0.3852114422798157  ,  0.29866554419100283\n",
            "epoch  16\n",
            "saving weights.... \n",
            "85.09  % ,  0.42970344882011413  ,  0.29279589302241804\n",
            "epoch  17\n",
            "saving weights.... \n",
            "87.13  % ,  0.3686750558257103  ,  0.29587005421966317\n",
            "epoch  18\n",
            "saving weights.... \n",
            "85.48  % ,  0.43478653070926665  ,  0.29911903942972423\n",
            "epoch  19\n",
            "saving weights.... \n",
            "86.39  % ,  0.41608525705337523  ,  0.29316971580684187\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.32  % ,  0.2787297412753105  ,  0.1664878933750093\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.41  % ,  0.26794817788004877  ,  0.1257459374267608\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.5  % ,  0.2755814252406359  ,  0.10771592030227184\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.43  % ,  0.28084198917746545  ,  0.09871435370519757\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.84  % ,  0.2787814876049757  ,  0.09209347659815102\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.88  % ,  0.2818325390279293  ,  0.08329371442031115\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.85  % ,  0.27881788937449453  ,  0.07726244004238397\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.81  % ,  0.27852412420213224  ,  0.07302833395041525\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.54  % ,  0.2844353796288371  ,  0.06492400068454444\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.83  % ,  0.2779920490384102  ,  0.06285624367371201\n",
            "Finished Training\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.17\n",
            "features.17\n",
            "7001985.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "81.27  % ,  0.5206531499624252  ,  0.4733887844115496\n",
            "epoch  1\n",
            "saving weights.... \n",
            "83.92  % ,  0.429666927754879  ,  0.3967711104929447\n",
            "epoch  2\n",
            "saving weights.... \n",
            "85.06  % ,  0.4009420718193054  ,  0.38226252407431605\n",
            "epoch  3\n",
            "saving weights.... \n",
            "81.7  % ,  0.5106000027656555  ,  0.37376976781487464\n",
            "epoch  4\n",
            "saving weights.... \n",
            "83.43  % ,  0.48909627113342286  ,  0.3659503265142441\n",
            "epoch  5\n",
            "saving weights.... \n",
            "85.79  % ,  0.400659051322937  ,  0.34900617617666724\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.91  % ,  0.40851053712368013  ,  0.3420859069138765\n",
            "epoch  7\n",
            "saving weights.... \n",
            "84.34  % ,  0.4551992407798767  ,  0.3396369770780206\n",
            "epoch  8\n",
            "saving weights.... \n",
            "86.0  % ,  0.4114827463388443  ,  0.3322650691509247\n",
            "epoch  9\n",
            "saving weights.... \n",
            "85.35  % ,  0.42719521021842954  ,  0.3315376799687743\n",
            "epoch  10\n",
            "saving weights.... \n",
            "84.31  % ,  0.4475166934490204  ,  0.3320910771906376\n",
            "epoch  11\n",
            "saving weights.... \n",
            "82.39  % ,  0.5524183402776718  ,  0.32335208224058154\n",
            "epoch  12\n",
            "saving weights.... \n",
            "85.32  % ,  0.4373073227286339  ,  0.32522230126559737\n",
            "epoch  13\n",
            "saving weights.... \n",
            "85.23  % ,  0.4312356474399567  ,  0.32136273719966413\n",
            "epoch  14\n",
            "saving weights.... \n",
            "84.59  % ,  0.4466846562623978  ,  0.3209088666766882\n",
            "epoch  15\n",
            "saving weights.... \n",
            "86.29  % ,  0.40197948117256166  ,  0.31665384257137774\n",
            "epoch  16\n",
            "saving weights.... \n",
            "86.32  % ,  0.396400754570961  ,  0.31296653770804406\n",
            "epoch  17\n",
            "saving weights.... \n",
            "85.2  % ,  0.4239976851940155  ,  0.31500618796646596\n",
            "epoch  18\n",
            "saving weights.... \n",
            "84.6  % ,  0.47031918416023255  ,  0.31010513770729303\n",
            "epoch  19\n",
            "saving weights.... \n",
            "86.04  % ,  0.4100718839645386  ,  0.30457976829111577\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.26  % ,  0.2802477910161018  ,  0.1792228907942772\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.42  % ,  0.26757230888605116  ,  0.1369694918051362\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.46  % ,  0.2751334415078163  ,  0.12206199594661593\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.81  % ,  0.2723855876505375  ,  0.11102653078995645\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.61  % ,  0.27121414547860623  ,  0.10415978348068893\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.94  % ,  0.2727083999410272  ,  0.09719408934563398\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.95  % ,  0.2719268013358116  ,  0.08918758451528848\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.5  % ,  0.27415639928877356  ,  0.08630386076215654\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.57  % ,  0.2805948691904545  ,  0.08085922618024051\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.78  % ,  0.28089840899407864  ,  0.0741429024990648\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_1a820210-47ed-480e-9382-44a1752c79ed\", \"features.17.npy\", 435)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.3 features.17': {'nbr_param': [510.0, 7098108.0],\n",
              "  'test_acc': [82.52,\n",
              "   85.01,\n",
              "   83.73,\n",
              "   85.0,\n",
              "   84.73,\n",
              "   85.54,\n",
              "   83.11,\n",
              "   86.34,\n",
              "   85.08,\n",
              "   84.93,\n",
              "   82.78,\n",
              "   84.63,\n",
              "   84.43,\n",
              "   84.6,\n",
              "   86.08,\n",
              "   85.62,\n",
              "   85.56,\n",
              "   85.69,\n",
              "   85.79,\n",
              "   83.26,\n",
              "   89.8,\n",
              "   89.89,\n",
              "   90.32,\n",
              "   90.63,\n",
              "   90.6,\n",
              "   90.71,\n",
              "   90.71,\n",
              "   90.78,\n",
              "   90.7,\n",
              "   90.6,\n",
              "   79.68,\n",
              "   81.94,\n",
              "   83.57,\n",
              "   82.51,\n",
              "   84.51,\n",
              "   83.98,\n",
              "   85.53,\n",
              "   84.25,\n",
              "   83.95,\n",
              "   84.22,\n",
              "   83.95,\n",
              "   85.34,\n",
              "   85.96,\n",
              "   84.3,\n",
              "   85.46,\n",
              "   86.59,\n",
              "   85.51,\n",
              "   85.05,\n",
              "   86.36,\n",
              "   84.16,\n",
              "   90.02,\n",
              "   90.22,\n",
              "   90.7,\n",
              "   90.59,\n",
              "   90.58,\n",
              "   90.48,\n",
              "   90.64,\n",
              "   90.22,\n",
              "   90.63,\n",
              "   90.55,\n",
              "   84.85,\n",
              "   85.14,\n",
              "   85.97,\n",
              "   86.47,\n",
              "   85.85,\n",
              "   84.84,\n",
              "   84.26,\n",
              "   86.4,\n",
              "   86.52,\n",
              "   86.11,\n",
              "   84.97,\n",
              "   85.45,\n",
              "   85.45,\n",
              "   85.88,\n",
              "   86.5,\n",
              "   86.6,\n",
              "   85.16,\n",
              "   86.44,\n",
              "   84.81,\n",
              "   86.56,\n",
              "   90.24,\n",
              "   90.62,\n",
              "   90.87,\n",
              "   90.99,\n",
              "   91.08,\n",
              "   91.25,\n",
              "   90.89,\n",
              "   91.03,\n",
              "   91.26,\n",
              "   91.05,\n",
              "   83.79,\n",
              "   86.59,\n",
              "   83.8,\n",
              "   85.63,\n",
              "   84.77,\n",
              "   85.62,\n",
              "   86.5,\n",
              "   84.33,\n",
              "   84.8,\n",
              "   85.55,\n",
              "   85.46,\n",
              "   85.83,\n",
              "   83.2,\n",
              "   84.77,\n",
              "   83.36,\n",
              "   86.32,\n",
              "   85.44,\n",
              "   85.26,\n",
              "   86.57,\n",
              "   85.36,\n",
              "   90.06,\n",
              "   90.39,\n",
              "   90.44,\n",
              "   90.47,\n",
              "   90.95,\n",
              "   90.83,\n",
              "   90.93,\n",
              "   90.64,\n",
              "   90.81,\n",
              "   91.12,\n",
              "   85.68,\n",
              "   84.31,\n",
              "   84.46,\n",
              "   86.06,\n",
              "   83.46,\n",
              "   83.73,\n",
              "   84.62,\n",
              "   87.1,\n",
              "   85.3,\n",
              "   84.62,\n",
              "   86.08,\n",
              "   84.97,\n",
              "   87.0,\n",
              "   86.06,\n",
              "   85.3,\n",
              "   85.98,\n",
              "   85.09,\n",
              "   87.13,\n",
              "   85.48,\n",
              "   86.39,\n",
              "   90.32,\n",
              "   90.41,\n",
              "   90.5,\n",
              "   90.43,\n",
              "   90.84,\n",
              "   90.88,\n",
              "   90.85,\n",
              "   90.81,\n",
              "   90.54,\n",
              "   90.83,\n",
              "   81.27,\n",
              "   83.92,\n",
              "   85.06,\n",
              "   81.7,\n",
              "   83.43,\n",
              "   85.79,\n",
              "   85.91,\n",
              "   84.34,\n",
              "   86.0,\n",
              "   85.35,\n",
              "   84.31,\n",
              "   82.39,\n",
              "   85.32,\n",
              "   85.23,\n",
              "   84.59,\n",
              "   86.29,\n",
              "   86.32,\n",
              "   85.2,\n",
              "   84.6,\n",
              "   86.04,\n",
              "   90.26,\n",
              "   90.42,\n",
              "   90.46,\n",
              "   90.81,\n",
              "   90.61,\n",
              "   90.94,\n",
              "   90.95,\n",
              "   90.5,\n",
              "   90.57,\n",
              "   90.78],\n",
              "  'training_loss': [0.40907566645145416,\n",
              "   0.360950541138649,\n",
              "   0.35040272166132924,\n",
              "   0.3473049049317837,\n",
              "   0.3275092122018337,\n",
              "   0.3349805044710636,\n",
              "   0.31958931329548357,\n",
              "   0.31514615920186045,\n",
              "   0.3107901745721698,\n",
              "   0.3090474818378687,\n",
              "   0.30886315927356484,\n",
              "   0.3099342880159616,\n",
              "   0.31027158932387827,\n",
              "   0.3020517555832863,\n",
              "   0.30246584529578685,\n",
              "   0.298972766559571,\n",
              "   0.2908178831547499,\n",
              "   0.30048629485815764,\n",
              "   0.2928548130407929,\n",
              "   0.2921842183619738,\n",
              "   0.16048257313892245,\n",
              "   0.12430203691832721,\n",
              "   0.10577896826770157,\n",
              "   0.09972059777788818,\n",
              "   0.09043372996971011,\n",
              "   0.08116793152783067,\n",
              "   0.07712968648150563,\n",
              "   0.0711976630795747,\n",
              "   0.0640951121110469,\n",
              "   0.06066714383419603,\n",
              "   0.6032445247888565,\n",
              "   0.4600371181309223,\n",
              "   0.42056222738623616,\n",
              "   0.40091566311120985,\n",
              "   0.39350112627148626,\n",
              "   0.3724687097400427,\n",
              "   0.37358303456008435,\n",
              "   0.36664005237817765,\n",
              "   0.3556244697600603,\n",
              "   0.3554153149843216,\n",
              "   0.35183879979252813,\n",
              "   0.3447562878936529,\n",
              "   0.3385072982698679,\n",
              "   0.3340740914642811,\n",
              "   0.3333474228441715,\n",
              "   0.3252881019204855,\n",
              "   0.3247955938056111,\n",
              "   0.324323251709342,\n",
              "   0.3257295282006264,\n",
              "   0.3136143458515406,\n",
              "   0.18703131733089687,\n",
              "   0.14455435569658875,\n",
              "   0.12962016634270548,\n",
              "   0.11906005847752094,\n",
              "   0.1089113010328263,\n",
              "   0.1044217544157058,\n",
              "   0.09653020181171595,\n",
              "   0.09167640583980828,\n",
              "   0.08709070926774293,\n",
              "   0.07952612738590688,\n",
              "   0.3915827317610383,\n",
              "   0.36015125019550326,\n",
              "   0.3394492732614279,\n",
              "   0.33276546038389204,\n",
              "   0.321261684307456,\n",
              "   0.317406006988883,\n",
              "   0.31472248481065035,\n",
              "   0.31368526110053063,\n",
              "   0.30694957921504973,\n",
              "   0.30771915841698644,\n",
              "   0.30400373374074696,\n",
              "   0.3019208874642849,\n",
              "   0.2940466021746397,\n",
              "   0.29928265756517647,\n",
              "   0.2930287833362818,\n",
              "   0.2921968050956726,\n",
              "   0.28996033757179973,\n",
              "   0.29818379101753234,\n",
              "   0.28905609567165375,\n",
              "   0.2829679045215249,\n",
              "   0.1546595427915454,\n",
              "   0.12215680264309049,\n",
              "   0.10269908316135407,\n",
              "   0.09301462987661362,\n",
              "   0.08477825286388398,\n",
              "   0.07879231826141476,\n",
              "   0.07056481614094227,\n",
              "   0.06797104831896722,\n",
              "   0.0619820798965171,\n",
              "   0.05927455047667027,\n",
              "   0.43319469038397074,\n",
              "   0.3730352936834097,\n",
              "   0.3572012962371111,\n",
              "   0.3487765677303076,\n",
              "   0.3396714374542236,\n",
              "   0.3310719349384308,\n",
              "   0.32411790349185465,\n",
              "   0.31850334607064723,\n",
              "   0.3203605338037014,\n",
              "   0.3191365311205387,\n",
              "   0.3103590044379234,\n",
              "   0.31409982268214226,\n",
              "   0.3018649290859699,\n",
              "   0.30651151714622976,\n",
              "   0.30179595678150656,\n",
              "   0.30144626151025294,\n",
              "   0.30031773758530617,\n",
              "   0.29617451620399954,\n",
              "   0.2962040895164013,\n",
              "   0.2949030253380537,\n",
              "   0.16728212798833847,\n",
              "   0.12332898807525634,\n",
              "   0.10870779219828546,\n",
              "   0.0953997129637748,\n",
              "   0.08869926975779235,\n",
              "   0.08311862417384983,\n",
              "   0.07547869409192354,\n",
              "   0.0719722601454705,\n",
              "   0.0650316122174263,\n",
              "   0.06117693085847423,\n",
              "   0.4165575761258602,\n",
              "   0.36688573741316793,\n",
              "   0.3547136522620916,\n",
              "   0.3405995216906071,\n",
              "   0.3373549881637096,\n",
              "   0.32738397109508516,\n",
              "   0.32126084246337416,\n",
              "   0.3222502499461174,\n",
              "   0.32008050666749477,\n",
              "   0.31754820965826513,\n",
              "   0.3089464570879936,\n",
              "   0.3091197503954172,\n",
              "   0.3076738418698311,\n",
              "   0.304758866456151,\n",
              "   0.2998647786572576,\n",
              "   0.29866554419100283,\n",
              "   0.29279589302241804,\n",
              "   0.29587005421966317,\n",
              "   0.29911903942972423,\n",
              "   0.29316971580684187,\n",
              "   0.1664878933750093,\n",
              "   0.1257459374267608,\n",
              "   0.10771592030227184,\n",
              "   0.09871435370519757,\n",
              "   0.09209347659815102,\n",
              "   0.08329371442031115,\n",
              "   0.07726244004238397,\n",
              "   0.07302833395041525,\n",
              "   0.06492400068454444,\n",
              "   0.06285624367371201,\n",
              "   0.4733887844115496,\n",
              "   0.3967711104929447,\n",
              "   0.38226252407431605,\n",
              "   0.37376976781487464,\n",
              "   0.3659503265142441,\n",
              "   0.34900617617666724,\n",
              "   0.3420859069138765,\n",
              "   0.3396369770780206,\n",
              "   0.3322650691509247,\n",
              "   0.3315376799687743,\n",
              "   0.3320910771906376,\n",
              "   0.32335208224058154,\n",
              "   0.32522230126559737,\n",
              "   0.32136273719966413,\n",
              "   0.3209088666766882,\n",
              "   0.31665384257137774,\n",
              "   0.31296653770804406,\n",
              "   0.31500618796646596,\n",
              "   0.31010513770729303,\n",
              "   0.30457976829111577,\n",
              "   0.1792228907942772,\n",
              "   0.1369694918051362,\n",
              "   0.12206199594661593,\n",
              "   0.11102653078995645,\n",
              "   0.10415978348068893,\n",
              "   0.09719408934563398,\n",
              "   0.08918758451528848,\n",
              "   0.08630386076215654,\n",
              "   0.08085922618024051,\n",
              "   0.0741429024990648],\n",
              "  'valid_loss': [0.42210225051641465,\n",
              "   0.35383942351341247,\n",
              "   0.4297317171573639,\n",
              "   0.4053603218317032,\n",
              "   0.4291050954580307,\n",
              "   0.3971800770521164,\n",
              "   0.4493438600540161,\n",
              "   0.3755212651491165,\n",
              "   0.4161437240600586,\n",
              "   0.40946616864204405,\n",
              "   0.4856018295288086,\n",
              "   0.43796557857990265,\n",
              "   0.46040558100938794,\n",
              "   0.4405988154411316,\n",
              "   0.4151842958331108,\n",
              "   0.44308626794815065,\n",
              "   0.422457066488266,\n",
              "   0.40483203476667406,\n",
              "   0.40512289390563966,\n",
              "   0.49783689572811124,\n",
              "   0.28154569994807244,\n",
              "   0.27318392246067524,\n",
              "   0.2721190889239311,\n",
              "   0.268596945810318,\n",
              "   0.27571865207254886,\n",
              "   0.2710410477638245,\n",
              "   0.2686416128218174,\n",
              "   0.2839177329212427,\n",
              "   0.28436483749076724,\n",
              "   0.2866449775144458,\n",
              "   0.6017296772003173,\n",
              "   0.5101664099693298,\n",
              "   0.44742883722782134,\n",
              "   0.5044509061813355,\n",
              "   0.4397493969917297,\n",
              "   0.4753770266056061,\n",
              "   0.4013705028295517,\n",
              "   0.44403475792407987,\n",
              "   0.4547236580371857,\n",
              "   0.44203361523151397,\n",
              "   0.4756121888399124,\n",
              "   0.439184329867363,\n",
              "   0.4061865078806877,\n",
              "   0.508562934923172,\n",
              "   0.44241422488689425,\n",
              "   0.3968075229883194,\n",
              "   0.4381755865812302,\n",
              "   0.4360711445331574,\n",
              "   0.43369366626739503,\n",
              "   0.4615786414384842,\n",
              "   0.2850886409282684,\n",
              "   0.2834207551896572,\n",
              "   0.2861843703627586,\n",
              "   0.28229024144113063,\n",
              "   0.2847004294723272,\n",
              "   0.2934260986864567,\n",
              "   0.2896845830738545,\n",
              "   0.28826696096211674,\n",
              "   0.2827771405771375,\n",
              "   0.29946278535425663,\n",
              "   0.37885906960964205,\n",
              "   0.3813879222869873,\n",
              "   0.36162704025506975,\n",
              "   0.36307315928936007,\n",
              "   0.373699124789238,\n",
              "   0.4199017300724983,\n",
              "   0.4386263092637062,\n",
              "   0.37494810302257536,\n",
              "   0.38244445865154264,\n",
              "   0.39212416303157804,\n",
              "   0.42861008958816527,\n",
              "   0.40980375032424926,\n",
              "   0.39386013095378875,\n",
              "   0.39420974303483963,\n",
              "   0.3733188946247101,\n",
              "   0.39367760735750196,\n",
              "   0.4251572137236595,\n",
              "   0.387861485004425,\n",
              "   0.4270662139892578,\n",
              "   0.39478837317228316,\n",
              "   0.2804021812051535,\n",
              "   0.2687478130489588,\n",
              "   0.2640162861764431,\n",
              "   0.2640109902530909,\n",
              "   0.2667842405796051,\n",
              "   0.2732843278825283,\n",
              "   0.2812460358470678,\n",
              "   0.2759890424236655,\n",
              "   0.2865777604825795,\n",
              "   0.27957971667200326,\n",
              "   0.4035875594854355,\n",
              "   0.34408994076251986,\n",
              "   0.4150781086206436,\n",
              "   0.3763122297525406,\n",
              "   0.41687004276514056,\n",
              "   0.3772286298036575,\n",
              "   0.37435594547986983,\n",
              "   0.436659460735321,\n",
              "   0.4233722271308303,\n",
              "   0.39898086166381835,\n",
              "   0.4139880071401596,\n",
              "   0.3978924362182617,\n",
              "   0.5051281242370605,\n",
              "   0.43509574440717697,\n",
              "   0.47109321502447127,\n",
              "   0.4048124081730843,\n",
              "   0.42337273358106614,\n",
              "   0.4663681983053684,\n",
              "   0.4163111712932587,\n",
              "   0.43114149227142334,\n",
              "   0.27357843561172485,\n",
              "   0.28219130519777536,\n",
              "   0.2825168494999409,\n",
              "   0.27767424177229405,\n",
              "   0.2851933906078339,\n",
              "   0.28464450816810133,\n",
              "   0.2854661216765642,\n",
              "   0.2778184846512973,\n",
              "   0.28704436569809916,\n",
              "   0.2970047707542777,\n",
              "   0.3673545283079147,\n",
              "   0.391107876932621,\n",
              "   0.424245005774498,\n",
              "   0.38230340807437896,\n",
              "   0.4369804766893387,\n",
              "   0.4667327652692795,\n",
              "   0.42256137087345125,\n",
              "   0.3763585899591446,\n",
              "   0.43210205738544466,\n",
              "   0.42068634634017943,\n",
              "   0.3826380405664444,\n",
              "   0.45791608271598816,\n",
              "   0.3638025357246399,\n",
              "   0.40323217886686324,\n",
              "   0.43324824776649473,\n",
              "   0.3852114422798157,\n",
              "   0.42970344882011413,\n",
              "   0.3686750558257103,\n",
              "   0.43478653070926665,\n",
              "   0.41608525705337523,\n",
              "   0.2787297412753105,\n",
              "   0.26794817788004877,\n",
              "   0.2755814252406359,\n",
              "   0.28084198917746545,\n",
              "   0.2787814876049757,\n",
              "   0.2818325390279293,\n",
              "   0.27881788937449453,\n",
              "   0.27852412420213224,\n",
              "   0.2844353796288371,\n",
              "   0.2779920490384102,\n",
              "   0.5206531499624252,\n",
              "   0.429666927754879,\n",
              "   0.4009420718193054,\n",
              "   0.5106000027656555,\n",
              "   0.48909627113342286,\n",
              "   0.400659051322937,\n",
              "   0.40851053712368013,\n",
              "   0.4551992407798767,\n",
              "   0.4114827463388443,\n",
              "   0.42719521021842954,\n",
              "   0.4475166934490204,\n",
              "   0.5524183402776718,\n",
              "   0.4373073227286339,\n",
              "   0.4312356474399567,\n",
              "   0.4466846562623978,\n",
              "   0.40197948117256166,\n",
              "   0.396400754570961,\n",
              "   0.4239976851940155,\n",
              "   0.47031918416023255,\n",
              "   0.4100718839645386,\n",
              "   0.2802477910161018,\n",
              "   0.26757230888605116,\n",
              "   0.2751334415078163,\n",
              "   0.2723855876505375,\n",
              "   0.27121414547860623,\n",
              "   0.2727083999410272,\n",
              "   0.2719268013358116,\n",
              "   0.27415639928877356,\n",
              "   0.2805948691904545,\n",
              "   0.28089840899407864]},\n",
              " '0.6 features.17': {'nbr_param': [510.0, 7001985.0],\n",
              "  'test_acc': [82.52,\n",
              "   85.01,\n",
              "   83.73,\n",
              "   85.0,\n",
              "   84.73,\n",
              "   85.54,\n",
              "   83.11,\n",
              "   86.34,\n",
              "   85.08,\n",
              "   84.93,\n",
              "   82.78,\n",
              "   84.63,\n",
              "   84.43,\n",
              "   84.6,\n",
              "   86.08,\n",
              "   85.62,\n",
              "   85.56,\n",
              "   85.69,\n",
              "   85.79,\n",
              "   83.26,\n",
              "   89.8,\n",
              "   89.89,\n",
              "   90.32,\n",
              "   90.63,\n",
              "   90.6,\n",
              "   90.71,\n",
              "   90.71,\n",
              "   90.78,\n",
              "   90.7,\n",
              "   90.6,\n",
              "   79.68,\n",
              "   81.94,\n",
              "   83.57,\n",
              "   82.51,\n",
              "   84.51,\n",
              "   83.98,\n",
              "   85.53,\n",
              "   84.25,\n",
              "   83.95,\n",
              "   84.22,\n",
              "   83.95,\n",
              "   85.34,\n",
              "   85.96,\n",
              "   84.3,\n",
              "   85.46,\n",
              "   86.59,\n",
              "   85.51,\n",
              "   85.05,\n",
              "   86.36,\n",
              "   84.16,\n",
              "   90.02,\n",
              "   90.22,\n",
              "   90.7,\n",
              "   90.59,\n",
              "   90.58,\n",
              "   90.48,\n",
              "   90.64,\n",
              "   90.22,\n",
              "   90.63,\n",
              "   90.55,\n",
              "   84.85,\n",
              "   85.14,\n",
              "   85.97,\n",
              "   86.47,\n",
              "   85.85,\n",
              "   84.84,\n",
              "   84.26,\n",
              "   86.4,\n",
              "   86.52,\n",
              "   86.11,\n",
              "   84.97,\n",
              "   85.45,\n",
              "   85.45,\n",
              "   85.88,\n",
              "   86.5,\n",
              "   86.6,\n",
              "   85.16,\n",
              "   86.44,\n",
              "   84.81,\n",
              "   86.56,\n",
              "   90.24,\n",
              "   90.62,\n",
              "   90.87,\n",
              "   90.99,\n",
              "   91.08,\n",
              "   91.25,\n",
              "   90.89,\n",
              "   91.03,\n",
              "   91.26,\n",
              "   91.05,\n",
              "   83.79,\n",
              "   86.59,\n",
              "   83.8,\n",
              "   85.63,\n",
              "   84.77,\n",
              "   85.62,\n",
              "   86.5,\n",
              "   84.33,\n",
              "   84.8,\n",
              "   85.55,\n",
              "   85.46,\n",
              "   85.83,\n",
              "   83.2,\n",
              "   84.77,\n",
              "   83.36,\n",
              "   86.32,\n",
              "   85.44,\n",
              "   85.26,\n",
              "   86.57,\n",
              "   85.36,\n",
              "   90.06,\n",
              "   90.39,\n",
              "   90.44,\n",
              "   90.47,\n",
              "   90.95,\n",
              "   90.83,\n",
              "   90.93,\n",
              "   90.64,\n",
              "   90.81,\n",
              "   91.12,\n",
              "   85.68,\n",
              "   84.31,\n",
              "   84.46,\n",
              "   86.06,\n",
              "   83.46,\n",
              "   83.73,\n",
              "   84.62,\n",
              "   87.1,\n",
              "   85.3,\n",
              "   84.62,\n",
              "   86.08,\n",
              "   84.97,\n",
              "   87.0,\n",
              "   86.06,\n",
              "   85.3,\n",
              "   85.98,\n",
              "   85.09,\n",
              "   87.13,\n",
              "   85.48,\n",
              "   86.39,\n",
              "   90.32,\n",
              "   90.41,\n",
              "   90.5,\n",
              "   90.43,\n",
              "   90.84,\n",
              "   90.88,\n",
              "   90.85,\n",
              "   90.81,\n",
              "   90.54,\n",
              "   90.83,\n",
              "   81.27,\n",
              "   83.92,\n",
              "   85.06,\n",
              "   81.7,\n",
              "   83.43,\n",
              "   85.79,\n",
              "   85.91,\n",
              "   84.34,\n",
              "   86.0,\n",
              "   85.35,\n",
              "   84.31,\n",
              "   82.39,\n",
              "   85.32,\n",
              "   85.23,\n",
              "   84.59,\n",
              "   86.29,\n",
              "   86.32,\n",
              "   85.2,\n",
              "   84.6,\n",
              "   86.04,\n",
              "   90.26,\n",
              "   90.42,\n",
              "   90.46,\n",
              "   90.81,\n",
              "   90.61,\n",
              "   90.94,\n",
              "   90.95,\n",
              "   90.5,\n",
              "   90.57,\n",
              "   90.78],\n",
              "  'training_loss': [0.40907566645145416,\n",
              "   0.360950541138649,\n",
              "   0.35040272166132924,\n",
              "   0.3473049049317837,\n",
              "   0.3275092122018337,\n",
              "   0.3349805044710636,\n",
              "   0.31958931329548357,\n",
              "   0.31514615920186045,\n",
              "   0.3107901745721698,\n",
              "   0.3090474818378687,\n",
              "   0.30886315927356484,\n",
              "   0.3099342880159616,\n",
              "   0.31027158932387827,\n",
              "   0.3020517555832863,\n",
              "   0.30246584529578685,\n",
              "   0.298972766559571,\n",
              "   0.2908178831547499,\n",
              "   0.30048629485815764,\n",
              "   0.2928548130407929,\n",
              "   0.2921842183619738,\n",
              "   0.16048257313892245,\n",
              "   0.12430203691832721,\n",
              "   0.10577896826770157,\n",
              "   0.09972059777788818,\n",
              "   0.09043372996971011,\n",
              "   0.08116793152783067,\n",
              "   0.07712968648150563,\n",
              "   0.0711976630795747,\n",
              "   0.0640951121110469,\n",
              "   0.06066714383419603,\n",
              "   0.6032445247888565,\n",
              "   0.4600371181309223,\n",
              "   0.42056222738623616,\n",
              "   0.40091566311120985,\n",
              "   0.39350112627148626,\n",
              "   0.3724687097400427,\n",
              "   0.37358303456008435,\n",
              "   0.36664005237817765,\n",
              "   0.3556244697600603,\n",
              "   0.3554153149843216,\n",
              "   0.35183879979252813,\n",
              "   0.3447562878936529,\n",
              "   0.3385072982698679,\n",
              "   0.3340740914642811,\n",
              "   0.3333474228441715,\n",
              "   0.3252881019204855,\n",
              "   0.3247955938056111,\n",
              "   0.324323251709342,\n",
              "   0.3257295282006264,\n",
              "   0.3136143458515406,\n",
              "   0.18703131733089687,\n",
              "   0.14455435569658875,\n",
              "   0.12962016634270548,\n",
              "   0.11906005847752094,\n",
              "   0.1089113010328263,\n",
              "   0.1044217544157058,\n",
              "   0.09653020181171595,\n",
              "   0.09167640583980828,\n",
              "   0.08709070926774293,\n",
              "   0.07952612738590688,\n",
              "   0.3915827317610383,\n",
              "   0.36015125019550326,\n",
              "   0.3394492732614279,\n",
              "   0.33276546038389204,\n",
              "   0.321261684307456,\n",
              "   0.317406006988883,\n",
              "   0.31472248481065035,\n",
              "   0.31368526110053063,\n",
              "   0.30694957921504973,\n",
              "   0.30771915841698644,\n",
              "   0.30400373374074696,\n",
              "   0.3019208874642849,\n",
              "   0.2940466021746397,\n",
              "   0.29928265756517647,\n",
              "   0.2930287833362818,\n",
              "   0.2921968050956726,\n",
              "   0.28996033757179973,\n",
              "   0.29818379101753234,\n",
              "   0.28905609567165375,\n",
              "   0.2829679045215249,\n",
              "   0.1546595427915454,\n",
              "   0.12215680264309049,\n",
              "   0.10269908316135407,\n",
              "   0.09301462987661362,\n",
              "   0.08477825286388398,\n",
              "   0.07879231826141476,\n",
              "   0.07056481614094227,\n",
              "   0.06797104831896722,\n",
              "   0.0619820798965171,\n",
              "   0.05927455047667027,\n",
              "   0.43319469038397074,\n",
              "   0.3730352936834097,\n",
              "   0.3572012962371111,\n",
              "   0.3487765677303076,\n",
              "   0.3396714374542236,\n",
              "   0.3310719349384308,\n",
              "   0.32411790349185465,\n",
              "   0.31850334607064723,\n",
              "   0.3203605338037014,\n",
              "   0.3191365311205387,\n",
              "   0.3103590044379234,\n",
              "   0.31409982268214226,\n",
              "   0.3018649290859699,\n",
              "   0.30651151714622976,\n",
              "   0.30179595678150656,\n",
              "   0.30144626151025294,\n",
              "   0.30031773758530617,\n",
              "   0.29617451620399954,\n",
              "   0.2962040895164013,\n",
              "   0.2949030253380537,\n",
              "   0.16728212798833847,\n",
              "   0.12332898807525634,\n",
              "   0.10870779219828546,\n",
              "   0.0953997129637748,\n",
              "   0.08869926975779235,\n",
              "   0.08311862417384983,\n",
              "   0.07547869409192354,\n",
              "   0.0719722601454705,\n",
              "   0.0650316122174263,\n",
              "   0.06117693085847423,\n",
              "   0.4165575761258602,\n",
              "   0.36688573741316793,\n",
              "   0.3547136522620916,\n",
              "   0.3405995216906071,\n",
              "   0.3373549881637096,\n",
              "   0.32738397109508516,\n",
              "   0.32126084246337416,\n",
              "   0.3222502499461174,\n",
              "   0.32008050666749477,\n",
              "   0.31754820965826513,\n",
              "   0.3089464570879936,\n",
              "   0.3091197503954172,\n",
              "   0.3076738418698311,\n",
              "   0.304758866456151,\n",
              "   0.2998647786572576,\n",
              "   0.29866554419100283,\n",
              "   0.29279589302241804,\n",
              "   0.29587005421966317,\n",
              "   0.29911903942972423,\n",
              "   0.29316971580684187,\n",
              "   0.1664878933750093,\n",
              "   0.1257459374267608,\n",
              "   0.10771592030227184,\n",
              "   0.09871435370519757,\n",
              "   0.09209347659815102,\n",
              "   0.08329371442031115,\n",
              "   0.07726244004238397,\n",
              "   0.07302833395041525,\n",
              "   0.06492400068454444,\n",
              "   0.06285624367371201,\n",
              "   0.4733887844115496,\n",
              "   0.3967711104929447,\n",
              "   0.38226252407431605,\n",
              "   0.37376976781487464,\n",
              "   0.3659503265142441,\n",
              "   0.34900617617666724,\n",
              "   0.3420859069138765,\n",
              "   0.3396369770780206,\n",
              "   0.3322650691509247,\n",
              "   0.3315376799687743,\n",
              "   0.3320910771906376,\n",
              "   0.32335208224058154,\n",
              "   0.32522230126559737,\n",
              "   0.32136273719966413,\n",
              "   0.3209088666766882,\n",
              "   0.31665384257137774,\n",
              "   0.31296653770804406,\n",
              "   0.31500618796646596,\n",
              "   0.31010513770729303,\n",
              "   0.30457976829111577,\n",
              "   0.1792228907942772,\n",
              "   0.1369694918051362,\n",
              "   0.12206199594661593,\n",
              "   0.11102653078995645,\n",
              "   0.10415978348068893,\n",
              "   0.09719408934563398,\n",
              "   0.08918758451528848,\n",
              "   0.08630386076215654,\n",
              "   0.08085922618024051,\n",
              "   0.0741429024990648],\n",
              "  'valid_loss': [0.42210225051641465,\n",
              "   0.35383942351341247,\n",
              "   0.4297317171573639,\n",
              "   0.4053603218317032,\n",
              "   0.4291050954580307,\n",
              "   0.3971800770521164,\n",
              "   0.4493438600540161,\n",
              "   0.3755212651491165,\n",
              "   0.4161437240600586,\n",
              "   0.40946616864204405,\n",
              "   0.4856018295288086,\n",
              "   0.43796557857990265,\n",
              "   0.46040558100938794,\n",
              "   0.4405988154411316,\n",
              "   0.4151842958331108,\n",
              "   0.44308626794815065,\n",
              "   0.422457066488266,\n",
              "   0.40483203476667406,\n",
              "   0.40512289390563966,\n",
              "   0.49783689572811124,\n",
              "   0.28154569994807244,\n",
              "   0.27318392246067524,\n",
              "   0.2721190889239311,\n",
              "   0.268596945810318,\n",
              "   0.27571865207254886,\n",
              "   0.2710410477638245,\n",
              "   0.2686416128218174,\n",
              "   0.2839177329212427,\n",
              "   0.28436483749076724,\n",
              "   0.2866449775144458,\n",
              "   0.6017296772003173,\n",
              "   0.5101664099693298,\n",
              "   0.44742883722782134,\n",
              "   0.5044509061813355,\n",
              "   0.4397493969917297,\n",
              "   0.4753770266056061,\n",
              "   0.4013705028295517,\n",
              "   0.44403475792407987,\n",
              "   0.4547236580371857,\n",
              "   0.44203361523151397,\n",
              "   0.4756121888399124,\n",
              "   0.439184329867363,\n",
              "   0.4061865078806877,\n",
              "   0.508562934923172,\n",
              "   0.44241422488689425,\n",
              "   0.3968075229883194,\n",
              "   0.4381755865812302,\n",
              "   0.4360711445331574,\n",
              "   0.43369366626739503,\n",
              "   0.4615786414384842,\n",
              "   0.2850886409282684,\n",
              "   0.2834207551896572,\n",
              "   0.2861843703627586,\n",
              "   0.28229024144113063,\n",
              "   0.2847004294723272,\n",
              "   0.2934260986864567,\n",
              "   0.2896845830738545,\n",
              "   0.28826696096211674,\n",
              "   0.2827771405771375,\n",
              "   0.29946278535425663,\n",
              "   0.37885906960964205,\n",
              "   0.3813879222869873,\n",
              "   0.36162704025506975,\n",
              "   0.36307315928936007,\n",
              "   0.373699124789238,\n",
              "   0.4199017300724983,\n",
              "   0.4386263092637062,\n",
              "   0.37494810302257536,\n",
              "   0.38244445865154264,\n",
              "   0.39212416303157804,\n",
              "   0.42861008958816527,\n",
              "   0.40980375032424926,\n",
              "   0.39386013095378875,\n",
              "   0.39420974303483963,\n",
              "   0.3733188946247101,\n",
              "   0.39367760735750196,\n",
              "   0.4251572137236595,\n",
              "   0.387861485004425,\n",
              "   0.4270662139892578,\n",
              "   0.39478837317228316,\n",
              "   0.2804021812051535,\n",
              "   0.2687478130489588,\n",
              "   0.2640162861764431,\n",
              "   0.2640109902530909,\n",
              "   0.2667842405796051,\n",
              "   0.2732843278825283,\n",
              "   0.2812460358470678,\n",
              "   0.2759890424236655,\n",
              "   0.2865777604825795,\n",
              "   0.27957971667200326,\n",
              "   0.4035875594854355,\n",
              "   0.34408994076251986,\n",
              "   0.4150781086206436,\n",
              "   0.3763122297525406,\n",
              "   0.41687004276514056,\n",
              "   0.3772286298036575,\n",
              "   0.37435594547986983,\n",
              "   0.436659460735321,\n",
              "   0.4233722271308303,\n",
              "   0.39898086166381835,\n",
              "   0.4139880071401596,\n",
              "   0.3978924362182617,\n",
              "   0.5051281242370605,\n",
              "   0.43509574440717697,\n",
              "   0.47109321502447127,\n",
              "   0.4048124081730843,\n",
              "   0.42337273358106614,\n",
              "   0.4663681983053684,\n",
              "   0.4163111712932587,\n",
              "   0.43114149227142334,\n",
              "   0.27357843561172485,\n",
              "   0.28219130519777536,\n",
              "   0.2825168494999409,\n",
              "   0.27767424177229405,\n",
              "   0.2851933906078339,\n",
              "   0.28464450816810133,\n",
              "   0.2854661216765642,\n",
              "   0.2778184846512973,\n",
              "   0.28704436569809916,\n",
              "   0.2970047707542777,\n",
              "   0.3673545283079147,\n",
              "   0.391107876932621,\n",
              "   0.424245005774498,\n",
              "   0.38230340807437896,\n",
              "   0.4369804766893387,\n",
              "   0.4667327652692795,\n",
              "   0.42256137087345125,\n",
              "   0.3763585899591446,\n",
              "   0.43210205738544466,\n",
              "   0.42068634634017943,\n",
              "   0.3826380405664444,\n",
              "   0.45791608271598816,\n",
              "   0.3638025357246399,\n",
              "   0.40323217886686324,\n",
              "   0.43324824776649473,\n",
              "   0.3852114422798157,\n",
              "   0.42970344882011413,\n",
              "   0.3686750558257103,\n",
              "   0.43478653070926665,\n",
              "   0.41608525705337523,\n",
              "   0.2787297412753105,\n",
              "   0.26794817788004877,\n",
              "   0.2755814252406359,\n",
              "   0.28084198917746545,\n",
              "   0.2787814876049757,\n",
              "   0.2818325390279293,\n",
              "   0.27881788937449453,\n",
              "   0.27852412420213224,\n",
              "   0.2844353796288371,\n",
              "   0.2779920490384102,\n",
              "   0.5206531499624252,\n",
              "   0.429666927754879,\n",
              "   0.4009420718193054,\n",
              "   0.5106000027656555,\n",
              "   0.48909627113342286,\n",
              "   0.400659051322937,\n",
              "   0.40851053712368013,\n",
              "   0.4551992407798767,\n",
              "   0.4114827463388443,\n",
              "   0.42719521021842954,\n",
              "   0.4475166934490204,\n",
              "   0.5524183402776718,\n",
              "   0.4373073227286339,\n",
              "   0.4312356474399567,\n",
              "   0.4466846562623978,\n",
              "   0.40197948117256166,\n",
              "   0.396400754570961,\n",
              "   0.4239976851940155,\n",
              "   0.47031918416023255,\n",
              "   0.4100718839645386,\n",
              "   0.2802477910161018,\n",
              "   0.26757230888605116,\n",
              "   0.2751334415078163,\n",
              "   0.2723855876505375,\n",
              "   0.27121414547860623,\n",
              "   0.2727083999410272,\n",
              "   0.2719268013358116,\n",
              "   0.27415639928877356,\n",
              "   0.2805948691904545,\n",
              "   0.28089840899407864]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRE98xVdna8v"
      },
      "source": [
        "### Prune sur \"features.20\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JnC6hLGEpnnO",
        "outputId": "14529fa1-1975-4841-8a17-eb6a7ebff453"
      },
      "source": [
        "prune_fine_tune(6,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.20\n",
            "features.20\n",
            "7098108.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "82.38  % ,  0.44386775991916655  ,  0.3860620348814875\n",
            "epoch  1\n",
            "saving weights.... \n",
            "85.91  % ,  0.3670612971305847  ,  0.3473760468780994\n",
            "epoch  2\n",
            "saving weights.... \n",
            "83.35  % ,  0.4275624608039856  ,  0.34364650651216505\n",
            "epoch  3\n",
            "saving weights.... \n",
            "82.6  % ,  0.4840765997886658  ,  0.3364586474478245\n",
            "epoch  4\n",
            "saving weights.... \n",
            "84.85  % ,  0.3922687263250351  ,  0.32359960429370405\n",
            "epoch  5\n",
            "saving weights.... \n",
            "84.45  % ,  0.41186701002120973  ,  0.32146553917229176\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.95  % ,  0.38272371337413785  ,  0.31804814372062684\n",
            "epoch  7\n",
            "saving weights.... \n",
            "85.39  % ,  0.41316329424381254  ,  0.31285910589694976\n",
            "epoch  8\n",
            "saving weights.... \n",
            "86.71  % ,  0.3868884770154953  ,  0.3148858815908432\n",
            "epoch  9\n",
            "saving weights.... \n",
            "86.42  % ,  0.36779265110492704  ,  0.30576938945651055\n",
            "epoch  10\n",
            "saving weights.... \n",
            "85.12  % ,  0.41728365547657015  ,  0.3063365601837635\n",
            "epoch  11\n",
            "saving weights.... \n",
            "85.41  % ,  0.4263409376621246  ,  0.30062141748666765\n",
            "epoch  12\n",
            "saving weights.... \n",
            "84.75  % ,  0.4301580375730991  ,  0.2993545781314373\n",
            "epoch  13\n",
            "saving weights.... \n",
            "86.8  % ,  0.3888466456890106  ,  0.3001605219215155\n",
            "epoch  14\n",
            "saving weights.... \n",
            "83.1  % ,  0.4838635781288147  ,  0.28857334924191236\n",
            "epoch  15\n",
            "saving weights.... \n",
            "85.77  % ,  0.3970391839504242  ,  0.2958887568935752\n",
            "epoch  16\n",
            "saving weights.... \n",
            "83.94  % ,  0.4730676563739777  ,  0.2913057204186916\n",
            "epoch  17\n",
            "saving weights.... \n",
            "86.8  % ,  0.4092042642116547  ,  0.2899511738821864\n",
            "epoch  18\n",
            "saving weights.... \n",
            "84.72  % ,  0.46052137135267257  ,  0.28772570683062076\n",
            "epoch  19\n",
            "saving weights.... \n",
            "87.74  % ,  0.3666691172838211  ,  0.288915158714354\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.4  % ,  0.2740298171520233  ,  0.1594512967415154\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.66  % ,  0.2712009917289019  ,  0.1207222311206162\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.87  % ,  0.27113800839483737  ,  0.1067908319965005\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.79  % ,  0.26494597097337247  ,  0.0956673397064209\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.86  % ,  0.26264226617366077  ,  0.08745537828188389\n",
            "epoch  5\n",
            "saving weights.... \n",
            "91.01  % ,  0.280000558643043  ,  0.07853388442602009\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.94  % ,  0.28604126423299314  ,  0.07377669008150697\n",
            "epoch  7\n",
            "saving weights.... \n",
            "91.21  % ,  0.2697761808782816  ,  0.06875396049395203\n",
            "epoch  8\n",
            "saving weights.... \n",
            "91.05  % ,  0.27286303380355237  ,  0.06398133979346603\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.88  % ,  0.2776239315807819  ,  0.05858299945201725\n",
            "Finished Training\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.20\n",
            "features.20\n",
            "7001985.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "85.15  % ,  0.3708002372264862  ,  0.41692796715646985\n",
            "epoch  1\n",
            "saving weights.... \n",
            "82.11  % ,  0.48971969764232637  ,  0.37421365467607975\n",
            "epoch  2\n",
            "saving weights.... \n",
            "84.31  % ,  0.41674353976249695  ,  0.35454579228758815\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.26  % ,  0.39188318364620206  ,  0.34309344879984854\n",
            "epoch  4\n",
            "saving weights.... \n",
            "84.53  % ,  0.39843380572795867  ,  0.34314197900295257\n",
            "epoch  5\n",
            "saving weights.... \n",
            "85.41  % ,  0.4077720334291458  ,  0.3322065245807171\n",
            "epoch  6\n",
            "saving weights.... \n",
            "83.16  % ,  0.4816466623067856  ,  0.32839029222726823\n",
            "epoch  7\n",
            "saving weights.... \n",
            "86.04  % ,  0.3724334686279297  ,  0.32604403930306436\n",
            "epoch  8\n",
            "saving weights.... \n",
            "84.82  % ,  0.44125110244750976  ,  0.32569564197361467\n",
            "epoch  9\n",
            "saving weights.... \n",
            "86.19  % ,  0.3717778384685516  ,  0.31888572548925875\n",
            "epoch  10\n",
            "saving weights.... \n",
            "82.93  % ,  0.5158000182628631  ,  0.31425385211706164\n",
            "epoch  11\n",
            "saving weights.... \n",
            "85.73  % ,  0.4005237329006195  ,  0.3165456406235695\n",
            "epoch  12\n",
            "saving weights.... \n",
            "86.0  % ,  0.41214445420503615  ,  0.3086583954393864\n",
            "epoch  13\n",
            "saving weights.... \n",
            "84.06  % ,  0.4853523488998413  ,  0.30877431236207487\n",
            "epoch  14\n",
            "saving weights.... \n",
            "85.72  % ,  0.4253634637594223  ,  0.30699188857376575\n",
            "epoch  15\n",
            "saving weights.... \n",
            "83.16  % ,  0.5117775224804878  ,  0.2979971289038658\n",
            "epoch  16\n",
            "saving weights.... \n",
            "84.21  % ,  0.4819313868045807  ,  0.30334426183998586\n",
            "epoch  17\n",
            "saving weights.... \n",
            "84.17  % ,  0.4871253279924393  ,  0.30063340058922766\n",
            "epoch  18\n",
            "saving weights.... \n",
            "83.98  % ,  0.4731770854473114  ,  0.30395807024240495\n",
            "epoch  19\n",
            "saving weights.... \n",
            "86.16  % ,  0.4050045459151268  ,  0.30147513495534656\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.19  % ,  0.27685935913324355  ,  0.1731691106930375\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.28  % ,  0.2754180576503277  ,  0.13155584780201315\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.73  % ,  0.27229371944069863  ,  0.11328826212771237\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.73  % ,  0.2607552524715662  ,  0.1030955337766558\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.78  % ,  0.27564868397712705  ,  0.09395133979320526\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.8  % ,  0.2762982927471399  ,  0.0887753002680838\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.79  % ,  0.27614277687370775  ,  0.08402597782202065\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.94  % ,  0.28065700797885657  ,  0.07580802707131952\n",
            "epoch  8\n",
            "saving weights.... \n",
            "91.06  % ,  0.2730574522688985  ,  0.07368027067482472\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.92  % ,  0.2777109387651086  ,  0.06729593048393727\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a6515c99-162a-419a-9b13-13b8ff9ff05f\", \"features.20.npy\", 435)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.3 features.20': {'nbr_param': [510.0, 7098108.0],\n",
              "  'test_acc': [82.52,\n",
              "   85.01,\n",
              "   83.73,\n",
              "   85.0,\n",
              "   84.73,\n",
              "   85.54,\n",
              "   83.11,\n",
              "   86.34,\n",
              "   85.08,\n",
              "   84.93,\n",
              "   82.78,\n",
              "   84.63,\n",
              "   84.43,\n",
              "   84.6,\n",
              "   86.08,\n",
              "   85.62,\n",
              "   85.56,\n",
              "   85.69,\n",
              "   85.79,\n",
              "   83.26,\n",
              "   89.8,\n",
              "   89.89,\n",
              "   90.32,\n",
              "   90.63,\n",
              "   90.6,\n",
              "   90.71,\n",
              "   90.71,\n",
              "   90.78,\n",
              "   90.7,\n",
              "   90.6,\n",
              "   79.68,\n",
              "   81.94,\n",
              "   83.57,\n",
              "   82.51,\n",
              "   84.51,\n",
              "   83.98,\n",
              "   85.53,\n",
              "   84.25,\n",
              "   83.95,\n",
              "   84.22,\n",
              "   83.95,\n",
              "   85.34,\n",
              "   85.96,\n",
              "   84.3,\n",
              "   85.46,\n",
              "   86.59,\n",
              "   85.51,\n",
              "   85.05,\n",
              "   86.36,\n",
              "   84.16,\n",
              "   90.02,\n",
              "   90.22,\n",
              "   90.7,\n",
              "   90.59,\n",
              "   90.58,\n",
              "   90.48,\n",
              "   90.64,\n",
              "   90.22,\n",
              "   90.63,\n",
              "   90.55,\n",
              "   84.85,\n",
              "   85.14,\n",
              "   85.97,\n",
              "   86.47,\n",
              "   85.85,\n",
              "   84.84,\n",
              "   84.26,\n",
              "   86.4,\n",
              "   86.52,\n",
              "   86.11,\n",
              "   84.97,\n",
              "   85.45,\n",
              "   85.45,\n",
              "   85.88,\n",
              "   86.5,\n",
              "   86.6,\n",
              "   85.16,\n",
              "   86.44,\n",
              "   84.81,\n",
              "   86.56,\n",
              "   90.24,\n",
              "   90.62,\n",
              "   90.87,\n",
              "   90.99,\n",
              "   91.08,\n",
              "   91.25,\n",
              "   90.89,\n",
              "   91.03,\n",
              "   91.26,\n",
              "   91.05,\n",
              "   83.79,\n",
              "   86.59,\n",
              "   83.8,\n",
              "   85.63,\n",
              "   84.77,\n",
              "   85.62,\n",
              "   86.5,\n",
              "   84.33,\n",
              "   84.8,\n",
              "   85.55,\n",
              "   85.46,\n",
              "   85.83,\n",
              "   83.2,\n",
              "   84.77,\n",
              "   83.36,\n",
              "   86.32,\n",
              "   85.44,\n",
              "   85.26,\n",
              "   86.57,\n",
              "   85.36,\n",
              "   90.06,\n",
              "   90.39,\n",
              "   90.44,\n",
              "   90.47,\n",
              "   90.95,\n",
              "   90.83,\n",
              "   90.93,\n",
              "   90.64,\n",
              "   90.81,\n",
              "   91.12,\n",
              "   85.68,\n",
              "   84.31,\n",
              "   84.46,\n",
              "   86.06,\n",
              "   83.46,\n",
              "   83.73,\n",
              "   84.62,\n",
              "   87.1,\n",
              "   85.3,\n",
              "   84.62,\n",
              "   86.08,\n",
              "   84.97,\n",
              "   87.0,\n",
              "   86.06,\n",
              "   85.3,\n",
              "   85.98,\n",
              "   85.09,\n",
              "   87.13,\n",
              "   85.48,\n",
              "   86.39,\n",
              "   90.32,\n",
              "   90.41,\n",
              "   90.5,\n",
              "   90.43,\n",
              "   90.84,\n",
              "   90.88,\n",
              "   90.85,\n",
              "   90.81,\n",
              "   90.54,\n",
              "   90.83,\n",
              "   81.27,\n",
              "   83.92,\n",
              "   85.06,\n",
              "   81.7,\n",
              "   83.43,\n",
              "   85.79,\n",
              "   85.91,\n",
              "   84.34,\n",
              "   86.0,\n",
              "   85.35,\n",
              "   84.31,\n",
              "   82.39,\n",
              "   85.32,\n",
              "   85.23,\n",
              "   84.59,\n",
              "   86.29,\n",
              "   86.32,\n",
              "   85.2,\n",
              "   84.6,\n",
              "   86.04,\n",
              "   90.26,\n",
              "   90.42,\n",
              "   90.46,\n",
              "   90.81,\n",
              "   90.61,\n",
              "   90.94,\n",
              "   90.95,\n",
              "   90.5,\n",
              "   90.57,\n",
              "   90.78,\n",
              "   82.38,\n",
              "   85.91,\n",
              "   83.35,\n",
              "   82.6,\n",
              "   84.85,\n",
              "   84.45,\n",
              "   85.95,\n",
              "   85.39,\n",
              "   86.71,\n",
              "   86.42,\n",
              "   85.12,\n",
              "   85.41,\n",
              "   84.75,\n",
              "   86.8,\n",
              "   83.1,\n",
              "   85.77,\n",
              "   83.94,\n",
              "   86.8,\n",
              "   84.72,\n",
              "   87.74,\n",
              "   90.4,\n",
              "   90.66,\n",
              "   90.87,\n",
              "   90.79,\n",
              "   90.86,\n",
              "   91.01,\n",
              "   90.94,\n",
              "   91.21,\n",
              "   91.05,\n",
              "   90.88,\n",
              "   85.15,\n",
              "   82.11,\n",
              "   84.31,\n",
              "   85.26,\n",
              "   84.53,\n",
              "   85.41,\n",
              "   83.16,\n",
              "   86.04,\n",
              "   84.82,\n",
              "   86.19,\n",
              "   82.93,\n",
              "   85.73,\n",
              "   86.0,\n",
              "   84.06,\n",
              "   85.72,\n",
              "   83.16,\n",
              "   84.21,\n",
              "   84.17,\n",
              "   83.98,\n",
              "   86.16,\n",
              "   90.19,\n",
              "   90.28,\n",
              "   90.73,\n",
              "   90.73,\n",
              "   90.78,\n",
              "   90.8,\n",
              "   90.79,\n",
              "   90.94,\n",
              "   91.06,\n",
              "   90.92],\n",
              "  'training_loss': [0.40907566645145416,\n",
              "   0.360950541138649,\n",
              "   0.35040272166132924,\n",
              "   0.3473049049317837,\n",
              "   0.3275092122018337,\n",
              "   0.3349805044710636,\n",
              "   0.31958931329548357,\n",
              "   0.31514615920186045,\n",
              "   0.3107901745721698,\n",
              "   0.3090474818378687,\n",
              "   0.30886315927356484,\n",
              "   0.3099342880159616,\n",
              "   0.31027158932387827,\n",
              "   0.3020517555832863,\n",
              "   0.30246584529578685,\n",
              "   0.298972766559571,\n",
              "   0.2908178831547499,\n",
              "   0.30048629485815764,\n",
              "   0.2928548130407929,\n",
              "   0.2921842183619738,\n",
              "   0.16048257313892245,\n",
              "   0.12430203691832721,\n",
              "   0.10577896826770157,\n",
              "   0.09972059777788818,\n",
              "   0.09043372996971011,\n",
              "   0.08116793152783067,\n",
              "   0.07712968648150563,\n",
              "   0.0711976630795747,\n",
              "   0.0640951121110469,\n",
              "   0.06066714383419603,\n",
              "   0.6032445247888565,\n",
              "   0.4600371181309223,\n",
              "   0.42056222738623616,\n",
              "   0.40091566311120985,\n",
              "   0.39350112627148626,\n",
              "   0.3724687097400427,\n",
              "   0.37358303456008435,\n",
              "   0.36664005237817765,\n",
              "   0.3556244697600603,\n",
              "   0.3554153149843216,\n",
              "   0.35183879979252813,\n",
              "   0.3447562878936529,\n",
              "   0.3385072982698679,\n",
              "   0.3340740914642811,\n",
              "   0.3333474228441715,\n",
              "   0.3252881019204855,\n",
              "   0.3247955938056111,\n",
              "   0.324323251709342,\n",
              "   0.3257295282006264,\n",
              "   0.3136143458515406,\n",
              "   0.18703131733089687,\n",
              "   0.14455435569658875,\n",
              "   0.12962016634270548,\n",
              "   0.11906005847752094,\n",
              "   0.1089113010328263,\n",
              "   0.1044217544157058,\n",
              "   0.09653020181171595,\n",
              "   0.09167640583980828,\n",
              "   0.08709070926774293,\n",
              "   0.07952612738590688,\n",
              "   0.3915827317610383,\n",
              "   0.36015125019550326,\n",
              "   0.3394492732614279,\n",
              "   0.33276546038389204,\n",
              "   0.321261684307456,\n",
              "   0.317406006988883,\n",
              "   0.31472248481065035,\n",
              "   0.31368526110053063,\n",
              "   0.30694957921504973,\n",
              "   0.30771915841698644,\n",
              "   0.30400373374074696,\n",
              "   0.3019208874642849,\n",
              "   0.2940466021746397,\n",
              "   0.29928265756517647,\n",
              "   0.2930287833362818,\n",
              "   0.2921968050956726,\n",
              "   0.28996033757179973,\n",
              "   0.29818379101753234,\n",
              "   0.28905609567165375,\n",
              "   0.2829679045215249,\n",
              "   0.1546595427915454,\n",
              "   0.12215680264309049,\n",
              "   0.10269908316135407,\n",
              "   0.09301462987661362,\n",
              "   0.08477825286388398,\n",
              "   0.07879231826141476,\n",
              "   0.07056481614094227,\n",
              "   0.06797104831896722,\n",
              "   0.0619820798965171,\n",
              "   0.05927455047667027,\n",
              "   0.43319469038397074,\n",
              "   0.3730352936834097,\n",
              "   0.3572012962371111,\n",
              "   0.3487765677303076,\n",
              "   0.3396714374542236,\n",
              "   0.3310719349384308,\n",
              "   0.32411790349185465,\n",
              "   0.31850334607064723,\n",
              "   0.3203605338037014,\n",
              "   0.3191365311205387,\n",
              "   0.3103590044379234,\n",
              "   0.31409982268214226,\n",
              "   0.3018649290859699,\n",
              "   0.30651151714622976,\n",
              "   0.30179595678150656,\n",
              "   0.30144626151025294,\n",
              "   0.30031773758530617,\n",
              "   0.29617451620399954,\n",
              "   0.2962040895164013,\n",
              "   0.2949030253380537,\n",
              "   0.16728212798833847,\n",
              "   0.12332898807525634,\n",
              "   0.10870779219828546,\n",
              "   0.0953997129637748,\n",
              "   0.08869926975779235,\n",
              "   0.08311862417384983,\n",
              "   0.07547869409192354,\n",
              "   0.0719722601454705,\n",
              "   0.0650316122174263,\n",
              "   0.06117693085847423,\n",
              "   0.4165575761258602,\n",
              "   0.36688573741316793,\n",
              "   0.3547136522620916,\n",
              "   0.3405995216906071,\n",
              "   0.3373549881637096,\n",
              "   0.32738397109508516,\n",
              "   0.32126084246337416,\n",
              "   0.3222502499461174,\n",
              "   0.32008050666749477,\n",
              "   0.31754820965826513,\n",
              "   0.3089464570879936,\n",
              "   0.3091197503954172,\n",
              "   0.3076738418698311,\n",
              "   0.304758866456151,\n",
              "   0.2998647786572576,\n",
              "   0.29866554419100283,\n",
              "   0.29279589302241804,\n",
              "   0.29587005421966317,\n",
              "   0.29911903942972423,\n",
              "   0.29316971580684187,\n",
              "   0.1664878933750093,\n",
              "   0.1257459374267608,\n",
              "   0.10771592030227184,\n",
              "   0.09871435370519757,\n",
              "   0.09209347659815102,\n",
              "   0.08329371442031115,\n",
              "   0.07726244004238397,\n",
              "   0.07302833395041525,\n",
              "   0.06492400068454444,\n",
              "   0.06285624367371201,\n",
              "   0.4733887844115496,\n",
              "   0.3967711104929447,\n",
              "   0.38226252407431605,\n",
              "   0.37376976781487464,\n",
              "   0.3659503265142441,\n",
              "   0.34900617617666724,\n",
              "   0.3420859069138765,\n",
              "   0.3396369770780206,\n",
              "   0.3322650691509247,\n",
              "   0.3315376799687743,\n",
              "   0.3320910771906376,\n",
              "   0.32335208224058154,\n",
              "   0.32522230126559737,\n",
              "   0.32136273719966413,\n",
              "   0.3209088666766882,\n",
              "   0.31665384257137774,\n",
              "   0.31296653770804406,\n",
              "   0.31500618796646596,\n",
              "   0.31010513770729303,\n",
              "   0.30457976829111577,\n",
              "   0.1792228907942772,\n",
              "   0.1369694918051362,\n",
              "   0.12206199594661593,\n",
              "   0.11102653078995645,\n",
              "   0.10415978348068893,\n",
              "   0.09719408934563398,\n",
              "   0.08918758451528848,\n",
              "   0.08630386076215654,\n",
              "   0.08085922618024051,\n",
              "   0.0741429024990648,\n",
              "   0.3860620348814875,\n",
              "   0.3473760468780994,\n",
              "   0.34364650651216505,\n",
              "   0.3364586474478245,\n",
              "   0.32359960429370405,\n",
              "   0.32146553917229176,\n",
              "   0.31804814372062684,\n",
              "   0.31285910589694976,\n",
              "   0.3148858815908432,\n",
              "   0.30576938945651055,\n",
              "   0.3063365601837635,\n",
              "   0.30062141748666765,\n",
              "   0.2993545781314373,\n",
              "   0.3001605219215155,\n",
              "   0.28857334924191236,\n",
              "   0.2958887568935752,\n",
              "   0.2913057204186916,\n",
              "   0.2899511738821864,\n",
              "   0.28772570683062076,\n",
              "   0.288915158714354,\n",
              "   0.1594512967415154,\n",
              "   0.1207222311206162,\n",
              "   0.1067908319965005,\n",
              "   0.0956673397064209,\n",
              "   0.08745537828188389,\n",
              "   0.07853388442602009,\n",
              "   0.07377669008150697,\n",
              "   0.06875396049395203,\n",
              "   0.06398133979346603,\n",
              "   0.05858299945201725,\n",
              "   0.41692796715646985,\n",
              "   0.37421365467607975,\n",
              "   0.35454579228758815,\n",
              "   0.34309344879984854,\n",
              "   0.34314197900295257,\n",
              "   0.3322065245807171,\n",
              "   0.32839029222726823,\n",
              "   0.32604403930306436,\n",
              "   0.32569564197361467,\n",
              "   0.31888572548925875,\n",
              "   0.31425385211706164,\n",
              "   0.3165456406235695,\n",
              "   0.3086583954393864,\n",
              "   0.30877431236207487,\n",
              "   0.30699188857376575,\n",
              "   0.2979971289038658,\n",
              "   0.30334426183998586,\n",
              "   0.30063340058922766,\n",
              "   0.30395807024240495,\n",
              "   0.30147513495534656,\n",
              "   0.1731691106930375,\n",
              "   0.13155584780201315,\n",
              "   0.11328826212771237,\n",
              "   0.1030955337766558,\n",
              "   0.09395133979320526,\n",
              "   0.0887753002680838,\n",
              "   0.08402597782202065,\n",
              "   0.07580802707131952,\n",
              "   0.07368027067482472,\n",
              "   0.06729593048393727],\n",
              "  'valid_loss': [0.42210225051641465,\n",
              "   0.35383942351341247,\n",
              "   0.4297317171573639,\n",
              "   0.4053603218317032,\n",
              "   0.4291050954580307,\n",
              "   0.3971800770521164,\n",
              "   0.4493438600540161,\n",
              "   0.3755212651491165,\n",
              "   0.4161437240600586,\n",
              "   0.40946616864204405,\n",
              "   0.4856018295288086,\n",
              "   0.43796557857990265,\n",
              "   0.46040558100938794,\n",
              "   0.4405988154411316,\n",
              "   0.4151842958331108,\n",
              "   0.44308626794815065,\n",
              "   0.422457066488266,\n",
              "   0.40483203476667406,\n",
              "   0.40512289390563966,\n",
              "   0.49783689572811124,\n",
              "   0.28154569994807244,\n",
              "   0.27318392246067524,\n",
              "   0.2721190889239311,\n",
              "   0.268596945810318,\n",
              "   0.27571865207254886,\n",
              "   0.2710410477638245,\n",
              "   0.2686416128218174,\n",
              "   0.2839177329212427,\n",
              "   0.28436483749076724,\n",
              "   0.2866449775144458,\n",
              "   0.6017296772003173,\n",
              "   0.5101664099693298,\n",
              "   0.44742883722782134,\n",
              "   0.5044509061813355,\n",
              "   0.4397493969917297,\n",
              "   0.4753770266056061,\n",
              "   0.4013705028295517,\n",
              "   0.44403475792407987,\n",
              "   0.4547236580371857,\n",
              "   0.44203361523151397,\n",
              "   0.4756121888399124,\n",
              "   0.439184329867363,\n",
              "   0.4061865078806877,\n",
              "   0.508562934923172,\n",
              "   0.44241422488689425,\n",
              "   0.3968075229883194,\n",
              "   0.4381755865812302,\n",
              "   0.4360711445331574,\n",
              "   0.43369366626739503,\n",
              "   0.4615786414384842,\n",
              "   0.2850886409282684,\n",
              "   0.2834207551896572,\n",
              "   0.2861843703627586,\n",
              "   0.28229024144113063,\n",
              "   0.2847004294723272,\n",
              "   0.2934260986864567,\n",
              "   0.2896845830738545,\n",
              "   0.28826696096211674,\n",
              "   0.2827771405771375,\n",
              "   0.29946278535425663,\n",
              "   0.37885906960964205,\n",
              "   0.3813879222869873,\n",
              "   0.36162704025506975,\n",
              "   0.36307315928936007,\n",
              "   0.373699124789238,\n",
              "   0.4199017300724983,\n",
              "   0.4386263092637062,\n",
              "   0.37494810302257536,\n",
              "   0.38244445865154264,\n",
              "   0.39212416303157804,\n",
              "   0.42861008958816527,\n",
              "   0.40980375032424926,\n",
              "   0.39386013095378875,\n",
              "   0.39420974303483963,\n",
              "   0.3733188946247101,\n",
              "   0.39367760735750196,\n",
              "   0.4251572137236595,\n",
              "   0.387861485004425,\n",
              "   0.4270662139892578,\n",
              "   0.39478837317228316,\n",
              "   0.2804021812051535,\n",
              "   0.2687478130489588,\n",
              "   0.2640162861764431,\n",
              "   0.2640109902530909,\n",
              "   0.2667842405796051,\n",
              "   0.2732843278825283,\n",
              "   0.2812460358470678,\n",
              "   0.2759890424236655,\n",
              "   0.2865777604825795,\n",
              "   0.27957971667200326,\n",
              "   0.4035875594854355,\n",
              "   0.34408994076251986,\n",
              "   0.4150781086206436,\n",
              "   0.3763122297525406,\n",
              "   0.41687004276514056,\n",
              "   0.3772286298036575,\n",
              "   0.37435594547986983,\n",
              "   0.436659460735321,\n",
              "   0.4233722271308303,\n",
              "   0.39898086166381835,\n",
              "   0.4139880071401596,\n",
              "   0.3978924362182617,\n",
              "   0.5051281242370605,\n",
              "   0.43509574440717697,\n",
              "   0.47109321502447127,\n",
              "   0.4048124081730843,\n",
              "   0.42337273358106614,\n",
              "   0.4663681983053684,\n",
              "   0.4163111712932587,\n",
              "   0.43114149227142334,\n",
              "   0.27357843561172485,\n",
              "   0.28219130519777536,\n",
              "   0.2825168494999409,\n",
              "   0.27767424177229405,\n",
              "   0.2851933906078339,\n",
              "   0.28464450816810133,\n",
              "   0.2854661216765642,\n",
              "   0.2778184846512973,\n",
              "   0.28704436569809916,\n",
              "   0.2970047707542777,\n",
              "   0.3673545283079147,\n",
              "   0.391107876932621,\n",
              "   0.424245005774498,\n",
              "   0.38230340807437896,\n",
              "   0.4369804766893387,\n",
              "   0.4667327652692795,\n",
              "   0.42256137087345125,\n",
              "   0.3763585899591446,\n",
              "   0.43210205738544466,\n",
              "   0.42068634634017943,\n",
              "   0.3826380405664444,\n",
              "   0.45791608271598816,\n",
              "   0.3638025357246399,\n",
              "   0.40323217886686324,\n",
              "   0.43324824776649473,\n",
              "   0.3852114422798157,\n",
              "   0.42970344882011413,\n",
              "   0.3686750558257103,\n",
              "   0.43478653070926665,\n",
              "   0.41608525705337523,\n",
              "   0.2787297412753105,\n",
              "   0.26794817788004877,\n",
              "   0.2755814252406359,\n",
              "   0.28084198917746545,\n",
              "   0.2787814876049757,\n",
              "   0.2818325390279293,\n",
              "   0.27881788937449453,\n",
              "   0.27852412420213224,\n",
              "   0.2844353796288371,\n",
              "   0.2779920490384102,\n",
              "   0.5206531499624252,\n",
              "   0.429666927754879,\n",
              "   0.4009420718193054,\n",
              "   0.5106000027656555,\n",
              "   0.48909627113342286,\n",
              "   0.400659051322937,\n",
              "   0.40851053712368013,\n",
              "   0.4551992407798767,\n",
              "   0.4114827463388443,\n",
              "   0.42719521021842954,\n",
              "   0.4475166934490204,\n",
              "   0.5524183402776718,\n",
              "   0.4373073227286339,\n",
              "   0.4312356474399567,\n",
              "   0.4466846562623978,\n",
              "   0.40197948117256166,\n",
              "   0.396400754570961,\n",
              "   0.4239976851940155,\n",
              "   0.47031918416023255,\n",
              "   0.4100718839645386,\n",
              "   0.2802477910161018,\n",
              "   0.26757230888605116,\n",
              "   0.2751334415078163,\n",
              "   0.2723855876505375,\n",
              "   0.27121414547860623,\n",
              "   0.2727083999410272,\n",
              "   0.2719268013358116,\n",
              "   0.27415639928877356,\n",
              "   0.2805948691904545,\n",
              "   0.28089840899407864,\n",
              "   0.44386775991916655,\n",
              "   0.3670612971305847,\n",
              "   0.4275624608039856,\n",
              "   0.4840765997886658,\n",
              "   0.3922687263250351,\n",
              "   0.41186701002120973,\n",
              "   0.38272371337413785,\n",
              "   0.41316329424381254,\n",
              "   0.3868884770154953,\n",
              "   0.36779265110492704,\n",
              "   0.41728365547657015,\n",
              "   0.4263409376621246,\n",
              "   0.4301580375730991,\n",
              "   0.3888466456890106,\n",
              "   0.4838635781288147,\n",
              "   0.3970391839504242,\n",
              "   0.4730676563739777,\n",
              "   0.4092042642116547,\n",
              "   0.46052137135267257,\n",
              "   0.3666691172838211,\n",
              "   0.2740298171520233,\n",
              "   0.2712009917289019,\n",
              "   0.27113800839483737,\n",
              "   0.26494597097337247,\n",
              "   0.26264226617366077,\n",
              "   0.280000558643043,\n",
              "   0.28604126423299314,\n",
              "   0.2697761808782816,\n",
              "   0.27286303380355237,\n",
              "   0.2776239315807819,\n",
              "   0.3708002372264862,\n",
              "   0.48971969764232637,\n",
              "   0.41674353976249695,\n",
              "   0.39188318364620206,\n",
              "   0.39843380572795867,\n",
              "   0.4077720334291458,\n",
              "   0.4816466623067856,\n",
              "   0.3724334686279297,\n",
              "   0.44125110244750976,\n",
              "   0.3717778384685516,\n",
              "   0.5158000182628631,\n",
              "   0.4005237329006195,\n",
              "   0.41214445420503615,\n",
              "   0.4853523488998413,\n",
              "   0.4253634637594223,\n",
              "   0.5117775224804878,\n",
              "   0.4819313868045807,\n",
              "   0.4871253279924393,\n",
              "   0.4731770854473114,\n",
              "   0.4050045459151268,\n",
              "   0.27685935913324355,\n",
              "   0.2754180576503277,\n",
              "   0.27229371944069863,\n",
              "   0.2607552524715662,\n",
              "   0.27564868397712705,\n",
              "   0.2762982927471399,\n",
              "   0.27614277687370775,\n",
              "   0.28065700797885657,\n",
              "   0.2730574522688985,\n",
              "   0.2777109387651086]},\n",
              " '0.6 features.20': {'nbr_param': [510.0, 7001985.0],\n",
              "  'test_acc': [82.52,\n",
              "   85.01,\n",
              "   83.73,\n",
              "   85.0,\n",
              "   84.73,\n",
              "   85.54,\n",
              "   83.11,\n",
              "   86.34,\n",
              "   85.08,\n",
              "   84.93,\n",
              "   82.78,\n",
              "   84.63,\n",
              "   84.43,\n",
              "   84.6,\n",
              "   86.08,\n",
              "   85.62,\n",
              "   85.56,\n",
              "   85.69,\n",
              "   85.79,\n",
              "   83.26,\n",
              "   89.8,\n",
              "   89.89,\n",
              "   90.32,\n",
              "   90.63,\n",
              "   90.6,\n",
              "   90.71,\n",
              "   90.71,\n",
              "   90.78,\n",
              "   90.7,\n",
              "   90.6,\n",
              "   79.68,\n",
              "   81.94,\n",
              "   83.57,\n",
              "   82.51,\n",
              "   84.51,\n",
              "   83.98,\n",
              "   85.53,\n",
              "   84.25,\n",
              "   83.95,\n",
              "   84.22,\n",
              "   83.95,\n",
              "   85.34,\n",
              "   85.96,\n",
              "   84.3,\n",
              "   85.46,\n",
              "   86.59,\n",
              "   85.51,\n",
              "   85.05,\n",
              "   86.36,\n",
              "   84.16,\n",
              "   90.02,\n",
              "   90.22,\n",
              "   90.7,\n",
              "   90.59,\n",
              "   90.58,\n",
              "   90.48,\n",
              "   90.64,\n",
              "   90.22,\n",
              "   90.63,\n",
              "   90.55,\n",
              "   84.85,\n",
              "   85.14,\n",
              "   85.97,\n",
              "   86.47,\n",
              "   85.85,\n",
              "   84.84,\n",
              "   84.26,\n",
              "   86.4,\n",
              "   86.52,\n",
              "   86.11,\n",
              "   84.97,\n",
              "   85.45,\n",
              "   85.45,\n",
              "   85.88,\n",
              "   86.5,\n",
              "   86.6,\n",
              "   85.16,\n",
              "   86.44,\n",
              "   84.81,\n",
              "   86.56,\n",
              "   90.24,\n",
              "   90.62,\n",
              "   90.87,\n",
              "   90.99,\n",
              "   91.08,\n",
              "   91.25,\n",
              "   90.89,\n",
              "   91.03,\n",
              "   91.26,\n",
              "   91.05,\n",
              "   83.79,\n",
              "   86.59,\n",
              "   83.8,\n",
              "   85.63,\n",
              "   84.77,\n",
              "   85.62,\n",
              "   86.5,\n",
              "   84.33,\n",
              "   84.8,\n",
              "   85.55,\n",
              "   85.46,\n",
              "   85.83,\n",
              "   83.2,\n",
              "   84.77,\n",
              "   83.36,\n",
              "   86.32,\n",
              "   85.44,\n",
              "   85.26,\n",
              "   86.57,\n",
              "   85.36,\n",
              "   90.06,\n",
              "   90.39,\n",
              "   90.44,\n",
              "   90.47,\n",
              "   90.95,\n",
              "   90.83,\n",
              "   90.93,\n",
              "   90.64,\n",
              "   90.81,\n",
              "   91.12,\n",
              "   85.68,\n",
              "   84.31,\n",
              "   84.46,\n",
              "   86.06,\n",
              "   83.46,\n",
              "   83.73,\n",
              "   84.62,\n",
              "   87.1,\n",
              "   85.3,\n",
              "   84.62,\n",
              "   86.08,\n",
              "   84.97,\n",
              "   87.0,\n",
              "   86.06,\n",
              "   85.3,\n",
              "   85.98,\n",
              "   85.09,\n",
              "   87.13,\n",
              "   85.48,\n",
              "   86.39,\n",
              "   90.32,\n",
              "   90.41,\n",
              "   90.5,\n",
              "   90.43,\n",
              "   90.84,\n",
              "   90.88,\n",
              "   90.85,\n",
              "   90.81,\n",
              "   90.54,\n",
              "   90.83,\n",
              "   81.27,\n",
              "   83.92,\n",
              "   85.06,\n",
              "   81.7,\n",
              "   83.43,\n",
              "   85.79,\n",
              "   85.91,\n",
              "   84.34,\n",
              "   86.0,\n",
              "   85.35,\n",
              "   84.31,\n",
              "   82.39,\n",
              "   85.32,\n",
              "   85.23,\n",
              "   84.59,\n",
              "   86.29,\n",
              "   86.32,\n",
              "   85.2,\n",
              "   84.6,\n",
              "   86.04,\n",
              "   90.26,\n",
              "   90.42,\n",
              "   90.46,\n",
              "   90.81,\n",
              "   90.61,\n",
              "   90.94,\n",
              "   90.95,\n",
              "   90.5,\n",
              "   90.57,\n",
              "   90.78,\n",
              "   82.38,\n",
              "   85.91,\n",
              "   83.35,\n",
              "   82.6,\n",
              "   84.85,\n",
              "   84.45,\n",
              "   85.95,\n",
              "   85.39,\n",
              "   86.71,\n",
              "   86.42,\n",
              "   85.12,\n",
              "   85.41,\n",
              "   84.75,\n",
              "   86.8,\n",
              "   83.1,\n",
              "   85.77,\n",
              "   83.94,\n",
              "   86.8,\n",
              "   84.72,\n",
              "   87.74,\n",
              "   90.4,\n",
              "   90.66,\n",
              "   90.87,\n",
              "   90.79,\n",
              "   90.86,\n",
              "   91.01,\n",
              "   90.94,\n",
              "   91.21,\n",
              "   91.05,\n",
              "   90.88,\n",
              "   85.15,\n",
              "   82.11,\n",
              "   84.31,\n",
              "   85.26,\n",
              "   84.53,\n",
              "   85.41,\n",
              "   83.16,\n",
              "   86.04,\n",
              "   84.82,\n",
              "   86.19,\n",
              "   82.93,\n",
              "   85.73,\n",
              "   86.0,\n",
              "   84.06,\n",
              "   85.72,\n",
              "   83.16,\n",
              "   84.21,\n",
              "   84.17,\n",
              "   83.98,\n",
              "   86.16,\n",
              "   90.19,\n",
              "   90.28,\n",
              "   90.73,\n",
              "   90.73,\n",
              "   90.78,\n",
              "   90.8,\n",
              "   90.79,\n",
              "   90.94,\n",
              "   91.06,\n",
              "   90.92],\n",
              "  'training_loss': [0.40907566645145416,\n",
              "   0.360950541138649,\n",
              "   0.35040272166132924,\n",
              "   0.3473049049317837,\n",
              "   0.3275092122018337,\n",
              "   0.3349805044710636,\n",
              "   0.31958931329548357,\n",
              "   0.31514615920186045,\n",
              "   0.3107901745721698,\n",
              "   0.3090474818378687,\n",
              "   0.30886315927356484,\n",
              "   0.3099342880159616,\n",
              "   0.31027158932387827,\n",
              "   0.3020517555832863,\n",
              "   0.30246584529578685,\n",
              "   0.298972766559571,\n",
              "   0.2908178831547499,\n",
              "   0.30048629485815764,\n",
              "   0.2928548130407929,\n",
              "   0.2921842183619738,\n",
              "   0.16048257313892245,\n",
              "   0.12430203691832721,\n",
              "   0.10577896826770157,\n",
              "   0.09972059777788818,\n",
              "   0.09043372996971011,\n",
              "   0.08116793152783067,\n",
              "   0.07712968648150563,\n",
              "   0.0711976630795747,\n",
              "   0.0640951121110469,\n",
              "   0.06066714383419603,\n",
              "   0.6032445247888565,\n",
              "   0.4600371181309223,\n",
              "   0.42056222738623616,\n",
              "   0.40091566311120985,\n",
              "   0.39350112627148626,\n",
              "   0.3724687097400427,\n",
              "   0.37358303456008435,\n",
              "   0.36664005237817765,\n",
              "   0.3556244697600603,\n",
              "   0.3554153149843216,\n",
              "   0.35183879979252813,\n",
              "   0.3447562878936529,\n",
              "   0.3385072982698679,\n",
              "   0.3340740914642811,\n",
              "   0.3333474228441715,\n",
              "   0.3252881019204855,\n",
              "   0.3247955938056111,\n",
              "   0.324323251709342,\n",
              "   0.3257295282006264,\n",
              "   0.3136143458515406,\n",
              "   0.18703131733089687,\n",
              "   0.14455435569658875,\n",
              "   0.12962016634270548,\n",
              "   0.11906005847752094,\n",
              "   0.1089113010328263,\n",
              "   0.1044217544157058,\n",
              "   0.09653020181171595,\n",
              "   0.09167640583980828,\n",
              "   0.08709070926774293,\n",
              "   0.07952612738590688,\n",
              "   0.3915827317610383,\n",
              "   0.36015125019550326,\n",
              "   0.3394492732614279,\n",
              "   0.33276546038389204,\n",
              "   0.321261684307456,\n",
              "   0.317406006988883,\n",
              "   0.31472248481065035,\n",
              "   0.31368526110053063,\n",
              "   0.30694957921504973,\n",
              "   0.30771915841698644,\n",
              "   0.30400373374074696,\n",
              "   0.3019208874642849,\n",
              "   0.2940466021746397,\n",
              "   0.29928265756517647,\n",
              "   0.2930287833362818,\n",
              "   0.2921968050956726,\n",
              "   0.28996033757179973,\n",
              "   0.29818379101753234,\n",
              "   0.28905609567165375,\n",
              "   0.2829679045215249,\n",
              "   0.1546595427915454,\n",
              "   0.12215680264309049,\n",
              "   0.10269908316135407,\n",
              "   0.09301462987661362,\n",
              "   0.08477825286388398,\n",
              "   0.07879231826141476,\n",
              "   0.07056481614094227,\n",
              "   0.06797104831896722,\n",
              "   0.0619820798965171,\n",
              "   0.05927455047667027,\n",
              "   0.43319469038397074,\n",
              "   0.3730352936834097,\n",
              "   0.3572012962371111,\n",
              "   0.3487765677303076,\n",
              "   0.3396714374542236,\n",
              "   0.3310719349384308,\n",
              "   0.32411790349185465,\n",
              "   0.31850334607064723,\n",
              "   0.3203605338037014,\n",
              "   0.3191365311205387,\n",
              "   0.3103590044379234,\n",
              "   0.31409982268214226,\n",
              "   0.3018649290859699,\n",
              "   0.30651151714622976,\n",
              "   0.30179595678150656,\n",
              "   0.30144626151025294,\n",
              "   0.30031773758530617,\n",
              "   0.29617451620399954,\n",
              "   0.2962040895164013,\n",
              "   0.2949030253380537,\n",
              "   0.16728212798833847,\n",
              "   0.12332898807525634,\n",
              "   0.10870779219828546,\n",
              "   0.0953997129637748,\n",
              "   0.08869926975779235,\n",
              "   0.08311862417384983,\n",
              "   0.07547869409192354,\n",
              "   0.0719722601454705,\n",
              "   0.0650316122174263,\n",
              "   0.06117693085847423,\n",
              "   0.4165575761258602,\n",
              "   0.36688573741316793,\n",
              "   0.3547136522620916,\n",
              "   0.3405995216906071,\n",
              "   0.3373549881637096,\n",
              "   0.32738397109508516,\n",
              "   0.32126084246337416,\n",
              "   0.3222502499461174,\n",
              "   0.32008050666749477,\n",
              "   0.31754820965826513,\n",
              "   0.3089464570879936,\n",
              "   0.3091197503954172,\n",
              "   0.3076738418698311,\n",
              "   0.304758866456151,\n",
              "   0.2998647786572576,\n",
              "   0.29866554419100283,\n",
              "   0.29279589302241804,\n",
              "   0.29587005421966317,\n",
              "   0.29911903942972423,\n",
              "   0.29316971580684187,\n",
              "   0.1664878933750093,\n",
              "   0.1257459374267608,\n",
              "   0.10771592030227184,\n",
              "   0.09871435370519757,\n",
              "   0.09209347659815102,\n",
              "   0.08329371442031115,\n",
              "   0.07726244004238397,\n",
              "   0.07302833395041525,\n",
              "   0.06492400068454444,\n",
              "   0.06285624367371201,\n",
              "   0.4733887844115496,\n",
              "   0.3967711104929447,\n",
              "   0.38226252407431605,\n",
              "   0.37376976781487464,\n",
              "   0.3659503265142441,\n",
              "   0.34900617617666724,\n",
              "   0.3420859069138765,\n",
              "   0.3396369770780206,\n",
              "   0.3322650691509247,\n",
              "   0.3315376799687743,\n",
              "   0.3320910771906376,\n",
              "   0.32335208224058154,\n",
              "   0.32522230126559737,\n",
              "   0.32136273719966413,\n",
              "   0.3209088666766882,\n",
              "   0.31665384257137774,\n",
              "   0.31296653770804406,\n",
              "   0.31500618796646596,\n",
              "   0.31010513770729303,\n",
              "   0.30457976829111577,\n",
              "   0.1792228907942772,\n",
              "   0.1369694918051362,\n",
              "   0.12206199594661593,\n",
              "   0.11102653078995645,\n",
              "   0.10415978348068893,\n",
              "   0.09719408934563398,\n",
              "   0.08918758451528848,\n",
              "   0.08630386076215654,\n",
              "   0.08085922618024051,\n",
              "   0.0741429024990648,\n",
              "   0.3860620348814875,\n",
              "   0.3473760468780994,\n",
              "   0.34364650651216505,\n",
              "   0.3364586474478245,\n",
              "   0.32359960429370405,\n",
              "   0.32146553917229176,\n",
              "   0.31804814372062684,\n",
              "   0.31285910589694976,\n",
              "   0.3148858815908432,\n",
              "   0.30576938945651055,\n",
              "   0.3063365601837635,\n",
              "   0.30062141748666765,\n",
              "   0.2993545781314373,\n",
              "   0.3001605219215155,\n",
              "   0.28857334924191236,\n",
              "   0.2958887568935752,\n",
              "   0.2913057204186916,\n",
              "   0.2899511738821864,\n",
              "   0.28772570683062076,\n",
              "   0.288915158714354,\n",
              "   0.1594512967415154,\n",
              "   0.1207222311206162,\n",
              "   0.1067908319965005,\n",
              "   0.0956673397064209,\n",
              "   0.08745537828188389,\n",
              "   0.07853388442602009,\n",
              "   0.07377669008150697,\n",
              "   0.06875396049395203,\n",
              "   0.06398133979346603,\n",
              "   0.05858299945201725,\n",
              "   0.41692796715646985,\n",
              "   0.37421365467607975,\n",
              "   0.35454579228758815,\n",
              "   0.34309344879984854,\n",
              "   0.34314197900295257,\n",
              "   0.3322065245807171,\n",
              "   0.32839029222726823,\n",
              "   0.32604403930306436,\n",
              "   0.32569564197361467,\n",
              "   0.31888572548925875,\n",
              "   0.31425385211706164,\n",
              "   0.3165456406235695,\n",
              "   0.3086583954393864,\n",
              "   0.30877431236207487,\n",
              "   0.30699188857376575,\n",
              "   0.2979971289038658,\n",
              "   0.30334426183998586,\n",
              "   0.30063340058922766,\n",
              "   0.30395807024240495,\n",
              "   0.30147513495534656,\n",
              "   0.1731691106930375,\n",
              "   0.13155584780201315,\n",
              "   0.11328826212771237,\n",
              "   0.1030955337766558,\n",
              "   0.09395133979320526,\n",
              "   0.0887753002680838,\n",
              "   0.08402597782202065,\n",
              "   0.07580802707131952,\n",
              "   0.07368027067482472,\n",
              "   0.06729593048393727],\n",
              "  'valid_loss': [0.42210225051641465,\n",
              "   0.35383942351341247,\n",
              "   0.4297317171573639,\n",
              "   0.4053603218317032,\n",
              "   0.4291050954580307,\n",
              "   0.3971800770521164,\n",
              "   0.4493438600540161,\n",
              "   0.3755212651491165,\n",
              "   0.4161437240600586,\n",
              "   0.40946616864204405,\n",
              "   0.4856018295288086,\n",
              "   0.43796557857990265,\n",
              "   0.46040558100938794,\n",
              "   0.4405988154411316,\n",
              "   0.4151842958331108,\n",
              "   0.44308626794815065,\n",
              "   0.422457066488266,\n",
              "   0.40483203476667406,\n",
              "   0.40512289390563966,\n",
              "   0.49783689572811124,\n",
              "   0.28154569994807244,\n",
              "   0.27318392246067524,\n",
              "   0.2721190889239311,\n",
              "   0.268596945810318,\n",
              "   0.27571865207254886,\n",
              "   0.2710410477638245,\n",
              "   0.2686416128218174,\n",
              "   0.2839177329212427,\n",
              "   0.28436483749076724,\n",
              "   0.2866449775144458,\n",
              "   0.6017296772003173,\n",
              "   0.5101664099693298,\n",
              "   0.44742883722782134,\n",
              "   0.5044509061813355,\n",
              "   0.4397493969917297,\n",
              "   0.4753770266056061,\n",
              "   0.4013705028295517,\n",
              "   0.44403475792407987,\n",
              "   0.4547236580371857,\n",
              "   0.44203361523151397,\n",
              "   0.4756121888399124,\n",
              "   0.439184329867363,\n",
              "   0.4061865078806877,\n",
              "   0.508562934923172,\n",
              "   0.44241422488689425,\n",
              "   0.3968075229883194,\n",
              "   0.4381755865812302,\n",
              "   0.4360711445331574,\n",
              "   0.43369366626739503,\n",
              "   0.4615786414384842,\n",
              "   0.2850886409282684,\n",
              "   0.2834207551896572,\n",
              "   0.2861843703627586,\n",
              "   0.28229024144113063,\n",
              "   0.2847004294723272,\n",
              "   0.2934260986864567,\n",
              "   0.2896845830738545,\n",
              "   0.28826696096211674,\n",
              "   0.2827771405771375,\n",
              "   0.29946278535425663,\n",
              "   0.37885906960964205,\n",
              "   0.3813879222869873,\n",
              "   0.36162704025506975,\n",
              "   0.36307315928936007,\n",
              "   0.373699124789238,\n",
              "   0.4199017300724983,\n",
              "   0.4386263092637062,\n",
              "   0.37494810302257536,\n",
              "   0.38244445865154264,\n",
              "   0.39212416303157804,\n",
              "   0.42861008958816527,\n",
              "   0.40980375032424926,\n",
              "   0.39386013095378875,\n",
              "   0.39420974303483963,\n",
              "   0.3733188946247101,\n",
              "   0.39367760735750196,\n",
              "   0.4251572137236595,\n",
              "   0.387861485004425,\n",
              "   0.4270662139892578,\n",
              "   0.39478837317228316,\n",
              "   0.2804021812051535,\n",
              "   0.2687478130489588,\n",
              "   0.2640162861764431,\n",
              "   0.2640109902530909,\n",
              "   0.2667842405796051,\n",
              "   0.2732843278825283,\n",
              "   0.2812460358470678,\n",
              "   0.2759890424236655,\n",
              "   0.2865777604825795,\n",
              "   0.27957971667200326,\n",
              "   0.4035875594854355,\n",
              "   0.34408994076251986,\n",
              "   0.4150781086206436,\n",
              "   0.3763122297525406,\n",
              "   0.41687004276514056,\n",
              "   0.3772286298036575,\n",
              "   0.37435594547986983,\n",
              "   0.436659460735321,\n",
              "   0.4233722271308303,\n",
              "   0.39898086166381835,\n",
              "   0.4139880071401596,\n",
              "   0.3978924362182617,\n",
              "   0.5051281242370605,\n",
              "   0.43509574440717697,\n",
              "   0.47109321502447127,\n",
              "   0.4048124081730843,\n",
              "   0.42337273358106614,\n",
              "   0.4663681983053684,\n",
              "   0.4163111712932587,\n",
              "   0.43114149227142334,\n",
              "   0.27357843561172485,\n",
              "   0.28219130519777536,\n",
              "   0.2825168494999409,\n",
              "   0.27767424177229405,\n",
              "   0.2851933906078339,\n",
              "   0.28464450816810133,\n",
              "   0.2854661216765642,\n",
              "   0.2778184846512973,\n",
              "   0.28704436569809916,\n",
              "   0.2970047707542777,\n",
              "   0.3673545283079147,\n",
              "   0.391107876932621,\n",
              "   0.424245005774498,\n",
              "   0.38230340807437896,\n",
              "   0.4369804766893387,\n",
              "   0.4667327652692795,\n",
              "   0.42256137087345125,\n",
              "   0.3763585899591446,\n",
              "   0.43210205738544466,\n",
              "   0.42068634634017943,\n",
              "   0.3826380405664444,\n",
              "   0.45791608271598816,\n",
              "   0.3638025357246399,\n",
              "   0.40323217886686324,\n",
              "   0.43324824776649473,\n",
              "   0.3852114422798157,\n",
              "   0.42970344882011413,\n",
              "   0.3686750558257103,\n",
              "   0.43478653070926665,\n",
              "   0.41608525705337523,\n",
              "   0.2787297412753105,\n",
              "   0.26794817788004877,\n",
              "   0.2755814252406359,\n",
              "   0.28084198917746545,\n",
              "   0.2787814876049757,\n",
              "   0.2818325390279293,\n",
              "   0.27881788937449453,\n",
              "   0.27852412420213224,\n",
              "   0.2844353796288371,\n",
              "   0.2779920490384102,\n",
              "   0.5206531499624252,\n",
              "   0.429666927754879,\n",
              "   0.4009420718193054,\n",
              "   0.5106000027656555,\n",
              "   0.48909627113342286,\n",
              "   0.400659051322937,\n",
              "   0.40851053712368013,\n",
              "   0.4551992407798767,\n",
              "   0.4114827463388443,\n",
              "   0.42719521021842954,\n",
              "   0.4475166934490204,\n",
              "   0.5524183402776718,\n",
              "   0.4373073227286339,\n",
              "   0.4312356474399567,\n",
              "   0.4466846562623978,\n",
              "   0.40197948117256166,\n",
              "   0.396400754570961,\n",
              "   0.4239976851940155,\n",
              "   0.47031918416023255,\n",
              "   0.4100718839645386,\n",
              "   0.2802477910161018,\n",
              "   0.26757230888605116,\n",
              "   0.2751334415078163,\n",
              "   0.2723855876505375,\n",
              "   0.27121414547860623,\n",
              "   0.2727083999410272,\n",
              "   0.2719268013358116,\n",
              "   0.27415639928877356,\n",
              "   0.2805948691904545,\n",
              "   0.28089840899407864,\n",
              "   0.44386775991916655,\n",
              "   0.3670612971305847,\n",
              "   0.4275624608039856,\n",
              "   0.4840765997886658,\n",
              "   0.3922687263250351,\n",
              "   0.41186701002120973,\n",
              "   0.38272371337413785,\n",
              "   0.41316329424381254,\n",
              "   0.3868884770154953,\n",
              "   0.36779265110492704,\n",
              "   0.41728365547657015,\n",
              "   0.4263409376621246,\n",
              "   0.4301580375730991,\n",
              "   0.3888466456890106,\n",
              "   0.4838635781288147,\n",
              "   0.3970391839504242,\n",
              "   0.4730676563739777,\n",
              "   0.4092042642116547,\n",
              "   0.46052137135267257,\n",
              "   0.3666691172838211,\n",
              "   0.2740298171520233,\n",
              "   0.2712009917289019,\n",
              "   0.27113800839483737,\n",
              "   0.26494597097337247,\n",
              "   0.26264226617366077,\n",
              "   0.280000558643043,\n",
              "   0.28604126423299314,\n",
              "   0.2697761808782816,\n",
              "   0.27286303380355237,\n",
              "   0.2776239315807819,\n",
              "   0.3708002372264862,\n",
              "   0.48971969764232637,\n",
              "   0.41674353976249695,\n",
              "   0.39188318364620206,\n",
              "   0.39843380572795867,\n",
              "   0.4077720334291458,\n",
              "   0.4816466623067856,\n",
              "   0.3724334686279297,\n",
              "   0.44125110244750976,\n",
              "   0.3717778384685516,\n",
              "   0.5158000182628631,\n",
              "   0.4005237329006195,\n",
              "   0.41214445420503615,\n",
              "   0.4853523488998413,\n",
              "   0.4253634637594223,\n",
              "   0.5117775224804878,\n",
              "   0.4819313868045807,\n",
              "   0.4871253279924393,\n",
              "   0.4731770854473114,\n",
              "   0.4050045459151268,\n",
              "   0.27685935913324355,\n",
              "   0.2754180576503277,\n",
              "   0.27229371944069863,\n",
              "   0.2607552524715662,\n",
              "   0.27564868397712705,\n",
              "   0.2762982927471399,\n",
              "   0.27614277687370775,\n",
              "   0.28065700797885657,\n",
              "   0.2730574522688985,\n",
              "   0.2777109387651086]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDRdC0a4nbAT"
      },
      "source": [
        "### Prune sur \"features.24\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OgtIgIippoN0",
        "outputId": "8929d7ec-29b1-4da3-ae79-853de4f0870e"
      },
      "source": [
        "prune_fine_tune(7,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chargez les poids\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3a77d05a-66c0-474a-921d-4f645ed62b86\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3a77d05a-66c0-474a-921d-4f645ed62b86\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving checkpoint.pt to checkpoint.pt\n",
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.24\n",
            "features.24\n",
            "7001985.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "83.82  % ,  0.4152320156812668  ,  0.3753677779302001\n",
            "epoch  1\n",
            "saving weights.... \n",
            "85.64  % ,  0.3291621312737465  ,  0.3462250084847212\n",
            "epoch  2\n",
            "saving weights.... \n",
            "84.67  % ,  0.42519748874902724  ,  0.33190153065472844\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.43  % ,  0.3677966022372246  ,  0.33085808129012584\n",
            "epoch  4\n",
            "saving weights.... \n",
            "84.26  % ,  0.42426420383453367  ,  0.31621220431029795\n",
            "epoch  5\n",
            "saving weights.... \n",
            "83.59  % ,  0.45190358481407167  ,  0.31845793751180174\n",
            "epoch  6\n",
            "saving weights.... \n",
            "86.91  % ,  0.3446594207048416  ,  0.3156200372368097\n",
            "epoch  7\n",
            "saving weights.... \n",
            "85.1  % ,  0.4067449975728989  ,  0.30667638978362083\n",
            "epoch  8\n",
            "saving weights.... \n",
            "85.63  % ,  0.41236174714565277  ,  0.3017140057712793\n",
            "epoch  9\n",
            "saving weights.... \n",
            "86.39  % ,  0.39068249773979186  ,  0.30335422294437886\n",
            "epoch  10\n",
            "saving weights.... \n",
            "85.28  % ,  0.4123280806303024  ,  0.29924887719750404\n",
            "epoch  11\n",
            "saving weights.... \n",
            "83.62  % ,  0.4497375878095627  ,  0.2995054669216275\n",
            "epoch  12\n",
            "saving weights.... \n",
            "85.92  % ,  0.41055611610412596  ,  0.30042183919698\n",
            "epoch  13\n",
            "saving weights.... \n",
            "87.37  % ,  0.36766749041080476  ,  0.29423499591201546\n",
            "epoch  14\n",
            "saving weights.... \n",
            "85.31  % ,  0.4245355642557144  ,  0.2879160968989134\n",
            "epoch  15\n",
            "saving weights.... \n",
            "85.62  % ,  0.43171266226768495  ,  0.29166054463386537\n",
            "epoch  16\n",
            "saving weights.... \n",
            "84.79  % ,  0.4214659571170807  ,  0.2852379479587078\n",
            "epoch  17\n",
            "saving weights.... \n",
            "81.36  % ,  0.5816758922696114  ,  0.28888662320971487\n",
            "epoch  18\n",
            "saving weights.... \n",
            "86.03  % ,  0.4123483793497086  ,  0.2843261227250099\n",
            "epoch  19\n",
            "saving weights.... \n",
            "86.93  % ,  0.3895028819680214  ,  0.2867090776130557\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.59  % ,  0.2815485555410385  ,  0.1597445847414434\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.9  % ,  0.2708542135596275  ,  0.1164959454987198\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.9  % ,  0.2725050296664238  ,  0.10149607819765806\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.97  % ,  0.2734640574067831  ,  0.09193713667113335\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.93  % ,  0.27715895022749903  ,  0.08299657302908599\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.73  % ,  0.2790839325353503  ,  0.07544284111168235\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.73  % ,  0.2827938908740878  ,  0.07416996223498136\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.76  % ,  0.2837661345630884  ,  0.06195641132164747\n",
            "epoch  8\n",
            "saving weights.... \n",
            "91.0  % ,  0.2741121260151267  ,  0.05894900473840535\n",
            "epoch  9\n",
            "saving weights.... \n",
            "91.0  % ,  0.2837418727725744  ,  0.05802076121531427\n",
            "Finished Training\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.24\n",
            "features.24\n",
            "6809739.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "85.91  % ,  0.354346767103672  ,  0.39429837280511854\n",
            "epoch  1\n",
            "saving weights.... \n",
            "85.18  % ,  0.39725565469264984  ,  0.3553671752125025\n",
            "epoch  2\n",
            "saving weights.... \n",
            "85.48  % ,  0.36713681919574737  ,  0.34501503686010837\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.93  % ,  0.37880265949964526  ,  0.33209083122611044\n",
            "epoch  4\n",
            "saving weights.... \n",
            "83.16  % ,  0.46249574167728424  ,  0.32796108105778693\n",
            "epoch  5\n",
            "saving weights.... \n",
            "82.51  % ,  0.472403818333149  ,  0.3170723281145096\n",
            "epoch  6\n",
            "saving weights.... \n",
            "86.63  % ,  0.3574519354104996  ,  0.3153465277403593\n",
            "epoch  7\n",
            "saving weights.... \n",
            "86.22  % ,  0.3797896231412888  ,  0.3122152041375637\n",
            "epoch  8\n",
            "saving weights.... \n",
            "84.39  % ,  0.43696511981487274  ,  0.3046056282103062\n",
            "epoch  9\n",
            "saving weights.... \n",
            "86.66  % ,  0.384654604434967  ,  0.3069080232709646\n",
            "epoch  10\n",
            "saving weights.... \n",
            "84.55  % ,  0.45122489848136904  ,  0.3021977352619171\n",
            "epoch  11\n",
            "saving weights.... \n",
            "84.8  % ,  0.43100540071725846  ,  0.30054977749884126\n",
            "epoch  12\n",
            "saving weights.... \n",
            "84.68  % ,  0.449257036113739  ,  0.29856739783585073\n",
            "epoch  13\n",
            "saving weights.... \n",
            "85.53  % ,  0.4280964708566666  ,  0.2997878760367632\n",
            "epoch  14\n",
            "saving weights.... \n",
            "87.41  % ,  0.3729886759161949  ,  0.2906134116381407\n",
            "epoch  15\n",
            "saving weights.... \n",
            "84.03  % ,  0.4561535767555237  ,  0.2955326520241797\n",
            "epoch  16\n",
            "saving weights.... \n",
            "85.44  % ,  0.4210824444055557  ,  0.29191973341703414\n",
            "epoch  17\n",
            "saving weights.... \n",
            "84.69  % ,  0.45999600698947907  ,  0.2940790369391441\n",
            "epoch  18\n",
            "saving weights.... \n",
            "86.09  % ,  0.41505226352214813  ,  0.288344905641675\n",
            "epoch  19\n",
            "saving weights.... \n",
            "80.33  % ,  0.6413579344272613  ,  0.28170894865095614\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.29  % ,  0.27564551243186  ,  0.1520954372741282\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.29  % ,  0.2759259744465351  ,  0.1117081845562905\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.66  % ,  0.2727304780572653  ,  0.09709253978468478\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.68  % ,  0.2777708668410778  ,  0.08833324372433125\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.66  % ,  0.2746670603513718  ,  0.07947835985943676\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.74  % ,  0.2740560863018036  ,  0.07543269692584872\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.82  % ,  0.27659193990305064  ,  0.07155101054087282\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.78  % ,  0.2726453690856695  ,  0.06445365063771606\n",
            "epoch  8\n",
            "saving weights.... \n",
            "91.08  % ,  0.27664557425677777  ,  0.060682239332795145\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.99  % ,  0.2713297633096576  ,  0.05864731523729861\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5f68752a-ec63-4146-b2e9-e569853f6817\", \"features.24.npy\", 435)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.3 features.24': {'nbr_param': [510.0, 7001985.0],\n",
              "  'test_acc': [83.82,\n",
              "   85.64,\n",
              "   84.67,\n",
              "   85.43,\n",
              "   84.26,\n",
              "   83.59,\n",
              "   86.91,\n",
              "   85.1,\n",
              "   85.63,\n",
              "   86.39,\n",
              "   85.28,\n",
              "   83.62,\n",
              "   85.92,\n",
              "   87.37,\n",
              "   85.31,\n",
              "   85.62,\n",
              "   84.79,\n",
              "   81.36,\n",
              "   86.03,\n",
              "   86.93,\n",
              "   90.59,\n",
              "   90.9,\n",
              "   90.9,\n",
              "   90.97,\n",
              "   90.93,\n",
              "   90.73,\n",
              "   90.73,\n",
              "   90.76,\n",
              "   91.0,\n",
              "   91.0,\n",
              "   85.91,\n",
              "   85.18,\n",
              "   85.48,\n",
              "   85.93,\n",
              "   83.16,\n",
              "   82.51,\n",
              "   86.63,\n",
              "   86.22,\n",
              "   84.39,\n",
              "   86.66,\n",
              "   84.55,\n",
              "   84.8,\n",
              "   84.68,\n",
              "   85.53,\n",
              "   87.41,\n",
              "   84.03,\n",
              "   85.44,\n",
              "   84.69,\n",
              "   86.09,\n",
              "   80.33,\n",
              "   90.29,\n",
              "   90.29,\n",
              "   90.66,\n",
              "   90.68,\n",
              "   90.66,\n",
              "   90.74,\n",
              "   90.82,\n",
              "   90.78,\n",
              "   91.08,\n",
              "   90.99],\n",
              "  'training_loss': [0.3753677779302001,\n",
              "   0.3462250084847212,\n",
              "   0.33190153065472844,\n",
              "   0.33085808129012584,\n",
              "   0.31621220431029795,\n",
              "   0.31845793751180174,\n",
              "   0.3156200372368097,\n",
              "   0.30667638978362083,\n",
              "   0.3017140057712793,\n",
              "   0.30335422294437886,\n",
              "   0.29924887719750404,\n",
              "   0.2995054669216275,\n",
              "   0.30042183919698,\n",
              "   0.29423499591201546,\n",
              "   0.2879160968989134,\n",
              "   0.29166054463386537,\n",
              "   0.2852379479587078,\n",
              "   0.28888662320971487,\n",
              "   0.2843261227250099,\n",
              "   0.2867090776130557,\n",
              "   0.1597445847414434,\n",
              "   0.1164959454987198,\n",
              "   0.10149607819765806,\n",
              "   0.09193713667113335,\n",
              "   0.08299657302908599,\n",
              "   0.07544284111168235,\n",
              "   0.07416996223498136,\n",
              "   0.06195641132164747,\n",
              "   0.05894900473840535,\n",
              "   0.05802076121531427,\n",
              "   0.39429837280511854,\n",
              "   0.3553671752125025,\n",
              "   0.34501503686010837,\n",
              "   0.33209083122611044,\n",
              "   0.32796108105778693,\n",
              "   0.3170723281145096,\n",
              "   0.3153465277403593,\n",
              "   0.3122152041375637,\n",
              "   0.3046056282103062,\n",
              "   0.3069080232709646,\n",
              "   0.3021977352619171,\n",
              "   0.30054977749884126,\n",
              "   0.29856739783585073,\n",
              "   0.2997878760367632,\n",
              "   0.2906134116381407,\n",
              "   0.2955326520241797,\n",
              "   0.29191973341703414,\n",
              "   0.2940790369391441,\n",
              "   0.288344905641675,\n",
              "   0.28170894865095614,\n",
              "   0.1520954372741282,\n",
              "   0.1117081845562905,\n",
              "   0.09709253978468478,\n",
              "   0.08833324372433125,\n",
              "   0.07947835985943676,\n",
              "   0.07543269692584872,\n",
              "   0.07155101054087282,\n",
              "   0.06445365063771606,\n",
              "   0.060682239332795145,\n",
              "   0.05864731523729861],\n",
              "  'valid_loss': [0.4152320156812668,\n",
              "   0.3291621312737465,\n",
              "   0.42519748874902724,\n",
              "   0.3677966022372246,\n",
              "   0.42426420383453367,\n",
              "   0.45190358481407167,\n",
              "   0.3446594207048416,\n",
              "   0.4067449975728989,\n",
              "   0.41236174714565277,\n",
              "   0.39068249773979186,\n",
              "   0.4123280806303024,\n",
              "   0.4497375878095627,\n",
              "   0.41055611610412596,\n",
              "   0.36766749041080476,\n",
              "   0.4245355642557144,\n",
              "   0.43171266226768495,\n",
              "   0.4214659571170807,\n",
              "   0.5816758922696114,\n",
              "   0.4123483793497086,\n",
              "   0.3895028819680214,\n",
              "   0.2815485555410385,\n",
              "   0.2708542135596275,\n",
              "   0.2725050296664238,\n",
              "   0.2734640574067831,\n",
              "   0.27715895022749903,\n",
              "   0.2790839325353503,\n",
              "   0.2827938908740878,\n",
              "   0.2837661345630884,\n",
              "   0.2741121260151267,\n",
              "   0.2837418727725744,\n",
              "   0.354346767103672,\n",
              "   0.39725565469264984,\n",
              "   0.36713681919574737,\n",
              "   0.37880265949964526,\n",
              "   0.46249574167728424,\n",
              "   0.472403818333149,\n",
              "   0.3574519354104996,\n",
              "   0.3797896231412888,\n",
              "   0.43696511981487274,\n",
              "   0.384654604434967,\n",
              "   0.45122489848136904,\n",
              "   0.43100540071725846,\n",
              "   0.449257036113739,\n",
              "   0.4280964708566666,\n",
              "   0.3729886759161949,\n",
              "   0.4561535767555237,\n",
              "   0.4210824444055557,\n",
              "   0.45999600698947907,\n",
              "   0.41505226352214813,\n",
              "   0.6413579344272613,\n",
              "   0.27564551243186,\n",
              "   0.2759259744465351,\n",
              "   0.2727304780572653,\n",
              "   0.2777708668410778,\n",
              "   0.2746670603513718,\n",
              "   0.2740560863018036,\n",
              "   0.27659193990305064,\n",
              "   0.2726453690856695,\n",
              "   0.27664557425677777,\n",
              "   0.2713297633096576]},\n",
              " '0.6 features.24': {'nbr_param': [510.0, 6809739.0],\n",
              "  'test_acc': [83.82,\n",
              "   85.64,\n",
              "   84.67,\n",
              "   85.43,\n",
              "   84.26,\n",
              "   83.59,\n",
              "   86.91,\n",
              "   85.1,\n",
              "   85.63,\n",
              "   86.39,\n",
              "   85.28,\n",
              "   83.62,\n",
              "   85.92,\n",
              "   87.37,\n",
              "   85.31,\n",
              "   85.62,\n",
              "   84.79,\n",
              "   81.36,\n",
              "   86.03,\n",
              "   86.93,\n",
              "   90.59,\n",
              "   90.9,\n",
              "   90.9,\n",
              "   90.97,\n",
              "   90.93,\n",
              "   90.73,\n",
              "   90.73,\n",
              "   90.76,\n",
              "   91.0,\n",
              "   91.0,\n",
              "   85.91,\n",
              "   85.18,\n",
              "   85.48,\n",
              "   85.93,\n",
              "   83.16,\n",
              "   82.51,\n",
              "   86.63,\n",
              "   86.22,\n",
              "   84.39,\n",
              "   86.66,\n",
              "   84.55,\n",
              "   84.8,\n",
              "   84.68,\n",
              "   85.53,\n",
              "   87.41,\n",
              "   84.03,\n",
              "   85.44,\n",
              "   84.69,\n",
              "   86.09,\n",
              "   80.33,\n",
              "   90.29,\n",
              "   90.29,\n",
              "   90.66,\n",
              "   90.68,\n",
              "   90.66,\n",
              "   90.74,\n",
              "   90.82,\n",
              "   90.78,\n",
              "   91.08,\n",
              "   90.99],\n",
              "  'training_loss': [0.3753677779302001,\n",
              "   0.3462250084847212,\n",
              "   0.33190153065472844,\n",
              "   0.33085808129012584,\n",
              "   0.31621220431029795,\n",
              "   0.31845793751180174,\n",
              "   0.3156200372368097,\n",
              "   0.30667638978362083,\n",
              "   0.3017140057712793,\n",
              "   0.30335422294437886,\n",
              "   0.29924887719750404,\n",
              "   0.2995054669216275,\n",
              "   0.30042183919698,\n",
              "   0.29423499591201546,\n",
              "   0.2879160968989134,\n",
              "   0.29166054463386537,\n",
              "   0.2852379479587078,\n",
              "   0.28888662320971487,\n",
              "   0.2843261227250099,\n",
              "   0.2867090776130557,\n",
              "   0.1597445847414434,\n",
              "   0.1164959454987198,\n",
              "   0.10149607819765806,\n",
              "   0.09193713667113335,\n",
              "   0.08299657302908599,\n",
              "   0.07544284111168235,\n",
              "   0.07416996223498136,\n",
              "   0.06195641132164747,\n",
              "   0.05894900473840535,\n",
              "   0.05802076121531427,\n",
              "   0.39429837280511854,\n",
              "   0.3553671752125025,\n",
              "   0.34501503686010837,\n",
              "   0.33209083122611044,\n",
              "   0.32796108105778693,\n",
              "   0.3170723281145096,\n",
              "   0.3153465277403593,\n",
              "   0.3122152041375637,\n",
              "   0.3046056282103062,\n",
              "   0.3069080232709646,\n",
              "   0.3021977352619171,\n",
              "   0.30054977749884126,\n",
              "   0.29856739783585073,\n",
              "   0.2997878760367632,\n",
              "   0.2906134116381407,\n",
              "   0.2955326520241797,\n",
              "   0.29191973341703414,\n",
              "   0.2940790369391441,\n",
              "   0.288344905641675,\n",
              "   0.28170894865095614,\n",
              "   0.1520954372741282,\n",
              "   0.1117081845562905,\n",
              "   0.09709253978468478,\n",
              "   0.08833324372433125,\n",
              "   0.07947835985943676,\n",
              "   0.07543269692584872,\n",
              "   0.07155101054087282,\n",
              "   0.06445365063771606,\n",
              "   0.060682239332795145,\n",
              "   0.05864731523729861],\n",
              "  'valid_loss': [0.4152320156812668,\n",
              "   0.3291621312737465,\n",
              "   0.42519748874902724,\n",
              "   0.3677966022372246,\n",
              "   0.42426420383453367,\n",
              "   0.45190358481407167,\n",
              "   0.3446594207048416,\n",
              "   0.4067449975728989,\n",
              "   0.41236174714565277,\n",
              "   0.39068249773979186,\n",
              "   0.4123280806303024,\n",
              "   0.4497375878095627,\n",
              "   0.41055611610412596,\n",
              "   0.36766749041080476,\n",
              "   0.4245355642557144,\n",
              "   0.43171266226768495,\n",
              "   0.4214659571170807,\n",
              "   0.5816758922696114,\n",
              "   0.4123483793497086,\n",
              "   0.3895028819680214,\n",
              "   0.2815485555410385,\n",
              "   0.2708542135596275,\n",
              "   0.2725050296664238,\n",
              "   0.2734640574067831,\n",
              "   0.27715895022749903,\n",
              "   0.2790839325353503,\n",
              "   0.2827938908740878,\n",
              "   0.2837661345630884,\n",
              "   0.2741121260151267,\n",
              "   0.2837418727725744,\n",
              "   0.354346767103672,\n",
              "   0.39725565469264984,\n",
              "   0.36713681919574737,\n",
              "   0.37880265949964526,\n",
              "   0.46249574167728424,\n",
              "   0.472403818333149,\n",
              "   0.3574519354104996,\n",
              "   0.3797896231412888,\n",
              "   0.43696511981487274,\n",
              "   0.384654604434967,\n",
              "   0.45122489848136904,\n",
              "   0.43100540071725846,\n",
              "   0.449257036113739,\n",
              "   0.4280964708566666,\n",
              "   0.3729886759161949,\n",
              "   0.4561535767555237,\n",
              "   0.4210824444055557,\n",
              "   0.45999600698947907,\n",
              "   0.41505226352214813,\n",
              "   0.6413579344272613,\n",
              "   0.27564551243186,\n",
              "   0.2759259744465351,\n",
              "   0.2727304780572653,\n",
              "   0.2777708668410778,\n",
              "   0.2746670603513718,\n",
              "   0.2740560863018036,\n",
              "   0.27659193990305064,\n",
              "   0.2726453690856695,\n",
              "   0.27664557425677777,\n",
              "   0.2713297633096576]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL8DehUmnbDu"
      },
      "source": [
        "### Prune sur \"features.27\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_3aqpa7HpovR",
        "outputId": "eda73670-9978-4de3-9a9f-acd8b2cd0829"
      },
      "source": [
        "prune_fine_tune(8,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.27\n",
            "features.27\n",
            "6809739.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "84.77  % ,  0.333239211666584  ,  0.3685512093707919\n",
            "epoch  1\n",
            "saving weights.... \n",
            "83.42  % ,  0.42184072368144987  ,  0.3371828979581594\n",
            "epoch  2\n",
            "saving weights.... \n",
            "85.72  % ,  0.34416522830724716  ,  0.3286381820946932\n",
            "epoch  3\n",
            "saving weights.... \n",
            "86.82  % ,  0.35359407682418825  ,  0.3241369148075581\n",
            "epoch  4\n",
            "saving weights.... \n",
            "86.15  % ,  0.36549833872318266  ,  0.3184570017397404\n",
            "epoch  5\n",
            "saving weights.... \n",
            "83.35  % ,  0.4521233827352524  ,  0.3079734340786934\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.23  % ,  0.397946688747406  ,  0.30973900848329067\n",
            "epoch  7\n",
            "saving weights.... \n",
            "85.55  % ,  0.41473715530633926  ,  0.306109617254138\n",
            "epoch  8\n",
            "saving weights.... \n",
            "83.15  % ,  0.5183982539653778  ,  0.29709719207584856\n",
            "epoch  9\n",
            "saving weights.... \n",
            "87.05  % ,  0.3535485223054886  ,  0.30380172375440595\n",
            "epoch  10\n",
            "saving weights.... \n",
            "85.44  % ,  0.4140156543493271  ,  0.3002468601167202\n",
            "epoch  11\n",
            "saving weights.... \n",
            "83.65  % ,  0.451261162519455  ,  0.29639660666286943\n",
            "epoch  12\n",
            "saving weights.... \n",
            "85.93  % ,  0.39337602719068526  ,  0.2965314051315188\n",
            "epoch  13\n",
            "saving weights.... \n",
            "84.13  % ,  0.4405787266254425  ,  0.2893953766167164\n",
            "epoch  14\n",
            "saving weights.... \n",
            "86.01  % ,  0.3934565791606903  ,  0.29016671568602326\n",
            "epoch  15\n",
            "saving weights.... \n",
            "86.83  % ,  0.37802358477115633  ,  0.28621738185584544\n",
            "epoch  16\n",
            "saving weights.... \n",
            "87.33  % ,  0.3756810461759567  ,  0.2849843683093786\n",
            "epoch  17\n",
            "saving weights.... \n",
            "82.88  % ,  0.4859171051740646  ,  0.28275206263959407\n",
            "epoch  18\n",
            "saving weights.... \n",
            "86.02  % ,  0.4060606942653656  ,  0.28600578876435756\n",
            "epoch  19\n",
            "saving weights.... \n",
            "85.86  % ,  0.4156432765722275  ,  0.28133200568705796\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.4  % ,  0.26642648804187774  ,  0.15443370637521148\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.31  % ,  0.2596003626346588  ,  0.11235335097424685\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.72  % ,  0.2662683255374432  ,  0.09984624680802226\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.84  % ,  0.26172658042907715  ,  0.0871989216748625\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.82  % ,  0.26752212966009975  ,  0.07900207051597535\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.89  % ,  0.2672723892509937  ,  0.07628536203242839\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.74  % ,  0.27207563925385475  ,  0.070257218753919\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.96  % ,  0.2638749162197113  ,  0.06484409718997776\n",
            "epoch  8\n",
            "saving weights.... \n",
            "91.1  % ,  0.27894575426876544  ,  0.06028659621998668\n",
            "epoch  9\n",
            "saving weights.... \n",
            "91.05  % ,  0.27140766783952713  ,  0.05799086586367339\n",
            "Finished Training\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.27\n",
            "features.27\n",
            "6425247.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "83.6  % ,  0.3792881876707077  ,  0.3934792795419693\n",
            "epoch  1\n",
            "saving weights.... \n",
            "84.61  % ,  0.4190415398478508  ,  0.34729202667474746\n",
            "epoch  2\n",
            "saving weights.... \n",
            "85.2  % ,  0.3696481939315796  ,  0.33922129538953305\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.12  % ,  0.40323504593372345  ,  0.32510631482601166\n",
            "epoch  4\n",
            "saving weights.... \n",
            "83.52  % ,  0.45052973322868345  ,  0.3214276333063841\n",
            "epoch  5\n",
            "saving weights.... \n",
            "85.67  % ,  0.3711602658033371  ,  0.3160181904524565\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.64  % ,  0.37740556936264036  ,  0.31638100820183757\n",
            "epoch  7\n",
            "saving weights.... \n",
            "86.09  % ,  0.3818258926272392  ,  0.30552080661654474\n",
            "epoch  8\n",
            "saving weights.... \n",
            "85.19  % ,  0.4199265293598175  ,  0.31130229723453523\n",
            "epoch  9\n",
            "saving weights.... \n",
            "84.1  % ,  0.4405544481039047  ,  0.31365844508707524\n",
            "epoch  10\n",
            "saving weights.... \n",
            "85.87  % ,  0.3831452165126801  ,  0.3045675350189209\n",
            "epoch  11\n",
            "saving weights.... \n",
            "86.38  % ,  0.4020632388830185  ,  0.30016504330337046\n",
            "epoch  12\n",
            "saving weights.... \n",
            "86.53  % ,  0.39706599984169005  ,  0.29302590612769125\n",
            "epoch  13\n",
            "saving weights.... \n",
            "86.58  % ,  0.4215278472661972  ,  0.2957667922079563\n",
            "epoch  14\n",
            "saving weights.... \n",
            "85.26  % ,  0.4283349490880966  ,  0.2903592157244682\n",
            "epoch  15\n",
            "saving weights.... \n",
            "86.12  % ,  0.4184819757938385  ,  0.29322415421158077\n",
            "epoch  16\n",
            "saving weights.... \n",
            "86.58  % ,  0.3918377013683319  ,  0.28749175202548505\n",
            "epoch  17\n",
            "saving weights.... \n",
            "85.94  % ,  0.4361732797861099  ,  0.2898221112132072\n",
            "epoch  18\n",
            "saving weights.... \n",
            "85.99  % ,  0.41984463455677035  ,  0.2880531962096691\n",
            "epoch  19\n",
            "saving weights.... \n",
            "87.22  % ,  0.3862010722458363  ,  0.28664574117958547\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.5  % ,  0.2705543124884367  ,  0.15966346889808775\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.66  % ,  0.2665045948773623  ,  0.11469841128066183\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.93  % ,  0.2603924602895975  ,  0.09643973640836775\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.66  % ,  0.25616821076124907  ,  0.093212246100232\n",
            "epoch  4\n",
            "saving weights.... \n",
            "91.23  % ,  0.27227321815937755  ,  0.0806673552710563\n",
            "epoch  5\n",
            "saving weights.... \n",
            "91.21  % ,  0.2642251648053527  ,  0.0757650531237945\n",
            "epoch  6\n",
            "saving weights.... \n",
            "91.35  % ,  0.26019140630215404  ,  0.06844618465341627\n",
            "epoch  7\n",
            "saving weights.... \n",
            "91.37  % ,  0.25554369194209575  ,  0.06614497600011528\n",
            "epoch  8\n",
            "saving weights.... \n",
            "91.03  % ,  0.27576667933017013  ,  0.05931416151504964\n",
            "epoch  9\n",
            "saving weights.... \n",
            "91.1  % ,  0.277538061132282  ,  0.057600654394179586\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e8795118-13bb-4d23-ab6c-2f747da74007\", \"features.27.npy\", 435)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.3 features.27': {'nbr_param': [510.0, 6809739.0],\n",
              "  'test_acc': [83.82,\n",
              "   85.64,\n",
              "   84.67,\n",
              "   85.43,\n",
              "   84.26,\n",
              "   83.59,\n",
              "   86.91,\n",
              "   85.1,\n",
              "   85.63,\n",
              "   86.39,\n",
              "   85.28,\n",
              "   83.62,\n",
              "   85.92,\n",
              "   87.37,\n",
              "   85.31,\n",
              "   85.62,\n",
              "   84.79,\n",
              "   81.36,\n",
              "   86.03,\n",
              "   86.93,\n",
              "   90.59,\n",
              "   90.9,\n",
              "   90.9,\n",
              "   90.97,\n",
              "   90.93,\n",
              "   90.73,\n",
              "   90.73,\n",
              "   90.76,\n",
              "   91.0,\n",
              "   91.0,\n",
              "   85.91,\n",
              "   85.18,\n",
              "   85.48,\n",
              "   85.93,\n",
              "   83.16,\n",
              "   82.51,\n",
              "   86.63,\n",
              "   86.22,\n",
              "   84.39,\n",
              "   86.66,\n",
              "   84.55,\n",
              "   84.8,\n",
              "   84.68,\n",
              "   85.53,\n",
              "   87.41,\n",
              "   84.03,\n",
              "   85.44,\n",
              "   84.69,\n",
              "   86.09,\n",
              "   80.33,\n",
              "   90.29,\n",
              "   90.29,\n",
              "   90.66,\n",
              "   90.68,\n",
              "   90.66,\n",
              "   90.74,\n",
              "   90.82,\n",
              "   90.78,\n",
              "   91.08,\n",
              "   90.99,\n",
              "   84.77,\n",
              "   83.42,\n",
              "   85.72,\n",
              "   86.82,\n",
              "   86.15,\n",
              "   83.35,\n",
              "   85.23,\n",
              "   85.55,\n",
              "   83.15,\n",
              "   87.05,\n",
              "   85.44,\n",
              "   83.65,\n",
              "   85.93,\n",
              "   84.13,\n",
              "   86.01,\n",
              "   86.83,\n",
              "   87.33,\n",
              "   82.88,\n",
              "   86.02,\n",
              "   85.86,\n",
              "   90.4,\n",
              "   90.31,\n",
              "   90.72,\n",
              "   90.84,\n",
              "   90.82,\n",
              "   90.89,\n",
              "   90.74,\n",
              "   90.96,\n",
              "   91.1,\n",
              "   91.05,\n",
              "   83.6,\n",
              "   84.61,\n",
              "   85.2,\n",
              "   85.12,\n",
              "   83.52,\n",
              "   85.67,\n",
              "   85.64,\n",
              "   86.09,\n",
              "   85.19,\n",
              "   84.1,\n",
              "   85.87,\n",
              "   86.38,\n",
              "   86.53,\n",
              "   86.58,\n",
              "   85.26,\n",
              "   86.12,\n",
              "   86.58,\n",
              "   85.94,\n",
              "   85.99,\n",
              "   87.22,\n",
              "   90.5,\n",
              "   90.66,\n",
              "   90.93,\n",
              "   90.66,\n",
              "   91.23,\n",
              "   91.21,\n",
              "   91.35,\n",
              "   91.37,\n",
              "   91.03,\n",
              "   91.1],\n",
              "  'training_loss': [0.3753677779302001,\n",
              "   0.3462250084847212,\n",
              "   0.33190153065472844,\n",
              "   0.33085808129012584,\n",
              "   0.31621220431029795,\n",
              "   0.31845793751180174,\n",
              "   0.3156200372368097,\n",
              "   0.30667638978362083,\n",
              "   0.3017140057712793,\n",
              "   0.30335422294437886,\n",
              "   0.29924887719750404,\n",
              "   0.2995054669216275,\n",
              "   0.30042183919698,\n",
              "   0.29423499591201546,\n",
              "   0.2879160968989134,\n",
              "   0.29166054463386537,\n",
              "   0.2852379479587078,\n",
              "   0.28888662320971487,\n",
              "   0.2843261227250099,\n",
              "   0.2867090776130557,\n",
              "   0.1597445847414434,\n",
              "   0.1164959454987198,\n",
              "   0.10149607819765806,\n",
              "   0.09193713667113335,\n",
              "   0.08299657302908599,\n",
              "   0.07544284111168235,\n",
              "   0.07416996223498136,\n",
              "   0.06195641132164747,\n",
              "   0.05894900473840535,\n",
              "   0.05802076121531427,\n",
              "   0.39429837280511854,\n",
              "   0.3553671752125025,\n",
              "   0.34501503686010837,\n",
              "   0.33209083122611044,\n",
              "   0.32796108105778693,\n",
              "   0.3170723281145096,\n",
              "   0.3153465277403593,\n",
              "   0.3122152041375637,\n",
              "   0.3046056282103062,\n",
              "   0.3069080232709646,\n",
              "   0.3021977352619171,\n",
              "   0.30054977749884126,\n",
              "   0.29856739783585073,\n",
              "   0.2997878760367632,\n",
              "   0.2906134116381407,\n",
              "   0.2955326520241797,\n",
              "   0.29191973341703414,\n",
              "   0.2940790369391441,\n",
              "   0.288344905641675,\n",
              "   0.28170894865095614,\n",
              "   0.1520954372741282,\n",
              "   0.1117081845562905,\n",
              "   0.09709253978468478,\n",
              "   0.08833324372433125,\n",
              "   0.07947835985943676,\n",
              "   0.07543269692584872,\n",
              "   0.07155101054087282,\n",
              "   0.06445365063771606,\n",
              "   0.060682239332795145,\n",
              "   0.05864731523729861,\n",
              "   0.3685512093707919,\n",
              "   0.3371828979581594,\n",
              "   0.3286381820946932,\n",
              "   0.3241369148075581,\n",
              "   0.3184570017397404,\n",
              "   0.3079734340786934,\n",
              "   0.30973900848329067,\n",
              "   0.306109617254138,\n",
              "   0.29709719207584856,\n",
              "   0.30380172375440595,\n",
              "   0.3002468601167202,\n",
              "   0.29639660666286943,\n",
              "   0.2965314051315188,\n",
              "   0.2893953766167164,\n",
              "   0.29016671568602326,\n",
              "   0.28621738185584544,\n",
              "   0.2849843683093786,\n",
              "   0.28275206263959407,\n",
              "   0.28600578876435756,\n",
              "   0.28133200568705796,\n",
              "   0.15443370637521148,\n",
              "   0.11235335097424685,\n",
              "   0.09984624680802226,\n",
              "   0.0871989216748625,\n",
              "   0.07900207051597535,\n",
              "   0.07628536203242839,\n",
              "   0.070257218753919,\n",
              "   0.06484409718997776,\n",
              "   0.06028659621998668,\n",
              "   0.05799086586367339,\n",
              "   0.3934792795419693,\n",
              "   0.34729202667474746,\n",
              "   0.33922129538953305,\n",
              "   0.32510631482601166,\n",
              "   0.3214276333063841,\n",
              "   0.3160181904524565,\n",
              "   0.31638100820183757,\n",
              "   0.30552080661654474,\n",
              "   0.31130229723453523,\n",
              "   0.31365844508707524,\n",
              "   0.3045675350189209,\n",
              "   0.30016504330337046,\n",
              "   0.29302590612769125,\n",
              "   0.2957667922079563,\n",
              "   0.2903592157244682,\n",
              "   0.29322415421158077,\n",
              "   0.28749175202548505,\n",
              "   0.2898221112132072,\n",
              "   0.2880531962096691,\n",
              "   0.28664574117958547,\n",
              "   0.15966346889808775,\n",
              "   0.11469841128066183,\n",
              "   0.09643973640836775,\n",
              "   0.093212246100232,\n",
              "   0.0806673552710563,\n",
              "   0.0757650531237945,\n",
              "   0.06844618465341627,\n",
              "   0.06614497600011528,\n",
              "   0.05931416151504964,\n",
              "   0.057600654394179586],\n",
              "  'valid_loss': [0.4152320156812668,\n",
              "   0.3291621312737465,\n",
              "   0.42519748874902724,\n",
              "   0.3677966022372246,\n",
              "   0.42426420383453367,\n",
              "   0.45190358481407167,\n",
              "   0.3446594207048416,\n",
              "   0.4067449975728989,\n",
              "   0.41236174714565277,\n",
              "   0.39068249773979186,\n",
              "   0.4123280806303024,\n",
              "   0.4497375878095627,\n",
              "   0.41055611610412596,\n",
              "   0.36766749041080476,\n",
              "   0.4245355642557144,\n",
              "   0.43171266226768495,\n",
              "   0.4214659571170807,\n",
              "   0.5816758922696114,\n",
              "   0.4123483793497086,\n",
              "   0.3895028819680214,\n",
              "   0.2815485555410385,\n",
              "   0.2708542135596275,\n",
              "   0.2725050296664238,\n",
              "   0.2734640574067831,\n",
              "   0.27715895022749903,\n",
              "   0.2790839325353503,\n",
              "   0.2827938908740878,\n",
              "   0.2837661345630884,\n",
              "   0.2741121260151267,\n",
              "   0.2837418727725744,\n",
              "   0.354346767103672,\n",
              "   0.39725565469264984,\n",
              "   0.36713681919574737,\n",
              "   0.37880265949964526,\n",
              "   0.46249574167728424,\n",
              "   0.472403818333149,\n",
              "   0.3574519354104996,\n",
              "   0.3797896231412888,\n",
              "   0.43696511981487274,\n",
              "   0.384654604434967,\n",
              "   0.45122489848136904,\n",
              "   0.43100540071725846,\n",
              "   0.449257036113739,\n",
              "   0.4280964708566666,\n",
              "   0.3729886759161949,\n",
              "   0.4561535767555237,\n",
              "   0.4210824444055557,\n",
              "   0.45999600698947907,\n",
              "   0.41505226352214813,\n",
              "   0.6413579344272613,\n",
              "   0.27564551243186,\n",
              "   0.2759259744465351,\n",
              "   0.2727304780572653,\n",
              "   0.2777708668410778,\n",
              "   0.2746670603513718,\n",
              "   0.2740560863018036,\n",
              "   0.27659193990305064,\n",
              "   0.2726453690856695,\n",
              "   0.27664557425677777,\n",
              "   0.2713297633096576,\n",
              "   0.333239211666584,\n",
              "   0.42184072368144987,\n",
              "   0.34416522830724716,\n",
              "   0.35359407682418825,\n",
              "   0.36549833872318266,\n",
              "   0.4521233827352524,\n",
              "   0.397946688747406,\n",
              "   0.41473715530633926,\n",
              "   0.5183982539653778,\n",
              "   0.3535485223054886,\n",
              "   0.4140156543493271,\n",
              "   0.451261162519455,\n",
              "   0.39337602719068526,\n",
              "   0.4405787266254425,\n",
              "   0.3934565791606903,\n",
              "   0.37802358477115633,\n",
              "   0.3756810461759567,\n",
              "   0.4859171051740646,\n",
              "   0.4060606942653656,\n",
              "   0.4156432765722275,\n",
              "   0.26642648804187774,\n",
              "   0.2596003626346588,\n",
              "   0.2662683255374432,\n",
              "   0.26172658042907715,\n",
              "   0.26752212966009975,\n",
              "   0.2672723892509937,\n",
              "   0.27207563925385475,\n",
              "   0.2638749162197113,\n",
              "   0.27894575426876544,\n",
              "   0.27140766783952713,\n",
              "   0.3792881876707077,\n",
              "   0.4190415398478508,\n",
              "   0.3696481939315796,\n",
              "   0.40323504593372345,\n",
              "   0.45052973322868345,\n",
              "   0.3711602658033371,\n",
              "   0.37740556936264036,\n",
              "   0.3818258926272392,\n",
              "   0.4199265293598175,\n",
              "   0.4405544481039047,\n",
              "   0.3831452165126801,\n",
              "   0.4020632388830185,\n",
              "   0.39706599984169005,\n",
              "   0.4215278472661972,\n",
              "   0.4283349490880966,\n",
              "   0.4184819757938385,\n",
              "   0.3918377013683319,\n",
              "   0.4361732797861099,\n",
              "   0.41984463455677035,\n",
              "   0.3862010722458363,\n",
              "   0.2705543124884367,\n",
              "   0.2665045948773623,\n",
              "   0.2603924602895975,\n",
              "   0.25616821076124907,\n",
              "   0.27227321815937755,\n",
              "   0.2642251648053527,\n",
              "   0.26019140630215404,\n",
              "   0.25554369194209575,\n",
              "   0.27576667933017013,\n",
              "   0.277538061132282]},\n",
              " '0.6 features.27': {'nbr_param': [510.0, 6425247.0],\n",
              "  'test_acc': [83.82,\n",
              "   85.64,\n",
              "   84.67,\n",
              "   85.43,\n",
              "   84.26,\n",
              "   83.59,\n",
              "   86.91,\n",
              "   85.1,\n",
              "   85.63,\n",
              "   86.39,\n",
              "   85.28,\n",
              "   83.62,\n",
              "   85.92,\n",
              "   87.37,\n",
              "   85.31,\n",
              "   85.62,\n",
              "   84.79,\n",
              "   81.36,\n",
              "   86.03,\n",
              "   86.93,\n",
              "   90.59,\n",
              "   90.9,\n",
              "   90.9,\n",
              "   90.97,\n",
              "   90.93,\n",
              "   90.73,\n",
              "   90.73,\n",
              "   90.76,\n",
              "   91.0,\n",
              "   91.0,\n",
              "   85.91,\n",
              "   85.18,\n",
              "   85.48,\n",
              "   85.93,\n",
              "   83.16,\n",
              "   82.51,\n",
              "   86.63,\n",
              "   86.22,\n",
              "   84.39,\n",
              "   86.66,\n",
              "   84.55,\n",
              "   84.8,\n",
              "   84.68,\n",
              "   85.53,\n",
              "   87.41,\n",
              "   84.03,\n",
              "   85.44,\n",
              "   84.69,\n",
              "   86.09,\n",
              "   80.33,\n",
              "   90.29,\n",
              "   90.29,\n",
              "   90.66,\n",
              "   90.68,\n",
              "   90.66,\n",
              "   90.74,\n",
              "   90.82,\n",
              "   90.78,\n",
              "   91.08,\n",
              "   90.99,\n",
              "   84.77,\n",
              "   83.42,\n",
              "   85.72,\n",
              "   86.82,\n",
              "   86.15,\n",
              "   83.35,\n",
              "   85.23,\n",
              "   85.55,\n",
              "   83.15,\n",
              "   87.05,\n",
              "   85.44,\n",
              "   83.65,\n",
              "   85.93,\n",
              "   84.13,\n",
              "   86.01,\n",
              "   86.83,\n",
              "   87.33,\n",
              "   82.88,\n",
              "   86.02,\n",
              "   85.86,\n",
              "   90.4,\n",
              "   90.31,\n",
              "   90.72,\n",
              "   90.84,\n",
              "   90.82,\n",
              "   90.89,\n",
              "   90.74,\n",
              "   90.96,\n",
              "   91.1,\n",
              "   91.05,\n",
              "   83.6,\n",
              "   84.61,\n",
              "   85.2,\n",
              "   85.12,\n",
              "   83.52,\n",
              "   85.67,\n",
              "   85.64,\n",
              "   86.09,\n",
              "   85.19,\n",
              "   84.1,\n",
              "   85.87,\n",
              "   86.38,\n",
              "   86.53,\n",
              "   86.58,\n",
              "   85.26,\n",
              "   86.12,\n",
              "   86.58,\n",
              "   85.94,\n",
              "   85.99,\n",
              "   87.22,\n",
              "   90.5,\n",
              "   90.66,\n",
              "   90.93,\n",
              "   90.66,\n",
              "   91.23,\n",
              "   91.21,\n",
              "   91.35,\n",
              "   91.37,\n",
              "   91.03,\n",
              "   91.1],\n",
              "  'training_loss': [0.3753677779302001,\n",
              "   0.3462250084847212,\n",
              "   0.33190153065472844,\n",
              "   0.33085808129012584,\n",
              "   0.31621220431029795,\n",
              "   0.31845793751180174,\n",
              "   0.3156200372368097,\n",
              "   0.30667638978362083,\n",
              "   0.3017140057712793,\n",
              "   0.30335422294437886,\n",
              "   0.29924887719750404,\n",
              "   0.2995054669216275,\n",
              "   0.30042183919698,\n",
              "   0.29423499591201546,\n",
              "   0.2879160968989134,\n",
              "   0.29166054463386537,\n",
              "   0.2852379479587078,\n",
              "   0.28888662320971487,\n",
              "   0.2843261227250099,\n",
              "   0.2867090776130557,\n",
              "   0.1597445847414434,\n",
              "   0.1164959454987198,\n",
              "   0.10149607819765806,\n",
              "   0.09193713667113335,\n",
              "   0.08299657302908599,\n",
              "   0.07544284111168235,\n",
              "   0.07416996223498136,\n",
              "   0.06195641132164747,\n",
              "   0.05894900473840535,\n",
              "   0.05802076121531427,\n",
              "   0.39429837280511854,\n",
              "   0.3553671752125025,\n",
              "   0.34501503686010837,\n",
              "   0.33209083122611044,\n",
              "   0.32796108105778693,\n",
              "   0.3170723281145096,\n",
              "   0.3153465277403593,\n",
              "   0.3122152041375637,\n",
              "   0.3046056282103062,\n",
              "   0.3069080232709646,\n",
              "   0.3021977352619171,\n",
              "   0.30054977749884126,\n",
              "   0.29856739783585073,\n",
              "   0.2997878760367632,\n",
              "   0.2906134116381407,\n",
              "   0.2955326520241797,\n",
              "   0.29191973341703414,\n",
              "   0.2940790369391441,\n",
              "   0.288344905641675,\n",
              "   0.28170894865095614,\n",
              "   0.1520954372741282,\n",
              "   0.1117081845562905,\n",
              "   0.09709253978468478,\n",
              "   0.08833324372433125,\n",
              "   0.07947835985943676,\n",
              "   0.07543269692584872,\n",
              "   0.07155101054087282,\n",
              "   0.06445365063771606,\n",
              "   0.060682239332795145,\n",
              "   0.05864731523729861,\n",
              "   0.3685512093707919,\n",
              "   0.3371828979581594,\n",
              "   0.3286381820946932,\n",
              "   0.3241369148075581,\n",
              "   0.3184570017397404,\n",
              "   0.3079734340786934,\n",
              "   0.30973900848329067,\n",
              "   0.306109617254138,\n",
              "   0.29709719207584856,\n",
              "   0.30380172375440595,\n",
              "   0.3002468601167202,\n",
              "   0.29639660666286943,\n",
              "   0.2965314051315188,\n",
              "   0.2893953766167164,\n",
              "   0.29016671568602326,\n",
              "   0.28621738185584544,\n",
              "   0.2849843683093786,\n",
              "   0.28275206263959407,\n",
              "   0.28600578876435756,\n",
              "   0.28133200568705796,\n",
              "   0.15443370637521148,\n",
              "   0.11235335097424685,\n",
              "   0.09984624680802226,\n",
              "   0.0871989216748625,\n",
              "   0.07900207051597535,\n",
              "   0.07628536203242839,\n",
              "   0.070257218753919,\n",
              "   0.06484409718997776,\n",
              "   0.06028659621998668,\n",
              "   0.05799086586367339,\n",
              "   0.3934792795419693,\n",
              "   0.34729202667474746,\n",
              "   0.33922129538953305,\n",
              "   0.32510631482601166,\n",
              "   0.3214276333063841,\n",
              "   0.3160181904524565,\n",
              "   0.31638100820183757,\n",
              "   0.30552080661654474,\n",
              "   0.31130229723453523,\n",
              "   0.31365844508707524,\n",
              "   0.3045675350189209,\n",
              "   0.30016504330337046,\n",
              "   0.29302590612769125,\n",
              "   0.2957667922079563,\n",
              "   0.2903592157244682,\n",
              "   0.29322415421158077,\n",
              "   0.28749175202548505,\n",
              "   0.2898221112132072,\n",
              "   0.2880531962096691,\n",
              "   0.28664574117958547,\n",
              "   0.15966346889808775,\n",
              "   0.11469841128066183,\n",
              "   0.09643973640836775,\n",
              "   0.093212246100232,\n",
              "   0.0806673552710563,\n",
              "   0.0757650531237945,\n",
              "   0.06844618465341627,\n",
              "   0.06614497600011528,\n",
              "   0.05931416151504964,\n",
              "   0.057600654394179586],\n",
              "  'valid_loss': [0.4152320156812668,\n",
              "   0.3291621312737465,\n",
              "   0.42519748874902724,\n",
              "   0.3677966022372246,\n",
              "   0.42426420383453367,\n",
              "   0.45190358481407167,\n",
              "   0.3446594207048416,\n",
              "   0.4067449975728989,\n",
              "   0.41236174714565277,\n",
              "   0.39068249773979186,\n",
              "   0.4123280806303024,\n",
              "   0.4497375878095627,\n",
              "   0.41055611610412596,\n",
              "   0.36766749041080476,\n",
              "   0.4245355642557144,\n",
              "   0.43171266226768495,\n",
              "   0.4214659571170807,\n",
              "   0.5816758922696114,\n",
              "   0.4123483793497086,\n",
              "   0.3895028819680214,\n",
              "   0.2815485555410385,\n",
              "   0.2708542135596275,\n",
              "   0.2725050296664238,\n",
              "   0.2734640574067831,\n",
              "   0.27715895022749903,\n",
              "   0.2790839325353503,\n",
              "   0.2827938908740878,\n",
              "   0.2837661345630884,\n",
              "   0.2741121260151267,\n",
              "   0.2837418727725744,\n",
              "   0.354346767103672,\n",
              "   0.39725565469264984,\n",
              "   0.36713681919574737,\n",
              "   0.37880265949964526,\n",
              "   0.46249574167728424,\n",
              "   0.472403818333149,\n",
              "   0.3574519354104996,\n",
              "   0.3797896231412888,\n",
              "   0.43696511981487274,\n",
              "   0.384654604434967,\n",
              "   0.45122489848136904,\n",
              "   0.43100540071725846,\n",
              "   0.449257036113739,\n",
              "   0.4280964708566666,\n",
              "   0.3729886759161949,\n",
              "   0.4561535767555237,\n",
              "   0.4210824444055557,\n",
              "   0.45999600698947907,\n",
              "   0.41505226352214813,\n",
              "   0.6413579344272613,\n",
              "   0.27564551243186,\n",
              "   0.2759259744465351,\n",
              "   0.2727304780572653,\n",
              "   0.2777708668410778,\n",
              "   0.2746670603513718,\n",
              "   0.2740560863018036,\n",
              "   0.27659193990305064,\n",
              "   0.2726453690856695,\n",
              "   0.27664557425677777,\n",
              "   0.2713297633096576,\n",
              "   0.333239211666584,\n",
              "   0.42184072368144987,\n",
              "   0.34416522830724716,\n",
              "   0.35359407682418825,\n",
              "   0.36549833872318266,\n",
              "   0.4521233827352524,\n",
              "   0.397946688747406,\n",
              "   0.41473715530633926,\n",
              "   0.5183982539653778,\n",
              "   0.3535485223054886,\n",
              "   0.4140156543493271,\n",
              "   0.451261162519455,\n",
              "   0.39337602719068526,\n",
              "   0.4405787266254425,\n",
              "   0.3934565791606903,\n",
              "   0.37802358477115633,\n",
              "   0.3756810461759567,\n",
              "   0.4859171051740646,\n",
              "   0.4060606942653656,\n",
              "   0.4156432765722275,\n",
              "   0.26642648804187774,\n",
              "   0.2596003626346588,\n",
              "   0.2662683255374432,\n",
              "   0.26172658042907715,\n",
              "   0.26752212966009975,\n",
              "   0.2672723892509937,\n",
              "   0.27207563925385475,\n",
              "   0.2638749162197113,\n",
              "   0.27894575426876544,\n",
              "   0.27140766783952713,\n",
              "   0.3792881876707077,\n",
              "   0.4190415398478508,\n",
              "   0.3696481939315796,\n",
              "   0.40323504593372345,\n",
              "   0.45052973322868345,\n",
              "   0.3711602658033371,\n",
              "   0.37740556936264036,\n",
              "   0.3818258926272392,\n",
              "   0.4199265293598175,\n",
              "   0.4405544481039047,\n",
              "   0.3831452165126801,\n",
              "   0.4020632388830185,\n",
              "   0.39706599984169005,\n",
              "   0.4215278472661972,\n",
              "   0.4283349490880966,\n",
              "   0.4184819757938385,\n",
              "   0.3918377013683319,\n",
              "   0.4361732797861099,\n",
              "   0.41984463455677035,\n",
              "   0.3862010722458363,\n",
              "   0.2705543124884367,\n",
              "   0.2665045948773623,\n",
              "   0.2603924602895975,\n",
              "   0.25616821076124907,\n",
              "   0.27227321815937755,\n",
              "   0.2642251648053527,\n",
              "   0.26019140630215404,\n",
              "   0.25554369194209575,\n",
              "   0.27576667933017013,\n",
              "   0.277538061132282]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp96CqAYnbSv"
      },
      "source": [
        "### Prune sur \"features.30\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ueo0W71TppZc",
        "outputId": "f73eff98-660e-47fa-8d00-cec8fd844b21"
      },
      "source": [
        "prune_fine_tune(9,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.30\n",
            "features.30\n",
            "6809739.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "85.3  % ,  0.3695487419843674  ,  0.384310731035471\n",
            "epoch  1\n",
            "saving weights.... \n",
            "83.53  % ,  0.3869398869037628  ,  0.3484098303705454\n",
            "epoch  2\n",
            "saving weights.... \n",
            "84.39  % ,  0.4132691576957703  ,  0.3305602998241782\n",
            "epoch  3\n",
            "saving weights.... \n",
            "86.78  % ,  0.35521349189281465  ,  0.3316704084724188\n",
            "epoch  4\n",
            "saving weights.... \n",
            "85.35  % ,  0.4082661004424095  ,  0.3201140031576157\n",
            "epoch  5\n",
            "saving weights.... \n",
            "84.93  % ,  0.43268994417190554  ,  0.3171138580173254\n",
            "epoch  6\n",
            "saving weights.... \n",
            "84.65  % ,  0.4418498308420181  ,  0.30742178422808647\n",
            "epoch  7\n",
            "saving weights.... \n",
            "85.64  % ,  0.38357009875774384  ,  0.3062221253603697\n",
            "epoch  8\n",
            "saving weights.... \n",
            "85.23  % ,  0.40524812899827956  ,  0.307082258066535\n",
            "epoch  9\n",
            "saving weights.... \n",
            "86.67  % ,  0.3893915811538696  ,  0.3041555880665779\n",
            "epoch  10\n",
            "saving weights.... \n",
            "85.61  % ,  0.404174605512619  ,  0.3012759686022997\n",
            "epoch  11\n",
            "saving weights.... \n",
            "85.22  % ,  0.4211113221168518  ,  0.3005621831998229\n",
            "epoch  12\n",
            "saving weights.... \n",
            "86.24  % ,  0.3829330843448639  ,  0.3001861132025719\n",
            "epoch  13\n",
            "saving weights.... \n",
            "85.25  % ,  0.40650593234300614  ,  0.2929652029752731\n",
            "epoch  14\n",
            "saving weights.... \n",
            "85.59  % ,  0.424862975358963  ,  0.29077415713071825\n",
            "epoch  15\n",
            "saving weights.... \n",
            "85.72  % ,  0.42295445343255994  ,  0.2863423902451992\n",
            "epoch  16\n",
            "saving weights.... \n",
            "86.57  % ,  0.41183751838207244  ,  0.29025631477683783\n",
            "epoch  17\n",
            "saving weights.... \n",
            "86.24  % ,  0.3968865024089813  ,  0.28362039989829063\n",
            "epoch  18\n",
            "saving weights.... \n",
            "82.26  % ,  0.5445519752264023  ,  0.2816042103484273\n",
            "epoch  19\n",
            "saving weights.... \n",
            "86.33  % ,  0.3924273215532303  ,  0.2918580346047878\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.36  % ,  0.27219469935894014  ,  0.15614130154028535\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.89  % ,  0.256604188233614  ,  0.11599626542776822\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.81  % ,  0.2640684949249029  ,  0.1016306979764253\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.8  % ,  0.26455358221530917  ,  0.09305325521491468\n",
            "epoch  4\n",
            "saving weights.... \n",
            "91.05  % ,  0.2704180570870638  ,  0.08230522548556328\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.73  % ,  0.27147540556490424  ,  0.07594876559246332\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.66  % ,  0.2710482469238341  ,  0.07018269618805498\n",
            "epoch  7\n",
            "saving weights.... \n",
            "91.12  % ,  0.2746137186989188  ,  0.06641674124747515\n",
            "epoch  8\n",
            "saving weights.... \n",
            "91.07  % ,  0.28445716969743373  ,  0.06010873744292185\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.68  % ,  0.28358544899225235  ,  0.05690802346384153\n",
            "Finished Training\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.30\n",
            "features.30\n",
            "6425247.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "84.75  % ,  0.3595377722263336  ,  0.389750537699461\n",
            "epoch  1\n",
            "saving weights.... \n",
            "85.22  % ,  0.3757767829656601  ,  0.34861727122366426\n",
            "epoch  2\n",
            "saving weights.... \n",
            "82.82  % ,  0.4407354746222496  ,  0.3387937770873308\n",
            "epoch  3\n",
            "saving weights.... \n",
            "82.99  % ,  0.4478885493278503  ,  0.32263996513038873\n",
            "epoch  4\n",
            "saving weights.... \n",
            "84.55  % ,  0.41084517695903777  ,  0.3197401510104537\n",
            "epoch  5\n",
            "saving weights.... \n",
            "84.37  % ,  0.4239084290266037  ,  0.32074317649900913\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.86  % ,  0.39846336534023286  ,  0.30821431506872177\n",
            "epoch  7\n",
            "saving weights.... \n",
            "85.86  % ,  0.3870759885549545  ,  0.3124615932092071\n",
            "epoch  8\n",
            "saving weights.... \n",
            "86.39  % ,  0.36840578147172925  ,  0.3057874827593565\n",
            "epoch  9\n",
            "saving weights.... \n",
            "86.44  % ,  0.38139032323360444  ,  0.3028174328818917\n",
            "epoch  10\n",
            "saving weights.... \n",
            "84.12  % ,  0.450024591255188  ,  0.3031667965173721\n",
            "epoch  11\n",
            "saving weights.... \n",
            "86.41  % ,  0.3878326279997826  ,  0.29817766211330893\n",
            "epoch  12\n",
            "saving weights.... \n",
            "87.16  % ,  0.37619855046272277  ,  0.29135231768488884\n",
            "epoch  13\n",
            "saving weights.... \n",
            "85.9  % ,  0.412757506108284  ,  0.2976404613256454\n",
            "epoch  14\n",
            "saving weights.... \n",
            "86.73  % ,  0.3889356889247894  ,  0.28838552982211113\n",
            "epoch  15\n",
            "saving weights.... \n",
            "84.81  % ,  0.42925305275917053  ,  0.2917184642970562\n",
            "epoch  16\n",
            "saving weights.... \n",
            "87.08  % ,  0.40768613035678863  ,  0.2887595699131489\n",
            "epoch  17\n",
            "saving weights.... \n",
            "86.47  % ,  0.3963875529050827  ,  0.28534866172373297\n",
            "epoch  18\n",
            "saving weights.... \n",
            "84.42  % ,  0.4845984498500824  ,  0.28470562533438204\n",
            "epoch  19\n",
            "saving weights.... \n",
            "85.46  % ,  0.43427531412839887  ,  0.2921884596079588\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.27  % ,  0.2672788423001766  ,  0.15464568573832513\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.47  % ,  0.2656542296677828  ,  0.1151473270829767\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.62  % ,  0.2660971188992262  ,  0.1018709402859211\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.68  % ,  0.2663109283410013  ,  0.08935190312415361\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.83  % ,  0.2644002717971802  ,  0.08475072659868747\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.6  % ,  0.27021953022703527  ,  0.07372305791005493\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.78  % ,  0.2720188515767455  ,  0.06992192987557501\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.89  % ,  0.27525102584809064  ,  0.06489441282218322\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.87  % ,  0.2730418864533305  ,  0.06107708724364638\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.93  % ,  0.27394128742814067  ,  0.05694713950231672\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_767c0b2d-ddd8-4d6a-a680-44927b76f282\", \"features.30.npy\", 435)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.3 features.30': {'nbr_param': [510.0, 6809739.0],\n",
              "  'test_acc': [83.82,\n",
              "   85.64,\n",
              "   84.67,\n",
              "   85.43,\n",
              "   84.26,\n",
              "   83.59,\n",
              "   86.91,\n",
              "   85.1,\n",
              "   85.63,\n",
              "   86.39,\n",
              "   85.28,\n",
              "   83.62,\n",
              "   85.92,\n",
              "   87.37,\n",
              "   85.31,\n",
              "   85.62,\n",
              "   84.79,\n",
              "   81.36,\n",
              "   86.03,\n",
              "   86.93,\n",
              "   90.59,\n",
              "   90.9,\n",
              "   90.9,\n",
              "   90.97,\n",
              "   90.93,\n",
              "   90.73,\n",
              "   90.73,\n",
              "   90.76,\n",
              "   91.0,\n",
              "   91.0,\n",
              "   85.91,\n",
              "   85.18,\n",
              "   85.48,\n",
              "   85.93,\n",
              "   83.16,\n",
              "   82.51,\n",
              "   86.63,\n",
              "   86.22,\n",
              "   84.39,\n",
              "   86.66,\n",
              "   84.55,\n",
              "   84.8,\n",
              "   84.68,\n",
              "   85.53,\n",
              "   87.41,\n",
              "   84.03,\n",
              "   85.44,\n",
              "   84.69,\n",
              "   86.09,\n",
              "   80.33,\n",
              "   90.29,\n",
              "   90.29,\n",
              "   90.66,\n",
              "   90.68,\n",
              "   90.66,\n",
              "   90.74,\n",
              "   90.82,\n",
              "   90.78,\n",
              "   91.08,\n",
              "   90.99,\n",
              "   84.77,\n",
              "   83.42,\n",
              "   85.72,\n",
              "   86.82,\n",
              "   86.15,\n",
              "   83.35,\n",
              "   85.23,\n",
              "   85.55,\n",
              "   83.15,\n",
              "   87.05,\n",
              "   85.44,\n",
              "   83.65,\n",
              "   85.93,\n",
              "   84.13,\n",
              "   86.01,\n",
              "   86.83,\n",
              "   87.33,\n",
              "   82.88,\n",
              "   86.02,\n",
              "   85.86,\n",
              "   90.4,\n",
              "   90.31,\n",
              "   90.72,\n",
              "   90.84,\n",
              "   90.82,\n",
              "   90.89,\n",
              "   90.74,\n",
              "   90.96,\n",
              "   91.1,\n",
              "   91.05,\n",
              "   83.6,\n",
              "   84.61,\n",
              "   85.2,\n",
              "   85.12,\n",
              "   83.52,\n",
              "   85.67,\n",
              "   85.64,\n",
              "   86.09,\n",
              "   85.19,\n",
              "   84.1,\n",
              "   85.87,\n",
              "   86.38,\n",
              "   86.53,\n",
              "   86.58,\n",
              "   85.26,\n",
              "   86.12,\n",
              "   86.58,\n",
              "   85.94,\n",
              "   85.99,\n",
              "   87.22,\n",
              "   90.5,\n",
              "   90.66,\n",
              "   90.93,\n",
              "   90.66,\n",
              "   91.23,\n",
              "   91.21,\n",
              "   91.35,\n",
              "   91.37,\n",
              "   91.03,\n",
              "   91.1,\n",
              "   85.3,\n",
              "   83.53,\n",
              "   84.39,\n",
              "   86.78,\n",
              "   85.35,\n",
              "   84.93,\n",
              "   84.65,\n",
              "   85.64,\n",
              "   85.23,\n",
              "   86.67,\n",
              "   85.61,\n",
              "   85.22,\n",
              "   86.24,\n",
              "   85.25,\n",
              "   85.59,\n",
              "   85.72,\n",
              "   86.57,\n",
              "   86.24,\n",
              "   82.26,\n",
              "   86.33,\n",
              "   90.36,\n",
              "   90.89,\n",
              "   90.81,\n",
              "   90.8,\n",
              "   91.05,\n",
              "   90.73,\n",
              "   90.66,\n",
              "   91.12,\n",
              "   91.07,\n",
              "   90.68,\n",
              "   84.75,\n",
              "   85.22,\n",
              "   82.82,\n",
              "   82.99,\n",
              "   84.55,\n",
              "   84.37,\n",
              "   85.86,\n",
              "   85.86,\n",
              "   86.39,\n",
              "   86.44,\n",
              "   84.12,\n",
              "   86.41,\n",
              "   87.16,\n",
              "   85.9,\n",
              "   86.73,\n",
              "   84.81,\n",
              "   87.08,\n",
              "   86.47,\n",
              "   84.42,\n",
              "   85.46,\n",
              "   90.27,\n",
              "   90.47,\n",
              "   90.62,\n",
              "   90.68,\n",
              "   90.83,\n",
              "   90.6,\n",
              "   90.78,\n",
              "   90.89,\n",
              "   90.87,\n",
              "   90.93],\n",
              "  'training_loss': [0.3753677779302001,\n",
              "   0.3462250084847212,\n",
              "   0.33190153065472844,\n",
              "   0.33085808129012584,\n",
              "   0.31621220431029795,\n",
              "   0.31845793751180174,\n",
              "   0.3156200372368097,\n",
              "   0.30667638978362083,\n",
              "   0.3017140057712793,\n",
              "   0.30335422294437886,\n",
              "   0.29924887719750404,\n",
              "   0.2995054669216275,\n",
              "   0.30042183919698,\n",
              "   0.29423499591201546,\n",
              "   0.2879160968989134,\n",
              "   0.29166054463386537,\n",
              "   0.2852379479587078,\n",
              "   0.28888662320971487,\n",
              "   0.2843261227250099,\n",
              "   0.2867090776130557,\n",
              "   0.1597445847414434,\n",
              "   0.1164959454987198,\n",
              "   0.10149607819765806,\n",
              "   0.09193713667113335,\n",
              "   0.08299657302908599,\n",
              "   0.07544284111168235,\n",
              "   0.07416996223498136,\n",
              "   0.06195641132164747,\n",
              "   0.05894900473840535,\n",
              "   0.05802076121531427,\n",
              "   0.39429837280511854,\n",
              "   0.3553671752125025,\n",
              "   0.34501503686010837,\n",
              "   0.33209083122611044,\n",
              "   0.32796108105778693,\n",
              "   0.3170723281145096,\n",
              "   0.3153465277403593,\n",
              "   0.3122152041375637,\n",
              "   0.3046056282103062,\n",
              "   0.3069080232709646,\n",
              "   0.3021977352619171,\n",
              "   0.30054977749884126,\n",
              "   0.29856739783585073,\n",
              "   0.2997878760367632,\n",
              "   0.2906134116381407,\n",
              "   0.2955326520241797,\n",
              "   0.29191973341703414,\n",
              "   0.2940790369391441,\n",
              "   0.288344905641675,\n",
              "   0.28170894865095614,\n",
              "   0.1520954372741282,\n",
              "   0.1117081845562905,\n",
              "   0.09709253978468478,\n",
              "   0.08833324372433125,\n",
              "   0.07947835985943676,\n",
              "   0.07543269692584872,\n",
              "   0.07155101054087282,\n",
              "   0.06445365063771606,\n",
              "   0.060682239332795145,\n",
              "   0.05864731523729861,\n",
              "   0.3685512093707919,\n",
              "   0.3371828979581594,\n",
              "   0.3286381820946932,\n",
              "   0.3241369148075581,\n",
              "   0.3184570017397404,\n",
              "   0.3079734340786934,\n",
              "   0.30973900848329067,\n",
              "   0.306109617254138,\n",
              "   0.29709719207584856,\n",
              "   0.30380172375440595,\n",
              "   0.3002468601167202,\n",
              "   0.29639660666286943,\n",
              "   0.2965314051315188,\n",
              "   0.2893953766167164,\n",
              "   0.29016671568602326,\n",
              "   0.28621738185584544,\n",
              "   0.2849843683093786,\n",
              "   0.28275206263959407,\n",
              "   0.28600578876435756,\n",
              "   0.28133200568705796,\n",
              "   0.15443370637521148,\n",
              "   0.11235335097424685,\n",
              "   0.09984624680802226,\n",
              "   0.0871989216748625,\n",
              "   0.07900207051597535,\n",
              "   0.07628536203242839,\n",
              "   0.070257218753919,\n",
              "   0.06484409718997776,\n",
              "   0.06028659621998668,\n",
              "   0.05799086586367339,\n",
              "   0.3934792795419693,\n",
              "   0.34729202667474746,\n",
              "   0.33922129538953305,\n",
              "   0.32510631482601166,\n",
              "   0.3214276333063841,\n",
              "   0.3160181904524565,\n",
              "   0.31638100820183757,\n",
              "   0.30552080661654474,\n",
              "   0.31130229723453523,\n",
              "   0.31365844508707524,\n",
              "   0.3045675350189209,\n",
              "   0.30016504330337046,\n",
              "   0.29302590612769125,\n",
              "   0.2957667922079563,\n",
              "   0.2903592157244682,\n",
              "   0.29322415421158077,\n",
              "   0.28749175202548505,\n",
              "   0.2898221112132072,\n",
              "   0.2880531962096691,\n",
              "   0.28664574117958547,\n",
              "   0.15966346889808775,\n",
              "   0.11469841128066183,\n",
              "   0.09643973640836775,\n",
              "   0.093212246100232,\n",
              "   0.0806673552710563,\n",
              "   0.0757650531237945,\n",
              "   0.06844618465341627,\n",
              "   0.06614497600011528,\n",
              "   0.05931416151504964,\n",
              "   0.057600654394179586,\n",
              "   0.384310731035471,\n",
              "   0.3484098303705454,\n",
              "   0.3305602998241782,\n",
              "   0.3316704084724188,\n",
              "   0.3201140031576157,\n",
              "   0.3171138580173254,\n",
              "   0.30742178422808647,\n",
              "   0.3062221253603697,\n",
              "   0.307082258066535,\n",
              "   0.3041555880665779,\n",
              "   0.3012759686022997,\n",
              "   0.3005621831998229,\n",
              "   0.3001861132025719,\n",
              "   0.2929652029752731,\n",
              "   0.29077415713071825,\n",
              "   0.2863423902451992,\n",
              "   0.29025631477683783,\n",
              "   0.28362039989829063,\n",
              "   0.2816042103484273,\n",
              "   0.2918580346047878,\n",
              "   0.15614130154028535,\n",
              "   0.11599626542776822,\n",
              "   0.1016306979764253,\n",
              "   0.09305325521491468,\n",
              "   0.08230522548556328,\n",
              "   0.07594876559246332,\n",
              "   0.07018269618805498,\n",
              "   0.06641674124747515,\n",
              "   0.06010873744292185,\n",
              "   0.05690802346384153,\n",
              "   0.389750537699461,\n",
              "   0.34861727122366426,\n",
              "   0.3387937770873308,\n",
              "   0.32263996513038873,\n",
              "   0.3197401510104537,\n",
              "   0.32074317649900913,\n",
              "   0.30821431506872177,\n",
              "   0.3124615932092071,\n",
              "   0.3057874827593565,\n",
              "   0.3028174328818917,\n",
              "   0.3031667965173721,\n",
              "   0.29817766211330893,\n",
              "   0.29135231768488884,\n",
              "   0.2976404613256454,\n",
              "   0.28838552982211113,\n",
              "   0.2917184642970562,\n",
              "   0.2887595699131489,\n",
              "   0.28534866172373297,\n",
              "   0.28470562533438204,\n",
              "   0.2921884596079588,\n",
              "   0.15464568573832513,\n",
              "   0.1151473270829767,\n",
              "   0.1018709402859211,\n",
              "   0.08935190312415361,\n",
              "   0.08475072659868747,\n",
              "   0.07372305791005493,\n",
              "   0.06992192987557501,\n",
              "   0.06489441282218322,\n",
              "   0.06107708724364638,\n",
              "   0.05694713950231672],\n",
              "  'valid_loss': [0.4152320156812668,\n",
              "   0.3291621312737465,\n",
              "   0.42519748874902724,\n",
              "   0.3677966022372246,\n",
              "   0.42426420383453367,\n",
              "   0.45190358481407167,\n",
              "   0.3446594207048416,\n",
              "   0.4067449975728989,\n",
              "   0.41236174714565277,\n",
              "   0.39068249773979186,\n",
              "   0.4123280806303024,\n",
              "   0.4497375878095627,\n",
              "   0.41055611610412596,\n",
              "   0.36766749041080476,\n",
              "   0.4245355642557144,\n",
              "   0.43171266226768495,\n",
              "   0.4214659571170807,\n",
              "   0.5816758922696114,\n",
              "   0.4123483793497086,\n",
              "   0.3895028819680214,\n",
              "   0.2815485555410385,\n",
              "   0.2708542135596275,\n",
              "   0.2725050296664238,\n",
              "   0.2734640574067831,\n",
              "   0.27715895022749903,\n",
              "   0.2790839325353503,\n",
              "   0.2827938908740878,\n",
              "   0.2837661345630884,\n",
              "   0.2741121260151267,\n",
              "   0.2837418727725744,\n",
              "   0.354346767103672,\n",
              "   0.39725565469264984,\n",
              "   0.36713681919574737,\n",
              "   0.37880265949964526,\n",
              "   0.46249574167728424,\n",
              "   0.472403818333149,\n",
              "   0.3574519354104996,\n",
              "   0.3797896231412888,\n",
              "   0.43696511981487274,\n",
              "   0.384654604434967,\n",
              "   0.45122489848136904,\n",
              "   0.43100540071725846,\n",
              "   0.449257036113739,\n",
              "   0.4280964708566666,\n",
              "   0.3729886759161949,\n",
              "   0.4561535767555237,\n",
              "   0.4210824444055557,\n",
              "   0.45999600698947907,\n",
              "   0.41505226352214813,\n",
              "   0.6413579344272613,\n",
              "   0.27564551243186,\n",
              "   0.2759259744465351,\n",
              "   0.2727304780572653,\n",
              "   0.2777708668410778,\n",
              "   0.2746670603513718,\n",
              "   0.2740560863018036,\n",
              "   0.27659193990305064,\n",
              "   0.2726453690856695,\n",
              "   0.27664557425677777,\n",
              "   0.2713297633096576,\n",
              "   0.333239211666584,\n",
              "   0.42184072368144987,\n",
              "   0.34416522830724716,\n",
              "   0.35359407682418825,\n",
              "   0.36549833872318266,\n",
              "   0.4521233827352524,\n",
              "   0.397946688747406,\n",
              "   0.41473715530633926,\n",
              "   0.5183982539653778,\n",
              "   0.3535485223054886,\n",
              "   0.4140156543493271,\n",
              "   0.451261162519455,\n",
              "   0.39337602719068526,\n",
              "   0.4405787266254425,\n",
              "   0.3934565791606903,\n",
              "   0.37802358477115633,\n",
              "   0.3756810461759567,\n",
              "   0.4859171051740646,\n",
              "   0.4060606942653656,\n",
              "   0.4156432765722275,\n",
              "   0.26642648804187774,\n",
              "   0.2596003626346588,\n",
              "   0.2662683255374432,\n",
              "   0.26172658042907715,\n",
              "   0.26752212966009975,\n",
              "   0.2672723892509937,\n",
              "   0.27207563925385475,\n",
              "   0.2638749162197113,\n",
              "   0.27894575426876544,\n",
              "   0.27140766783952713,\n",
              "   0.3792881876707077,\n",
              "   0.4190415398478508,\n",
              "   0.3696481939315796,\n",
              "   0.40323504593372345,\n",
              "   0.45052973322868345,\n",
              "   0.3711602658033371,\n",
              "   0.37740556936264036,\n",
              "   0.3818258926272392,\n",
              "   0.4199265293598175,\n",
              "   0.4405544481039047,\n",
              "   0.3831452165126801,\n",
              "   0.4020632388830185,\n",
              "   0.39706599984169005,\n",
              "   0.4215278472661972,\n",
              "   0.4283349490880966,\n",
              "   0.4184819757938385,\n",
              "   0.3918377013683319,\n",
              "   0.4361732797861099,\n",
              "   0.41984463455677035,\n",
              "   0.3862010722458363,\n",
              "   0.2705543124884367,\n",
              "   0.2665045948773623,\n",
              "   0.2603924602895975,\n",
              "   0.25616821076124907,\n",
              "   0.27227321815937755,\n",
              "   0.2642251648053527,\n",
              "   0.26019140630215404,\n",
              "   0.25554369194209575,\n",
              "   0.27576667933017013,\n",
              "   0.277538061132282,\n",
              "   0.3695487419843674,\n",
              "   0.3869398869037628,\n",
              "   0.4132691576957703,\n",
              "   0.35521349189281465,\n",
              "   0.4082661004424095,\n",
              "   0.43268994417190554,\n",
              "   0.4418498308420181,\n",
              "   0.38357009875774384,\n",
              "   0.40524812899827956,\n",
              "   0.3893915811538696,\n",
              "   0.404174605512619,\n",
              "   0.4211113221168518,\n",
              "   0.3829330843448639,\n",
              "   0.40650593234300614,\n",
              "   0.424862975358963,\n",
              "   0.42295445343255994,\n",
              "   0.41183751838207244,\n",
              "   0.3968865024089813,\n",
              "   0.5445519752264023,\n",
              "   0.3924273215532303,\n",
              "   0.27219469935894014,\n",
              "   0.256604188233614,\n",
              "   0.2640684949249029,\n",
              "   0.26455358221530917,\n",
              "   0.2704180570870638,\n",
              "   0.27147540556490424,\n",
              "   0.2710482469238341,\n",
              "   0.2746137186989188,\n",
              "   0.28445716969743373,\n",
              "   0.28358544899225235,\n",
              "   0.3595377722263336,\n",
              "   0.3757767829656601,\n",
              "   0.4407354746222496,\n",
              "   0.4478885493278503,\n",
              "   0.41084517695903777,\n",
              "   0.4239084290266037,\n",
              "   0.39846336534023286,\n",
              "   0.3870759885549545,\n",
              "   0.36840578147172925,\n",
              "   0.38139032323360444,\n",
              "   0.450024591255188,\n",
              "   0.3878326279997826,\n",
              "   0.37619855046272277,\n",
              "   0.412757506108284,\n",
              "   0.3889356889247894,\n",
              "   0.42925305275917053,\n",
              "   0.40768613035678863,\n",
              "   0.3963875529050827,\n",
              "   0.4845984498500824,\n",
              "   0.43427531412839887,\n",
              "   0.2672788423001766,\n",
              "   0.2656542296677828,\n",
              "   0.2660971188992262,\n",
              "   0.2663109283410013,\n",
              "   0.2644002717971802,\n",
              "   0.27021953022703527,\n",
              "   0.2720188515767455,\n",
              "   0.27525102584809064,\n",
              "   0.2730418864533305,\n",
              "   0.27394128742814067]},\n",
              " '0.6 features.30': {'nbr_param': [510.0, 6425247.0],\n",
              "  'test_acc': [83.82,\n",
              "   85.64,\n",
              "   84.67,\n",
              "   85.43,\n",
              "   84.26,\n",
              "   83.59,\n",
              "   86.91,\n",
              "   85.1,\n",
              "   85.63,\n",
              "   86.39,\n",
              "   85.28,\n",
              "   83.62,\n",
              "   85.92,\n",
              "   87.37,\n",
              "   85.31,\n",
              "   85.62,\n",
              "   84.79,\n",
              "   81.36,\n",
              "   86.03,\n",
              "   86.93,\n",
              "   90.59,\n",
              "   90.9,\n",
              "   90.9,\n",
              "   90.97,\n",
              "   90.93,\n",
              "   90.73,\n",
              "   90.73,\n",
              "   90.76,\n",
              "   91.0,\n",
              "   91.0,\n",
              "   85.91,\n",
              "   85.18,\n",
              "   85.48,\n",
              "   85.93,\n",
              "   83.16,\n",
              "   82.51,\n",
              "   86.63,\n",
              "   86.22,\n",
              "   84.39,\n",
              "   86.66,\n",
              "   84.55,\n",
              "   84.8,\n",
              "   84.68,\n",
              "   85.53,\n",
              "   87.41,\n",
              "   84.03,\n",
              "   85.44,\n",
              "   84.69,\n",
              "   86.09,\n",
              "   80.33,\n",
              "   90.29,\n",
              "   90.29,\n",
              "   90.66,\n",
              "   90.68,\n",
              "   90.66,\n",
              "   90.74,\n",
              "   90.82,\n",
              "   90.78,\n",
              "   91.08,\n",
              "   90.99,\n",
              "   84.77,\n",
              "   83.42,\n",
              "   85.72,\n",
              "   86.82,\n",
              "   86.15,\n",
              "   83.35,\n",
              "   85.23,\n",
              "   85.55,\n",
              "   83.15,\n",
              "   87.05,\n",
              "   85.44,\n",
              "   83.65,\n",
              "   85.93,\n",
              "   84.13,\n",
              "   86.01,\n",
              "   86.83,\n",
              "   87.33,\n",
              "   82.88,\n",
              "   86.02,\n",
              "   85.86,\n",
              "   90.4,\n",
              "   90.31,\n",
              "   90.72,\n",
              "   90.84,\n",
              "   90.82,\n",
              "   90.89,\n",
              "   90.74,\n",
              "   90.96,\n",
              "   91.1,\n",
              "   91.05,\n",
              "   83.6,\n",
              "   84.61,\n",
              "   85.2,\n",
              "   85.12,\n",
              "   83.52,\n",
              "   85.67,\n",
              "   85.64,\n",
              "   86.09,\n",
              "   85.19,\n",
              "   84.1,\n",
              "   85.87,\n",
              "   86.38,\n",
              "   86.53,\n",
              "   86.58,\n",
              "   85.26,\n",
              "   86.12,\n",
              "   86.58,\n",
              "   85.94,\n",
              "   85.99,\n",
              "   87.22,\n",
              "   90.5,\n",
              "   90.66,\n",
              "   90.93,\n",
              "   90.66,\n",
              "   91.23,\n",
              "   91.21,\n",
              "   91.35,\n",
              "   91.37,\n",
              "   91.03,\n",
              "   91.1,\n",
              "   85.3,\n",
              "   83.53,\n",
              "   84.39,\n",
              "   86.78,\n",
              "   85.35,\n",
              "   84.93,\n",
              "   84.65,\n",
              "   85.64,\n",
              "   85.23,\n",
              "   86.67,\n",
              "   85.61,\n",
              "   85.22,\n",
              "   86.24,\n",
              "   85.25,\n",
              "   85.59,\n",
              "   85.72,\n",
              "   86.57,\n",
              "   86.24,\n",
              "   82.26,\n",
              "   86.33,\n",
              "   90.36,\n",
              "   90.89,\n",
              "   90.81,\n",
              "   90.8,\n",
              "   91.05,\n",
              "   90.73,\n",
              "   90.66,\n",
              "   91.12,\n",
              "   91.07,\n",
              "   90.68,\n",
              "   84.75,\n",
              "   85.22,\n",
              "   82.82,\n",
              "   82.99,\n",
              "   84.55,\n",
              "   84.37,\n",
              "   85.86,\n",
              "   85.86,\n",
              "   86.39,\n",
              "   86.44,\n",
              "   84.12,\n",
              "   86.41,\n",
              "   87.16,\n",
              "   85.9,\n",
              "   86.73,\n",
              "   84.81,\n",
              "   87.08,\n",
              "   86.47,\n",
              "   84.42,\n",
              "   85.46,\n",
              "   90.27,\n",
              "   90.47,\n",
              "   90.62,\n",
              "   90.68,\n",
              "   90.83,\n",
              "   90.6,\n",
              "   90.78,\n",
              "   90.89,\n",
              "   90.87,\n",
              "   90.93],\n",
              "  'training_loss': [0.3753677779302001,\n",
              "   0.3462250084847212,\n",
              "   0.33190153065472844,\n",
              "   0.33085808129012584,\n",
              "   0.31621220431029795,\n",
              "   0.31845793751180174,\n",
              "   0.3156200372368097,\n",
              "   0.30667638978362083,\n",
              "   0.3017140057712793,\n",
              "   0.30335422294437886,\n",
              "   0.29924887719750404,\n",
              "   0.2995054669216275,\n",
              "   0.30042183919698,\n",
              "   0.29423499591201546,\n",
              "   0.2879160968989134,\n",
              "   0.29166054463386537,\n",
              "   0.2852379479587078,\n",
              "   0.28888662320971487,\n",
              "   0.2843261227250099,\n",
              "   0.2867090776130557,\n",
              "   0.1597445847414434,\n",
              "   0.1164959454987198,\n",
              "   0.10149607819765806,\n",
              "   0.09193713667113335,\n",
              "   0.08299657302908599,\n",
              "   0.07544284111168235,\n",
              "   0.07416996223498136,\n",
              "   0.06195641132164747,\n",
              "   0.05894900473840535,\n",
              "   0.05802076121531427,\n",
              "   0.39429837280511854,\n",
              "   0.3553671752125025,\n",
              "   0.34501503686010837,\n",
              "   0.33209083122611044,\n",
              "   0.32796108105778693,\n",
              "   0.3170723281145096,\n",
              "   0.3153465277403593,\n",
              "   0.3122152041375637,\n",
              "   0.3046056282103062,\n",
              "   0.3069080232709646,\n",
              "   0.3021977352619171,\n",
              "   0.30054977749884126,\n",
              "   0.29856739783585073,\n",
              "   0.2997878760367632,\n",
              "   0.2906134116381407,\n",
              "   0.2955326520241797,\n",
              "   0.29191973341703414,\n",
              "   0.2940790369391441,\n",
              "   0.288344905641675,\n",
              "   0.28170894865095614,\n",
              "   0.1520954372741282,\n",
              "   0.1117081845562905,\n",
              "   0.09709253978468478,\n",
              "   0.08833324372433125,\n",
              "   0.07947835985943676,\n",
              "   0.07543269692584872,\n",
              "   0.07155101054087282,\n",
              "   0.06445365063771606,\n",
              "   0.060682239332795145,\n",
              "   0.05864731523729861,\n",
              "   0.3685512093707919,\n",
              "   0.3371828979581594,\n",
              "   0.3286381820946932,\n",
              "   0.3241369148075581,\n",
              "   0.3184570017397404,\n",
              "   0.3079734340786934,\n",
              "   0.30973900848329067,\n",
              "   0.306109617254138,\n",
              "   0.29709719207584856,\n",
              "   0.30380172375440595,\n",
              "   0.3002468601167202,\n",
              "   0.29639660666286943,\n",
              "   0.2965314051315188,\n",
              "   0.2893953766167164,\n",
              "   0.29016671568602326,\n",
              "   0.28621738185584544,\n",
              "   0.2849843683093786,\n",
              "   0.28275206263959407,\n",
              "   0.28600578876435756,\n",
              "   0.28133200568705796,\n",
              "   0.15443370637521148,\n",
              "   0.11235335097424685,\n",
              "   0.09984624680802226,\n",
              "   0.0871989216748625,\n",
              "   0.07900207051597535,\n",
              "   0.07628536203242839,\n",
              "   0.070257218753919,\n",
              "   0.06484409718997776,\n",
              "   0.06028659621998668,\n",
              "   0.05799086586367339,\n",
              "   0.3934792795419693,\n",
              "   0.34729202667474746,\n",
              "   0.33922129538953305,\n",
              "   0.32510631482601166,\n",
              "   0.3214276333063841,\n",
              "   0.3160181904524565,\n",
              "   0.31638100820183757,\n",
              "   0.30552080661654474,\n",
              "   0.31130229723453523,\n",
              "   0.31365844508707524,\n",
              "   0.3045675350189209,\n",
              "   0.30016504330337046,\n",
              "   0.29302590612769125,\n",
              "   0.2957667922079563,\n",
              "   0.2903592157244682,\n",
              "   0.29322415421158077,\n",
              "   0.28749175202548505,\n",
              "   0.2898221112132072,\n",
              "   0.2880531962096691,\n",
              "   0.28664574117958547,\n",
              "   0.15966346889808775,\n",
              "   0.11469841128066183,\n",
              "   0.09643973640836775,\n",
              "   0.093212246100232,\n",
              "   0.0806673552710563,\n",
              "   0.0757650531237945,\n",
              "   0.06844618465341627,\n",
              "   0.06614497600011528,\n",
              "   0.05931416151504964,\n",
              "   0.057600654394179586,\n",
              "   0.384310731035471,\n",
              "   0.3484098303705454,\n",
              "   0.3305602998241782,\n",
              "   0.3316704084724188,\n",
              "   0.3201140031576157,\n",
              "   0.3171138580173254,\n",
              "   0.30742178422808647,\n",
              "   0.3062221253603697,\n",
              "   0.307082258066535,\n",
              "   0.3041555880665779,\n",
              "   0.3012759686022997,\n",
              "   0.3005621831998229,\n",
              "   0.3001861132025719,\n",
              "   0.2929652029752731,\n",
              "   0.29077415713071825,\n",
              "   0.2863423902451992,\n",
              "   0.29025631477683783,\n",
              "   0.28362039989829063,\n",
              "   0.2816042103484273,\n",
              "   0.2918580346047878,\n",
              "   0.15614130154028535,\n",
              "   0.11599626542776822,\n",
              "   0.1016306979764253,\n",
              "   0.09305325521491468,\n",
              "   0.08230522548556328,\n",
              "   0.07594876559246332,\n",
              "   0.07018269618805498,\n",
              "   0.06641674124747515,\n",
              "   0.06010873744292185,\n",
              "   0.05690802346384153,\n",
              "   0.389750537699461,\n",
              "   0.34861727122366426,\n",
              "   0.3387937770873308,\n",
              "   0.32263996513038873,\n",
              "   0.3197401510104537,\n",
              "   0.32074317649900913,\n",
              "   0.30821431506872177,\n",
              "   0.3124615932092071,\n",
              "   0.3057874827593565,\n",
              "   0.3028174328818917,\n",
              "   0.3031667965173721,\n",
              "   0.29817766211330893,\n",
              "   0.29135231768488884,\n",
              "   0.2976404613256454,\n",
              "   0.28838552982211113,\n",
              "   0.2917184642970562,\n",
              "   0.2887595699131489,\n",
              "   0.28534866172373297,\n",
              "   0.28470562533438204,\n",
              "   0.2921884596079588,\n",
              "   0.15464568573832513,\n",
              "   0.1151473270829767,\n",
              "   0.1018709402859211,\n",
              "   0.08935190312415361,\n",
              "   0.08475072659868747,\n",
              "   0.07372305791005493,\n",
              "   0.06992192987557501,\n",
              "   0.06489441282218322,\n",
              "   0.06107708724364638,\n",
              "   0.05694713950231672],\n",
              "  'valid_loss': [0.4152320156812668,\n",
              "   0.3291621312737465,\n",
              "   0.42519748874902724,\n",
              "   0.3677966022372246,\n",
              "   0.42426420383453367,\n",
              "   0.45190358481407167,\n",
              "   0.3446594207048416,\n",
              "   0.4067449975728989,\n",
              "   0.41236174714565277,\n",
              "   0.39068249773979186,\n",
              "   0.4123280806303024,\n",
              "   0.4497375878095627,\n",
              "   0.41055611610412596,\n",
              "   0.36766749041080476,\n",
              "   0.4245355642557144,\n",
              "   0.43171266226768495,\n",
              "   0.4214659571170807,\n",
              "   0.5816758922696114,\n",
              "   0.4123483793497086,\n",
              "   0.3895028819680214,\n",
              "   0.2815485555410385,\n",
              "   0.2708542135596275,\n",
              "   0.2725050296664238,\n",
              "   0.2734640574067831,\n",
              "   0.27715895022749903,\n",
              "   0.2790839325353503,\n",
              "   0.2827938908740878,\n",
              "   0.2837661345630884,\n",
              "   0.2741121260151267,\n",
              "   0.2837418727725744,\n",
              "   0.354346767103672,\n",
              "   0.39725565469264984,\n",
              "   0.36713681919574737,\n",
              "   0.37880265949964526,\n",
              "   0.46249574167728424,\n",
              "   0.472403818333149,\n",
              "   0.3574519354104996,\n",
              "   0.3797896231412888,\n",
              "   0.43696511981487274,\n",
              "   0.384654604434967,\n",
              "   0.45122489848136904,\n",
              "   0.43100540071725846,\n",
              "   0.449257036113739,\n",
              "   0.4280964708566666,\n",
              "   0.3729886759161949,\n",
              "   0.4561535767555237,\n",
              "   0.4210824444055557,\n",
              "   0.45999600698947907,\n",
              "   0.41505226352214813,\n",
              "   0.6413579344272613,\n",
              "   0.27564551243186,\n",
              "   0.2759259744465351,\n",
              "   0.2727304780572653,\n",
              "   0.2777708668410778,\n",
              "   0.2746670603513718,\n",
              "   0.2740560863018036,\n",
              "   0.27659193990305064,\n",
              "   0.2726453690856695,\n",
              "   0.27664557425677777,\n",
              "   0.2713297633096576,\n",
              "   0.333239211666584,\n",
              "   0.42184072368144987,\n",
              "   0.34416522830724716,\n",
              "   0.35359407682418825,\n",
              "   0.36549833872318266,\n",
              "   0.4521233827352524,\n",
              "   0.397946688747406,\n",
              "   0.41473715530633926,\n",
              "   0.5183982539653778,\n",
              "   0.3535485223054886,\n",
              "   0.4140156543493271,\n",
              "   0.451261162519455,\n",
              "   0.39337602719068526,\n",
              "   0.4405787266254425,\n",
              "   0.3934565791606903,\n",
              "   0.37802358477115633,\n",
              "   0.3756810461759567,\n",
              "   0.4859171051740646,\n",
              "   0.4060606942653656,\n",
              "   0.4156432765722275,\n",
              "   0.26642648804187774,\n",
              "   0.2596003626346588,\n",
              "   0.2662683255374432,\n",
              "   0.26172658042907715,\n",
              "   0.26752212966009975,\n",
              "   0.2672723892509937,\n",
              "   0.27207563925385475,\n",
              "   0.2638749162197113,\n",
              "   0.27894575426876544,\n",
              "   0.27140766783952713,\n",
              "   0.3792881876707077,\n",
              "   0.4190415398478508,\n",
              "   0.3696481939315796,\n",
              "   0.40323504593372345,\n",
              "   0.45052973322868345,\n",
              "   0.3711602658033371,\n",
              "   0.37740556936264036,\n",
              "   0.3818258926272392,\n",
              "   0.4199265293598175,\n",
              "   0.4405544481039047,\n",
              "   0.3831452165126801,\n",
              "   0.4020632388830185,\n",
              "   0.39706599984169005,\n",
              "   0.4215278472661972,\n",
              "   0.4283349490880966,\n",
              "   0.4184819757938385,\n",
              "   0.3918377013683319,\n",
              "   0.4361732797861099,\n",
              "   0.41984463455677035,\n",
              "   0.3862010722458363,\n",
              "   0.2705543124884367,\n",
              "   0.2665045948773623,\n",
              "   0.2603924602895975,\n",
              "   0.25616821076124907,\n",
              "   0.27227321815937755,\n",
              "   0.2642251648053527,\n",
              "   0.26019140630215404,\n",
              "   0.25554369194209575,\n",
              "   0.27576667933017013,\n",
              "   0.277538061132282,\n",
              "   0.3695487419843674,\n",
              "   0.3869398869037628,\n",
              "   0.4132691576957703,\n",
              "   0.35521349189281465,\n",
              "   0.4082661004424095,\n",
              "   0.43268994417190554,\n",
              "   0.4418498308420181,\n",
              "   0.38357009875774384,\n",
              "   0.40524812899827956,\n",
              "   0.3893915811538696,\n",
              "   0.404174605512619,\n",
              "   0.4211113221168518,\n",
              "   0.3829330843448639,\n",
              "   0.40650593234300614,\n",
              "   0.424862975358963,\n",
              "   0.42295445343255994,\n",
              "   0.41183751838207244,\n",
              "   0.3968865024089813,\n",
              "   0.5445519752264023,\n",
              "   0.3924273215532303,\n",
              "   0.27219469935894014,\n",
              "   0.256604188233614,\n",
              "   0.2640684949249029,\n",
              "   0.26455358221530917,\n",
              "   0.2704180570870638,\n",
              "   0.27147540556490424,\n",
              "   0.2710482469238341,\n",
              "   0.2746137186989188,\n",
              "   0.28445716969743373,\n",
              "   0.28358544899225235,\n",
              "   0.3595377722263336,\n",
              "   0.3757767829656601,\n",
              "   0.4407354746222496,\n",
              "   0.4478885493278503,\n",
              "   0.41084517695903777,\n",
              "   0.4239084290266037,\n",
              "   0.39846336534023286,\n",
              "   0.3870759885549545,\n",
              "   0.36840578147172925,\n",
              "   0.38139032323360444,\n",
              "   0.450024591255188,\n",
              "   0.3878326279997826,\n",
              "   0.37619855046272277,\n",
              "   0.412757506108284,\n",
              "   0.3889356889247894,\n",
              "   0.42925305275917053,\n",
              "   0.40768613035678863,\n",
              "   0.3963875529050827,\n",
              "   0.4845984498500824,\n",
              "   0.43427531412839887,\n",
              "   0.2672788423001766,\n",
              "   0.2656542296677828,\n",
              "   0.2660971188992262,\n",
              "   0.2663109283410013,\n",
              "   0.2644002717971802,\n",
              "   0.27021953022703527,\n",
              "   0.2720188515767455,\n",
              "   0.27525102584809064,\n",
              "   0.2730418864533305,\n",
              "   0.27394128742814067]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qO3s-fmXnbdu"
      },
      "source": [
        "### Prune sur \"features.34\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "95v05ewspp3G",
        "outputId": "8fa90718-5a5b-4875-ea76-180057f479d2"
      },
      "source": [
        "prune_fine_tune(10,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chargez les poids\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-06e91771-e29c-4f63-bb20-850b4ac8f4d3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-06e91771-e29c-4f63-bb20-850b4ac8f4d3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving checkpoint.pt to checkpoint.pt\n",
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.34\n",
            "features.34\n",
            "6809739.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "81.45  % ,  0.445020611679554  ,  0.35084924191869793\n",
            "epoch  1\n",
            "saving weights.... \n",
            "83.87  % ,  0.43143266570568084  ,  0.33106836324334143\n",
            "epoch  2\n",
            "saving weights.... \n",
            "86.15  % ,  0.3336475734829903  ,  0.3264572918057442\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.34  % ,  0.39070509229898454  ,  0.32258193586170675\n",
            "epoch  4\n",
            "saving weights.... \n",
            "85.9  % ,  0.37596815447807314  ,  0.30834390057921407\n",
            "epoch  5\n",
            "saving weights.... \n",
            "86.16  % ,  0.3753319277882576  ,  0.3096859028726816\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.47  % ,  0.39195153474807737  ,  0.3019952151685953\n",
            "epoch  7\n",
            "saving weights.... \n",
            "86.3  % ,  0.3899280695796013  ,  0.3045683059573174\n",
            "epoch  8\n",
            "saving weights.... \n",
            "85.49  % ,  0.4148231076717377  ,  0.2969441183447838\n",
            "epoch  9\n",
            "saving weights.... \n",
            "84.39  % ,  0.4671101590037346  ,  0.29681066096127035\n",
            "epoch  10\n",
            "saving weights.... \n",
            "86.94  % ,  0.37577719094753265  ,  0.29954650689661505\n",
            "epoch  11\n",
            "saving weights.... \n",
            "84.58  % ,  0.44902252085208894  ,  0.2892873609080911\n",
            "epoch  12\n",
            "saving weights.... \n",
            "83.76  % ,  0.48366415395736695  ,  0.29731301822960376\n",
            "epoch  13\n",
            "saving weights.... \n",
            "86.98  % ,  0.38142940596342084  ,  0.2883898736402392\n",
            "epoch  14\n",
            "saving weights.... \n",
            "85.56  % ,  0.4349351691126823  ,  0.28643721699416635\n",
            "epoch  15\n",
            "saving weights.... \n",
            "84.68  % ,  0.4561902080774307  ,  0.2871512551739812\n",
            "epoch  16\n",
            "saving weights.... \n",
            "86.86  % ,  0.374283144402504  ,  0.2908075893908739\n",
            "epoch  17\n",
            "saving weights.... \n",
            "85.73  % ,  0.4381800763249397  ,  0.2831595894038677\n",
            "epoch  18\n",
            "saving weights.... \n",
            "84.84  % ,  0.4702612398147583  ,  0.27482943947315214\n",
            "epoch  19\n",
            "saving weights.... \n",
            "84.46  % ,  0.49551295857429506  ,  0.27688033969402315\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.46  % ,  0.27742889786362646  ,  0.1570197295330465\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.74  % ,  0.26843400350213054  ,  0.11273897748738528\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.62  % ,  0.2728784598827362  ,  0.09747598305270076\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.88  % ,  0.27477525190114976  ,  0.0871900948897004\n",
            "epoch  4\n",
            "saving weights.... \n",
            "91.03  % ,  0.2835668076232076  ,  0.08088943989910186\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.94  % ,  0.2755814109209925  ,  0.07286589509807527\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.82  % ,  0.2766463681876659  ,  0.06815798930078745\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.76  % ,  0.2857860713973641  ,  0.06490975444931536\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.96  % ,  0.288745869487524  ,  0.05689116675425321\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.95  % ,  0.28298514307141304  ,  0.05504231601404026\n",
            "Finished Training\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.34\n",
            "features.34\n",
            "6425247.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "83.39  % ,  0.37600775475502013  ,  0.3490093362249434\n",
            "epoch  1\n",
            "saving weights.... \n",
            "84.83  % ,  0.39765059051513674  ,  0.3341561746329069\n",
            "epoch  2\n",
            "saving weights.... \n",
            "85.22  % ,  0.38134266875982287  ,  0.3225847297281027\n",
            "epoch  3\n",
            "saving weights.... \n",
            "84.36  % ,  0.42533464567661283  ,  0.3229009267747402\n",
            "epoch  4\n",
            "saving weights.... \n",
            "84.2  % ,  0.4227616546392441  ,  0.31207209722101686\n",
            "epoch  5\n",
            "saving weights.... \n",
            "85.99  % ,  0.37355665125846865  ,  0.3082287725716829\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.64  % ,  0.3589714109659195  ,  0.30282032886892557\n",
            "epoch  7\n",
            "saving weights.... \n",
            "85.06  % ,  0.42305661754608154  ,  0.3000041302204132\n",
            "epoch  8\n",
            "saving weights.... \n",
            "86.18  % ,  0.3894899540185928  ,  0.297326535987854\n",
            "epoch  9\n",
            "saving weights.... \n",
            "85.07  % ,  0.43216359377503394  ,  0.29553226284086703\n",
            "epoch  10\n",
            "saving weights.... \n",
            "85.59  % ,  0.41648665083646774  ,  0.29052219577431676\n",
            "epoch  11\n",
            "saving weights.... \n",
            "85.3  % ,  0.41947494733929636  ,  0.29814579522311685\n",
            "epoch  12\n",
            "saving weights.... \n",
            "83.05  % ,  0.47482827951908113  ,  0.2911034922719002\n",
            "epoch  13\n",
            "saving weights.... \n",
            "83.72  % ,  0.5120783518075943  ,  0.2860803555697203\n",
            "epoch  14\n",
            "saving weights.... \n",
            "85.13  % ,  0.4353105638861656  ,  0.28511460112929343\n",
            "epoch  15\n",
            "saving weights.... \n",
            "85.88  % ,  0.4125898102760315  ,  0.2889214350298047\n",
            "epoch  16\n",
            "saving weights.... \n",
            "86.78  % ,  0.3897325955629349  ,  0.28548475922346117\n",
            "epoch  17\n",
            "saving weights.... \n",
            "84.97  % ,  0.44729194438457487  ,  0.29005676914751527\n",
            "epoch  18\n",
            "saving weights.... \n",
            "84.54  % ,  0.44835713720321657  ,  0.28315103187263013\n",
            "epoch  19\n",
            "saving weights.... \n",
            "85.23  % ,  0.4361024504184723  ,  0.2783119087189436\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.37  % ,  0.2747956767976284  ,  0.14971897754147648\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.71  % ,  0.2685240683019161  ,  0.10876924117542804\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.88  % ,  0.2720258569866419  ,  0.09644272176064551\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.79  % ,  0.26815159222483637  ,  0.08653565613180399\n",
            "epoch  4\n",
            "saving weights.... \n",
            "91.04  % ,  0.264681710896641  ,  0.07774113832805306\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.69  % ,  0.2755093263477087  ,  0.07490336100943387\n",
            "epoch  6\n",
            "saving weights.... \n",
            "91.11  % ,  0.27389654819965364  ,  0.06530787081457674\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.89  % ,  0.2693574653826654  ,  0.06330178949665279\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.89  % ,  0.2791830378778279  ,  0.05504229255290702\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.87  % ,  0.2693709916852415  ,  0.05482144209183753\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_18712f5e-66f3-4926-afab-87b4f2a3f017\", \"features.34.npy\", 435)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.3 features.34': {'nbr_param': [510.0, 6809739.0],\n",
              "  'test_acc': [81.45,\n",
              "   83.87,\n",
              "   86.15,\n",
              "   85.34,\n",
              "   85.9,\n",
              "   86.16,\n",
              "   85.47,\n",
              "   86.3,\n",
              "   85.49,\n",
              "   84.39,\n",
              "   86.94,\n",
              "   84.58,\n",
              "   83.76,\n",
              "   86.98,\n",
              "   85.56,\n",
              "   84.68,\n",
              "   86.86,\n",
              "   85.73,\n",
              "   84.84,\n",
              "   84.46,\n",
              "   90.46,\n",
              "   90.74,\n",
              "   90.62,\n",
              "   90.88,\n",
              "   91.03,\n",
              "   90.94,\n",
              "   90.82,\n",
              "   90.76,\n",
              "   90.96,\n",
              "   90.95,\n",
              "   83.39,\n",
              "   84.83,\n",
              "   85.22,\n",
              "   84.36,\n",
              "   84.2,\n",
              "   85.99,\n",
              "   85.64,\n",
              "   85.06,\n",
              "   86.18,\n",
              "   85.07,\n",
              "   85.59,\n",
              "   85.3,\n",
              "   83.05,\n",
              "   83.72,\n",
              "   85.13,\n",
              "   85.88,\n",
              "   86.78,\n",
              "   84.97,\n",
              "   84.54,\n",
              "   85.23,\n",
              "   90.37,\n",
              "   90.71,\n",
              "   90.88,\n",
              "   90.79,\n",
              "   91.04,\n",
              "   90.69,\n",
              "   91.11,\n",
              "   90.89,\n",
              "   90.89,\n",
              "   90.87],\n",
              "  'training_loss': [0.35084924191869793,\n",
              "   0.33106836324334143,\n",
              "   0.3264572918057442,\n",
              "   0.32258193586170675,\n",
              "   0.30834390057921407,\n",
              "   0.3096859028726816,\n",
              "   0.3019952151685953,\n",
              "   0.3045683059573174,\n",
              "   0.2969441183447838,\n",
              "   0.29681066096127035,\n",
              "   0.29954650689661505,\n",
              "   0.2892873609080911,\n",
              "   0.29731301822960376,\n",
              "   0.2883898736402392,\n",
              "   0.28643721699416635,\n",
              "   0.2871512551739812,\n",
              "   0.2908075893908739,\n",
              "   0.2831595894038677,\n",
              "   0.27482943947315214,\n",
              "   0.27688033969402315,\n",
              "   0.1570197295330465,\n",
              "   0.11273897748738528,\n",
              "   0.09747598305270076,\n",
              "   0.0871900948897004,\n",
              "   0.08088943989910186,\n",
              "   0.07286589509807527,\n",
              "   0.06815798930078745,\n",
              "   0.06490975444931536,\n",
              "   0.05689116675425321,\n",
              "   0.05504231601404026,\n",
              "   0.3490093362249434,\n",
              "   0.3341561746329069,\n",
              "   0.3225847297281027,\n",
              "   0.3229009267747402,\n",
              "   0.31207209722101686,\n",
              "   0.3082287725716829,\n",
              "   0.30282032886892557,\n",
              "   0.3000041302204132,\n",
              "   0.297326535987854,\n",
              "   0.29553226284086703,\n",
              "   0.29052219577431676,\n",
              "   0.29814579522311685,\n",
              "   0.2911034922719002,\n",
              "   0.2860803555697203,\n",
              "   0.28511460112929343,\n",
              "   0.2889214350298047,\n",
              "   0.28548475922346117,\n",
              "   0.29005676914751527,\n",
              "   0.28315103187263013,\n",
              "   0.2783119087189436,\n",
              "   0.14971897754147648,\n",
              "   0.10876924117542804,\n",
              "   0.09644272176064551,\n",
              "   0.08653565613180399,\n",
              "   0.07774113832805306,\n",
              "   0.07490336100943387,\n",
              "   0.06530787081457674,\n",
              "   0.06330178949665279,\n",
              "   0.05504229255290702,\n",
              "   0.05482144209183753],\n",
              "  'valid_loss': [0.445020611679554,\n",
              "   0.43143266570568084,\n",
              "   0.3336475734829903,\n",
              "   0.39070509229898454,\n",
              "   0.37596815447807314,\n",
              "   0.3753319277882576,\n",
              "   0.39195153474807737,\n",
              "   0.3899280695796013,\n",
              "   0.4148231076717377,\n",
              "   0.4671101590037346,\n",
              "   0.37577719094753265,\n",
              "   0.44902252085208894,\n",
              "   0.48366415395736695,\n",
              "   0.38142940596342084,\n",
              "   0.4349351691126823,\n",
              "   0.4561902080774307,\n",
              "   0.374283144402504,\n",
              "   0.4381800763249397,\n",
              "   0.4702612398147583,\n",
              "   0.49551295857429506,\n",
              "   0.27742889786362646,\n",
              "   0.26843400350213054,\n",
              "   0.2728784598827362,\n",
              "   0.27477525190114976,\n",
              "   0.2835668076232076,\n",
              "   0.2755814109209925,\n",
              "   0.2766463681876659,\n",
              "   0.2857860713973641,\n",
              "   0.288745869487524,\n",
              "   0.28298514307141304,\n",
              "   0.37600775475502013,\n",
              "   0.39765059051513674,\n",
              "   0.38134266875982287,\n",
              "   0.42533464567661283,\n",
              "   0.4227616546392441,\n",
              "   0.37355665125846865,\n",
              "   0.3589714109659195,\n",
              "   0.42305661754608154,\n",
              "   0.3894899540185928,\n",
              "   0.43216359377503394,\n",
              "   0.41648665083646774,\n",
              "   0.41947494733929636,\n",
              "   0.47482827951908113,\n",
              "   0.5120783518075943,\n",
              "   0.4353105638861656,\n",
              "   0.4125898102760315,\n",
              "   0.3897325955629349,\n",
              "   0.44729194438457487,\n",
              "   0.44835713720321657,\n",
              "   0.4361024504184723,\n",
              "   0.2747956767976284,\n",
              "   0.2685240683019161,\n",
              "   0.2720258569866419,\n",
              "   0.26815159222483637,\n",
              "   0.264681710896641,\n",
              "   0.2755093263477087,\n",
              "   0.27389654819965364,\n",
              "   0.2693574653826654,\n",
              "   0.2791830378778279,\n",
              "   0.2693709916852415]},\n",
              " '0.6 features.34': {'nbr_param': [510.0, 6425247.0],\n",
              "  'test_acc': [81.45,\n",
              "   83.87,\n",
              "   86.15,\n",
              "   85.34,\n",
              "   85.9,\n",
              "   86.16,\n",
              "   85.47,\n",
              "   86.3,\n",
              "   85.49,\n",
              "   84.39,\n",
              "   86.94,\n",
              "   84.58,\n",
              "   83.76,\n",
              "   86.98,\n",
              "   85.56,\n",
              "   84.68,\n",
              "   86.86,\n",
              "   85.73,\n",
              "   84.84,\n",
              "   84.46,\n",
              "   90.46,\n",
              "   90.74,\n",
              "   90.62,\n",
              "   90.88,\n",
              "   91.03,\n",
              "   90.94,\n",
              "   90.82,\n",
              "   90.76,\n",
              "   90.96,\n",
              "   90.95,\n",
              "   83.39,\n",
              "   84.83,\n",
              "   85.22,\n",
              "   84.36,\n",
              "   84.2,\n",
              "   85.99,\n",
              "   85.64,\n",
              "   85.06,\n",
              "   86.18,\n",
              "   85.07,\n",
              "   85.59,\n",
              "   85.3,\n",
              "   83.05,\n",
              "   83.72,\n",
              "   85.13,\n",
              "   85.88,\n",
              "   86.78,\n",
              "   84.97,\n",
              "   84.54,\n",
              "   85.23,\n",
              "   90.37,\n",
              "   90.71,\n",
              "   90.88,\n",
              "   90.79,\n",
              "   91.04,\n",
              "   90.69,\n",
              "   91.11,\n",
              "   90.89,\n",
              "   90.89,\n",
              "   90.87],\n",
              "  'training_loss': [0.35084924191869793,\n",
              "   0.33106836324334143,\n",
              "   0.3264572918057442,\n",
              "   0.32258193586170675,\n",
              "   0.30834390057921407,\n",
              "   0.3096859028726816,\n",
              "   0.3019952151685953,\n",
              "   0.3045683059573174,\n",
              "   0.2969441183447838,\n",
              "   0.29681066096127035,\n",
              "   0.29954650689661505,\n",
              "   0.2892873609080911,\n",
              "   0.29731301822960376,\n",
              "   0.2883898736402392,\n",
              "   0.28643721699416635,\n",
              "   0.2871512551739812,\n",
              "   0.2908075893908739,\n",
              "   0.2831595894038677,\n",
              "   0.27482943947315214,\n",
              "   0.27688033969402315,\n",
              "   0.1570197295330465,\n",
              "   0.11273897748738528,\n",
              "   0.09747598305270076,\n",
              "   0.0871900948897004,\n",
              "   0.08088943989910186,\n",
              "   0.07286589509807527,\n",
              "   0.06815798930078745,\n",
              "   0.06490975444931536,\n",
              "   0.05689116675425321,\n",
              "   0.05504231601404026,\n",
              "   0.3490093362249434,\n",
              "   0.3341561746329069,\n",
              "   0.3225847297281027,\n",
              "   0.3229009267747402,\n",
              "   0.31207209722101686,\n",
              "   0.3082287725716829,\n",
              "   0.30282032886892557,\n",
              "   0.3000041302204132,\n",
              "   0.297326535987854,\n",
              "   0.29553226284086703,\n",
              "   0.29052219577431676,\n",
              "   0.29814579522311685,\n",
              "   0.2911034922719002,\n",
              "   0.2860803555697203,\n",
              "   0.28511460112929343,\n",
              "   0.2889214350298047,\n",
              "   0.28548475922346117,\n",
              "   0.29005676914751527,\n",
              "   0.28315103187263013,\n",
              "   0.2783119087189436,\n",
              "   0.14971897754147648,\n",
              "   0.10876924117542804,\n",
              "   0.09644272176064551,\n",
              "   0.08653565613180399,\n",
              "   0.07774113832805306,\n",
              "   0.07490336100943387,\n",
              "   0.06530787081457674,\n",
              "   0.06330178949665279,\n",
              "   0.05504229255290702,\n",
              "   0.05482144209183753],\n",
              "  'valid_loss': [0.445020611679554,\n",
              "   0.43143266570568084,\n",
              "   0.3336475734829903,\n",
              "   0.39070509229898454,\n",
              "   0.37596815447807314,\n",
              "   0.3753319277882576,\n",
              "   0.39195153474807737,\n",
              "   0.3899280695796013,\n",
              "   0.4148231076717377,\n",
              "   0.4671101590037346,\n",
              "   0.37577719094753265,\n",
              "   0.44902252085208894,\n",
              "   0.48366415395736695,\n",
              "   0.38142940596342084,\n",
              "   0.4349351691126823,\n",
              "   0.4561902080774307,\n",
              "   0.374283144402504,\n",
              "   0.4381800763249397,\n",
              "   0.4702612398147583,\n",
              "   0.49551295857429506,\n",
              "   0.27742889786362646,\n",
              "   0.26843400350213054,\n",
              "   0.2728784598827362,\n",
              "   0.27477525190114976,\n",
              "   0.2835668076232076,\n",
              "   0.2755814109209925,\n",
              "   0.2766463681876659,\n",
              "   0.2857860713973641,\n",
              "   0.288745869487524,\n",
              "   0.28298514307141304,\n",
              "   0.37600775475502013,\n",
              "   0.39765059051513674,\n",
              "   0.38134266875982287,\n",
              "   0.42533464567661283,\n",
              "   0.4227616546392441,\n",
              "   0.37355665125846865,\n",
              "   0.3589714109659195,\n",
              "   0.42305661754608154,\n",
              "   0.3894899540185928,\n",
              "   0.43216359377503394,\n",
              "   0.41648665083646774,\n",
              "   0.41947494733929636,\n",
              "   0.47482827951908113,\n",
              "   0.5120783518075943,\n",
              "   0.4353105638861656,\n",
              "   0.4125898102760315,\n",
              "   0.3897325955629349,\n",
              "   0.44729194438457487,\n",
              "   0.44835713720321657,\n",
              "   0.4361024504184723,\n",
              "   0.2747956767976284,\n",
              "   0.2685240683019161,\n",
              "   0.2720258569866419,\n",
              "   0.26815159222483637,\n",
              "   0.264681710896641,\n",
              "   0.2755093263477087,\n",
              "   0.27389654819965364,\n",
              "   0.2693574653826654,\n",
              "   0.2791830378778279,\n",
              "   0.2693709916852415]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW3JYLk5nsY7"
      },
      "source": [
        "### Prune sur \"features.37\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iFZCEh2QpqdY",
        "outputId": "efd78bf1-a3bc-4ff0-9996-d4eb36946e8a"
      },
      "source": [
        "prune_fine_tune(11,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.37\n",
            "features.37\n",
            "6809739.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "84.6  % ,  0.39301682991981507  ,  0.35343674695268273\n",
            "epoch  1\n",
            "saving weights.... \n",
            "86.78  % ,  0.3161373514652252  ,  0.3337914038449526\n",
            "epoch  2\n",
            "saving weights.... \n",
            "85.39  % ,  0.39213719725608825  ,  0.3252654498666525\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.95  % ,  0.37707878687381746  ,  0.3239388402611017\n",
            "epoch  4\n",
            "saving weights.... \n",
            "84.75  % ,  0.40358913493156434  ,  0.3091171863377094\n",
            "epoch  5\n",
            "saving weights.... \n",
            "84.37  % ,  0.446502927339077  ,  0.3079055591315031\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.27  % ,  0.40094965193271637  ,  0.30265917486846444\n",
            "epoch  7\n",
            "saving weights.... \n",
            "85.36  % ,  0.3972926742196083  ,  0.3009394941329956\n",
            "epoch  8\n",
            "saving weights.... \n",
            "86.16  % ,  0.39943355021476745  ,  0.2980901392430067\n",
            "epoch  9\n",
            "saving weights.... \n",
            "84.9  % ,  0.44570843710899355  ,  0.30237568849176166\n",
            "epoch  10\n",
            "saving weights.... \n",
            "86.11  % ,  0.41265030524730684  ,  0.2932415252715349\n",
            "epoch  11\n",
            "saving weights.... \n",
            "86.11  % ,  0.3954956883907318  ,  0.2921770704388619\n",
            "epoch  12\n",
            "saving weights.... \n",
            "86.44  % ,  0.39606044201850893  ,  0.2927383310556412\n",
            "epoch  13\n",
            "saving weights.... \n",
            "83.47  % ,  0.4882007834196091  ,  0.29115306247770784\n",
            "epoch  14\n",
            "saving weights.... \n",
            "84.73  % ,  0.45454874344468116  ,  0.2895364363715053\n",
            "epoch  15\n",
            "saving weights.... \n",
            "84.59  % ,  0.46786845004558564  ,  0.2886902376636863\n",
            "epoch  16\n",
            "saving weights.... \n",
            "85.8  % ,  0.40299729911088944  ,  0.2881756612852216\n",
            "epoch  17\n",
            "saving weights.... \n",
            "87.58  % ,  0.38746205883026125  ,  0.2859439132660627\n",
            "epoch  18\n",
            "saving weights.... \n",
            "85.14  % ,  0.4441994534254074  ,  0.2755860184654593\n",
            "epoch  19\n",
            "saving weights.... \n",
            "86.18  % ,  0.4348069209098816  ,  0.28417954752743246\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.33  % ,  0.274203447163105  ,  0.16052379014119506\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.59  % ,  0.2629163083910942  ,  0.11273329152613878\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.99  % ,  0.262554563139379  ,  0.09676752012558282\n",
            "epoch  3\n",
            "saving weights.... \n",
            "91.02  % ,  0.2643008347868919  ,  0.09003175244890153\n",
            "epoch  4\n",
            "saving weights.... \n",
            "91.2  % ,  0.2694631364941597  ,  0.07966110332384706\n",
            "epoch  5\n",
            "saving weights.... \n",
            "91.28  % ,  0.2651593653574586  ,  0.0732280059479177\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.96  % ,  0.26784609842002394  ,  0.06837274461966009\n",
            "epoch  7\n",
            "saving weights.... \n",
            "91.09  % ,  0.2613277198076248  ,  0.06388776531741024\n",
            "epoch  8\n",
            "saving weights.... \n",
            "91.02  % ,  0.30135913898050787  ,  0.05818172602578998\n",
            "epoch  9\n",
            "saving weights.... \n",
            "91.31  % ,  0.2924967523664236  ,  0.054275437531806525\n",
            "Finished Training\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.37\n",
            "features.37\n",
            "6425247.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "85.36  % ,  0.3545898533821106  ,  0.36295564119517804\n",
            "epoch  1\n",
            "saving weights.... \n",
            "85.19  % ,  0.3840061910390854  ,  0.34082623996436595\n",
            "epoch  2\n",
            "saving weights.... \n",
            "85.88  % ,  0.34267830995321275  ,  0.32982140679657457\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.78  % ,  0.3630928805828095  ,  0.3191536955565214\n",
            "epoch  4\n",
            "saving weights.... \n",
            "85.4  % ,  0.37955370533466337  ,  0.3134048237979412\n",
            "epoch  5\n",
            "saving weights.... \n",
            "86.52  % ,  0.33606634571552274  ,  0.3092199051529169\n",
            "epoch  6\n",
            "saving weights.... \n",
            "84.98  % ,  0.44334350703954695  ,  0.30672234110832214\n",
            "epoch  7\n",
            "saving weights.... \n",
            "86.39  % ,  0.3689177916407585  ,  0.303634811770916\n",
            "epoch  8\n",
            "saving weights.... \n",
            "85.24  % ,  0.4257809668302536  ,  0.2993334087818861\n",
            "epoch  9\n",
            "saving weights.... \n",
            "85.33  % ,  0.42625437548160555  ,  0.29577902571707965\n",
            "epoch  10\n",
            "saving weights.... \n",
            "85.92  % ,  0.4034199438333511  ,  0.2960544077605009\n",
            "epoch  11\n",
            "saving weights.... \n",
            "84.45  % ,  0.46403091373443606  ,  0.29373334351927044\n",
            "epoch  12\n",
            "saving weights.... \n",
            "85.22  % ,  0.43054973328113555  ,  0.28747491617798804\n",
            "epoch  13\n",
            "saving weights.... \n",
            "85.41  % ,  0.4183733963131905  ,  0.2896588575989008\n",
            "epoch  14\n",
            "saving weights.... \n",
            "85.78  % ,  0.41321967861652376  ,  0.2853454539358616\n",
            "epoch  15\n",
            "saving weights.... \n",
            "84.66  % ,  0.4583630894899368  ,  0.2792084229826927\n",
            "epoch  16\n",
            "saving weights.... \n",
            "84.44  % ,  0.4728661587715149  ,  0.2857408996373415\n",
            "epoch  17\n",
            "saving weights.... \n",
            "86.38  % ,  0.40966051380634305  ,  0.2790056697443128\n",
            "epoch  18\n",
            "saving weights.... \n",
            "85.59  % ,  0.43310665038228036  ,  0.2799019447401166\n",
            "epoch  19\n",
            "saving weights.... \n",
            "85.6  % ,  0.4166913940548897  ,  0.27800903359651563\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.55  % ,  0.28051051164269447  ,  0.15863063304126263\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.9  % ,  0.2742628143578768  ,  0.1155070024497807\n",
            "epoch  2\n",
            "saving weights.... \n",
            "91.11  % ,  0.2684525733411312  ,  0.09722974300570786\n",
            "epoch  3\n",
            "saving weights.... \n",
            "91.28  % ,  0.2741292292892933  ,  0.08993825114965438\n",
            "epoch  4\n",
            "saving weights.... \n",
            "91.21  % ,  0.2826760642796755  ,  0.08214524556156248\n",
            "epoch  5\n",
            "saving weights.... \n",
            "91.45  % ,  0.2843408197283745  ,  0.07159718549009413\n",
            "epoch  6\n",
            "saving weights.... \n",
            "91.24  % ,  0.27347555152773856  ,  0.06737418829258532\n",
            "epoch  7\n",
            "saving weights.... \n",
            "91.32  % ,  0.2839307977795601  ,  0.0630772864177823\n",
            "epoch  8\n",
            "saving weights.... \n",
            "91.42  % ,  0.2899189811892807  ,  0.05784286631122231\n",
            "epoch  9\n",
            "saving weights.... \n",
            "91.12  % ,  0.2872998674452305  ,  0.055296975947450845\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_97ad7345-7d25-479d-a20b-9b2a3a9ffbcf\", \"features.37.npy\", 435)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.3 features.37': {'nbr_param': [510.0, 6809739.0],\n",
              "  'test_acc': [81.45,\n",
              "   83.87,\n",
              "   86.15,\n",
              "   85.34,\n",
              "   85.9,\n",
              "   86.16,\n",
              "   85.47,\n",
              "   86.3,\n",
              "   85.49,\n",
              "   84.39,\n",
              "   86.94,\n",
              "   84.58,\n",
              "   83.76,\n",
              "   86.98,\n",
              "   85.56,\n",
              "   84.68,\n",
              "   86.86,\n",
              "   85.73,\n",
              "   84.84,\n",
              "   84.46,\n",
              "   90.46,\n",
              "   90.74,\n",
              "   90.62,\n",
              "   90.88,\n",
              "   91.03,\n",
              "   90.94,\n",
              "   90.82,\n",
              "   90.76,\n",
              "   90.96,\n",
              "   90.95,\n",
              "   83.39,\n",
              "   84.83,\n",
              "   85.22,\n",
              "   84.36,\n",
              "   84.2,\n",
              "   85.99,\n",
              "   85.64,\n",
              "   85.06,\n",
              "   86.18,\n",
              "   85.07,\n",
              "   85.59,\n",
              "   85.3,\n",
              "   83.05,\n",
              "   83.72,\n",
              "   85.13,\n",
              "   85.88,\n",
              "   86.78,\n",
              "   84.97,\n",
              "   84.54,\n",
              "   85.23,\n",
              "   90.37,\n",
              "   90.71,\n",
              "   90.88,\n",
              "   90.79,\n",
              "   91.04,\n",
              "   90.69,\n",
              "   91.11,\n",
              "   90.89,\n",
              "   90.89,\n",
              "   90.87,\n",
              "   84.6,\n",
              "   86.78,\n",
              "   85.39,\n",
              "   85.95,\n",
              "   84.75,\n",
              "   84.37,\n",
              "   85.27,\n",
              "   85.36,\n",
              "   86.16,\n",
              "   84.9,\n",
              "   86.11,\n",
              "   86.11,\n",
              "   86.44,\n",
              "   83.47,\n",
              "   84.73,\n",
              "   84.59,\n",
              "   85.8,\n",
              "   87.58,\n",
              "   85.14,\n",
              "   86.18,\n",
              "   90.33,\n",
              "   90.59,\n",
              "   90.99,\n",
              "   91.02,\n",
              "   91.2,\n",
              "   91.28,\n",
              "   90.96,\n",
              "   91.09,\n",
              "   91.02,\n",
              "   91.31,\n",
              "   85.36,\n",
              "   85.19,\n",
              "   85.88,\n",
              "   85.78,\n",
              "   85.4,\n",
              "   86.52,\n",
              "   84.98,\n",
              "   86.39,\n",
              "   85.24,\n",
              "   85.33,\n",
              "   85.92,\n",
              "   84.45,\n",
              "   85.22,\n",
              "   85.41,\n",
              "   85.78,\n",
              "   84.66,\n",
              "   84.44,\n",
              "   86.38,\n",
              "   85.59,\n",
              "   85.6,\n",
              "   90.55,\n",
              "   90.9,\n",
              "   91.11,\n",
              "   91.28,\n",
              "   91.21,\n",
              "   91.45,\n",
              "   91.24,\n",
              "   91.32,\n",
              "   91.42,\n",
              "   91.12],\n",
              "  'training_loss': [0.35084924191869793,\n",
              "   0.33106836324334143,\n",
              "   0.3264572918057442,\n",
              "   0.32258193586170675,\n",
              "   0.30834390057921407,\n",
              "   0.3096859028726816,\n",
              "   0.3019952151685953,\n",
              "   0.3045683059573174,\n",
              "   0.2969441183447838,\n",
              "   0.29681066096127035,\n",
              "   0.29954650689661505,\n",
              "   0.2892873609080911,\n",
              "   0.29731301822960376,\n",
              "   0.2883898736402392,\n",
              "   0.28643721699416635,\n",
              "   0.2871512551739812,\n",
              "   0.2908075893908739,\n",
              "   0.2831595894038677,\n",
              "   0.27482943947315214,\n",
              "   0.27688033969402315,\n",
              "   0.1570197295330465,\n",
              "   0.11273897748738528,\n",
              "   0.09747598305270076,\n",
              "   0.0871900948897004,\n",
              "   0.08088943989910186,\n",
              "   0.07286589509807527,\n",
              "   0.06815798930078745,\n",
              "   0.06490975444931536,\n",
              "   0.05689116675425321,\n",
              "   0.05504231601404026,\n",
              "   0.3490093362249434,\n",
              "   0.3341561746329069,\n",
              "   0.3225847297281027,\n",
              "   0.3229009267747402,\n",
              "   0.31207209722101686,\n",
              "   0.3082287725716829,\n",
              "   0.30282032886892557,\n",
              "   0.3000041302204132,\n",
              "   0.297326535987854,\n",
              "   0.29553226284086703,\n",
              "   0.29052219577431676,\n",
              "   0.29814579522311685,\n",
              "   0.2911034922719002,\n",
              "   0.2860803555697203,\n",
              "   0.28511460112929343,\n",
              "   0.2889214350298047,\n",
              "   0.28548475922346117,\n",
              "   0.29005676914751527,\n",
              "   0.28315103187263013,\n",
              "   0.2783119087189436,\n",
              "   0.14971897754147648,\n",
              "   0.10876924117542804,\n",
              "   0.09644272176064551,\n",
              "   0.08653565613180399,\n",
              "   0.07774113832805306,\n",
              "   0.07490336100943387,\n",
              "   0.06530787081457674,\n",
              "   0.06330178949665279,\n",
              "   0.05504229255290702,\n",
              "   0.05482144209183753,\n",
              "   0.35343674695268273,\n",
              "   0.3337914038449526,\n",
              "   0.3252654498666525,\n",
              "   0.3239388402611017,\n",
              "   0.3091171863377094,\n",
              "   0.3079055591315031,\n",
              "   0.30265917486846444,\n",
              "   0.3009394941329956,\n",
              "   0.2980901392430067,\n",
              "   0.30237568849176166,\n",
              "   0.2932415252715349,\n",
              "   0.2921770704388619,\n",
              "   0.2927383310556412,\n",
              "   0.29115306247770784,\n",
              "   0.2895364363715053,\n",
              "   0.2886902376636863,\n",
              "   0.2881756612852216,\n",
              "   0.2859439132660627,\n",
              "   0.2755860184654593,\n",
              "   0.28417954752743246,\n",
              "   0.16052379014119506,\n",
              "   0.11273329152613878,\n",
              "   0.09676752012558282,\n",
              "   0.09003175244890153,\n",
              "   0.07966110332384706,\n",
              "   0.0732280059479177,\n",
              "   0.06837274461966009,\n",
              "   0.06388776531741024,\n",
              "   0.05818172602578998,\n",
              "   0.054275437531806525,\n",
              "   0.36295564119517804,\n",
              "   0.34082623996436595,\n",
              "   0.32982140679657457,\n",
              "   0.3191536955565214,\n",
              "   0.3134048237979412,\n",
              "   0.3092199051529169,\n",
              "   0.30672234110832214,\n",
              "   0.303634811770916,\n",
              "   0.2993334087818861,\n",
              "   0.29577902571707965,\n",
              "   0.2960544077605009,\n",
              "   0.29373334351927044,\n",
              "   0.28747491617798804,\n",
              "   0.2896588575989008,\n",
              "   0.2853454539358616,\n",
              "   0.2792084229826927,\n",
              "   0.2857408996373415,\n",
              "   0.2790056697443128,\n",
              "   0.2799019447401166,\n",
              "   0.27800903359651563,\n",
              "   0.15863063304126263,\n",
              "   0.1155070024497807,\n",
              "   0.09722974300570786,\n",
              "   0.08993825114965438,\n",
              "   0.08214524556156248,\n",
              "   0.07159718549009413,\n",
              "   0.06737418829258532,\n",
              "   0.0630772864177823,\n",
              "   0.05784286631122231,\n",
              "   0.055296975947450845],\n",
              "  'valid_loss': [0.445020611679554,\n",
              "   0.43143266570568084,\n",
              "   0.3336475734829903,\n",
              "   0.39070509229898454,\n",
              "   0.37596815447807314,\n",
              "   0.3753319277882576,\n",
              "   0.39195153474807737,\n",
              "   0.3899280695796013,\n",
              "   0.4148231076717377,\n",
              "   0.4671101590037346,\n",
              "   0.37577719094753265,\n",
              "   0.44902252085208894,\n",
              "   0.48366415395736695,\n",
              "   0.38142940596342084,\n",
              "   0.4349351691126823,\n",
              "   0.4561902080774307,\n",
              "   0.374283144402504,\n",
              "   0.4381800763249397,\n",
              "   0.4702612398147583,\n",
              "   0.49551295857429506,\n",
              "   0.27742889786362646,\n",
              "   0.26843400350213054,\n",
              "   0.2728784598827362,\n",
              "   0.27477525190114976,\n",
              "   0.2835668076232076,\n",
              "   0.2755814109209925,\n",
              "   0.2766463681876659,\n",
              "   0.2857860713973641,\n",
              "   0.288745869487524,\n",
              "   0.28298514307141304,\n",
              "   0.37600775475502013,\n",
              "   0.39765059051513674,\n",
              "   0.38134266875982287,\n",
              "   0.42533464567661283,\n",
              "   0.4227616546392441,\n",
              "   0.37355665125846865,\n",
              "   0.3589714109659195,\n",
              "   0.42305661754608154,\n",
              "   0.3894899540185928,\n",
              "   0.43216359377503394,\n",
              "   0.41648665083646774,\n",
              "   0.41947494733929636,\n",
              "   0.47482827951908113,\n",
              "   0.5120783518075943,\n",
              "   0.4353105638861656,\n",
              "   0.4125898102760315,\n",
              "   0.3897325955629349,\n",
              "   0.44729194438457487,\n",
              "   0.44835713720321657,\n",
              "   0.4361024504184723,\n",
              "   0.2747956767976284,\n",
              "   0.2685240683019161,\n",
              "   0.2720258569866419,\n",
              "   0.26815159222483637,\n",
              "   0.264681710896641,\n",
              "   0.2755093263477087,\n",
              "   0.27389654819965364,\n",
              "   0.2693574653826654,\n",
              "   0.2791830378778279,\n",
              "   0.2693709916852415,\n",
              "   0.39301682991981507,\n",
              "   0.3161373514652252,\n",
              "   0.39213719725608825,\n",
              "   0.37707878687381746,\n",
              "   0.40358913493156434,\n",
              "   0.446502927339077,\n",
              "   0.40094965193271637,\n",
              "   0.3972926742196083,\n",
              "   0.39943355021476745,\n",
              "   0.44570843710899355,\n",
              "   0.41265030524730684,\n",
              "   0.3954956883907318,\n",
              "   0.39606044201850893,\n",
              "   0.4882007834196091,\n",
              "   0.45454874344468116,\n",
              "   0.46786845004558564,\n",
              "   0.40299729911088944,\n",
              "   0.38746205883026125,\n",
              "   0.4441994534254074,\n",
              "   0.4348069209098816,\n",
              "   0.274203447163105,\n",
              "   0.2629163083910942,\n",
              "   0.262554563139379,\n",
              "   0.2643008347868919,\n",
              "   0.2694631364941597,\n",
              "   0.2651593653574586,\n",
              "   0.26784609842002394,\n",
              "   0.2613277198076248,\n",
              "   0.30135913898050787,\n",
              "   0.2924967523664236,\n",
              "   0.3545898533821106,\n",
              "   0.3840061910390854,\n",
              "   0.34267830995321275,\n",
              "   0.3630928805828095,\n",
              "   0.37955370533466337,\n",
              "   0.33606634571552274,\n",
              "   0.44334350703954695,\n",
              "   0.3689177916407585,\n",
              "   0.4257809668302536,\n",
              "   0.42625437548160555,\n",
              "   0.4034199438333511,\n",
              "   0.46403091373443606,\n",
              "   0.43054973328113555,\n",
              "   0.4183733963131905,\n",
              "   0.41321967861652376,\n",
              "   0.4583630894899368,\n",
              "   0.4728661587715149,\n",
              "   0.40966051380634305,\n",
              "   0.43310665038228036,\n",
              "   0.4166913940548897,\n",
              "   0.28051051164269447,\n",
              "   0.2742628143578768,\n",
              "   0.2684525733411312,\n",
              "   0.2741292292892933,\n",
              "   0.2826760642796755,\n",
              "   0.2843408197283745,\n",
              "   0.27347555152773856,\n",
              "   0.2839307977795601,\n",
              "   0.2899189811892807,\n",
              "   0.2872998674452305]},\n",
              " '0.6 features.37': {'nbr_param': [510.0, 6425247.0],\n",
              "  'test_acc': [81.45,\n",
              "   83.87,\n",
              "   86.15,\n",
              "   85.34,\n",
              "   85.9,\n",
              "   86.16,\n",
              "   85.47,\n",
              "   86.3,\n",
              "   85.49,\n",
              "   84.39,\n",
              "   86.94,\n",
              "   84.58,\n",
              "   83.76,\n",
              "   86.98,\n",
              "   85.56,\n",
              "   84.68,\n",
              "   86.86,\n",
              "   85.73,\n",
              "   84.84,\n",
              "   84.46,\n",
              "   90.46,\n",
              "   90.74,\n",
              "   90.62,\n",
              "   90.88,\n",
              "   91.03,\n",
              "   90.94,\n",
              "   90.82,\n",
              "   90.76,\n",
              "   90.96,\n",
              "   90.95,\n",
              "   83.39,\n",
              "   84.83,\n",
              "   85.22,\n",
              "   84.36,\n",
              "   84.2,\n",
              "   85.99,\n",
              "   85.64,\n",
              "   85.06,\n",
              "   86.18,\n",
              "   85.07,\n",
              "   85.59,\n",
              "   85.3,\n",
              "   83.05,\n",
              "   83.72,\n",
              "   85.13,\n",
              "   85.88,\n",
              "   86.78,\n",
              "   84.97,\n",
              "   84.54,\n",
              "   85.23,\n",
              "   90.37,\n",
              "   90.71,\n",
              "   90.88,\n",
              "   90.79,\n",
              "   91.04,\n",
              "   90.69,\n",
              "   91.11,\n",
              "   90.89,\n",
              "   90.89,\n",
              "   90.87,\n",
              "   84.6,\n",
              "   86.78,\n",
              "   85.39,\n",
              "   85.95,\n",
              "   84.75,\n",
              "   84.37,\n",
              "   85.27,\n",
              "   85.36,\n",
              "   86.16,\n",
              "   84.9,\n",
              "   86.11,\n",
              "   86.11,\n",
              "   86.44,\n",
              "   83.47,\n",
              "   84.73,\n",
              "   84.59,\n",
              "   85.8,\n",
              "   87.58,\n",
              "   85.14,\n",
              "   86.18,\n",
              "   90.33,\n",
              "   90.59,\n",
              "   90.99,\n",
              "   91.02,\n",
              "   91.2,\n",
              "   91.28,\n",
              "   90.96,\n",
              "   91.09,\n",
              "   91.02,\n",
              "   91.31,\n",
              "   85.36,\n",
              "   85.19,\n",
              "   85.88,\n",
              "   85.78,\n",
              "   85.4,\n",
              "   86.52,\n",
              "   84.98,\n",
              "   86.39,\n",
              "   85.24,\n",
              "   85.33,\n",
              "   85.92,\n",
              "   84.45,\n",
              "   85.22,\n",
              "   85.41,\n",
              "   85.78,\n",
              "   84.66,\n",
              "   84.44,\n",
              "   86.38,\n",
              "   85.59,\n",
              "   85.6,\n",
              "   90.55,\n",
              "   90.9,\n",
              "   91.11,\n",
              "   91.28,\n",
              "   91.21,\n",
              "   91.45,\n",
              "   91.24,\n",
              "   91.32,\n",
              "   91.42,\n",
              "   91.12],\n",
              "  'training_loss': [0.35084924191869793,\n",
              "   0.33106836324334143,\n",
              "   0.3264572918057442,\n",
              "   0.32258193586170675,\n",
              "   0.30834390057921407,\n",
              "   0.3096859028726816,\n",
              "   0.3019952151685953,\n",
              "   0.3045683059573174,\n",
              "   0.2969441183447838,\n",
              "   0.29681066096127035,\n",
              "   0.29954650689661505,\n",
              "   0.2892873609080911,\n",
              "   0.29731301822960376,\n",
              "   0.2883898736402392,\n",
              "   0.28643721699416635,\n",
              "   0.2871512551739812,\n",
              "   0.2908075893908739,\n",
              "   0.2831595894038677,\n",
              "   0.27482943947315214,\n",
              "   0.27688033969402315,\n",
              "   0.1570197295330465,\n",
              "   0.11273897748738528,\n",
              "   0.09747598305270076,\n",
              "   0.0871900948897004,\n",
              "   0.08088943989910186,\n",
              "   0.07286589509807527,\n",
              "   0.06815798930078745,\n",
              "   0.06490975444931536,\n",
              "   0.05689116675425321,\n",
              "   0.05504231601404026,\n",
              "   0.3490093362249434,\n",
              "   0.3341561746329069,\n",
              "   0.3225847297281027,\n",
              "   0.3229009267747402,\n",
              "   0.31207209722101686,\n",
              "   0.3082287725716829,\n",
              "   0.30282032886892557,\n",
              "   0.3000041302204132,\n",
              "   0.297326535987854,\n",
              "   0.29553226284086703,\n",
              "   0.29052219577431676,\n",
              "   0.29814579522311685,\n",
              "   0.2911034922719002,\n",
              "   0.2860803555697203,\n",
              "   0.28511460112929343,\n",
              "   0.2889214350298047,\n",
              "   0.28548475922346117,\n",
              "   0.29005676914751527,\n",
              "   0.28315103187263013,\n",
              "   0.2783119087189436,\n",
              "   0.14971897754147648,\n",
              "   0.10876924117542804,\n",
              "   0.09644272176064551,\n",
              "   0.08653565613180399,\n",
              "   0.07774113832805306,\n",
              "   0.07490336100943387,\n",
              "   0.06530787081457674,\n",
              "   0.06330178949665279,\n",
              "   0.05504229255290702,\n",
              "   0.05482144209183753,\n",
              "   0.35343674695268273,\n",
              "   0.3337914038449526,\n",
              "   0.3252654498666525,\n",
              "   0.3239388402611017,\n",
              "   0.3091171863377094,\n",
              "   0.3079055591315031,\n",
              "   0.30265917486846444,\n",
              "   0.3009394941329956,\n",
              "   0.2980901392430067,\n",
              "   0.30237568849176166,\n",
              "   0.2932415252715349,\n",
              "   0.2921770704388619,\n",
              "   0.2927383310556412,\n",
              "   0.29115306247770784,\n",
              "   0.2895364363715053,\n",
              "   0.2886902376636863,\n",
              "   0.2881756612852216,\n",
              "   0.2859439132660627,\n",
              "   0.2755860184654593,\n",
              "   0.28417954752743246,\n",
              "   0.16052379014119506,\n",
              "   0.11273329152613878,\n",
              "   0.09676752012558282,\n",
              "   0.09003175244890153,\n",
              "   0.07966110332384706,\n",
              "   0.0732280059479177,\n",
              "   0.06837274461966009,\n",
              "   0.06388776531741024,\n",
              "   0.05818172602578998,\n",
              "   0.054275437531806525,\n",
              "   0.36295564119517804,\n",
              "   0.34082623996436595,\n",
              "   0.32982140679657457,\n",
              "   0.3191536955565214,\n",
              "   0.3134048237979412,\n",
              "   0.3092199051529169,\n",
              "   0.30672234110832214,\n",
              "   0.303634811770916,\n",
              "   0.2993334087818861,\n",
              "   0.29577902571707965,\n",
              "   0.2960544077605009,\n",
              "   0.29373334351927044,\n",
              "   0.28747491617798804,\n",
              "   0.2896588575989008,\n",
              "   0.2853454539358616,\n",
              "   0.2792084229826927,\n",
              "   0.2857408996373415,\n",
              "   0.2790056697443128,\n",
              "   0.2799019447401166,\n",
              "   0.27800903359651563,\n",
              "   0.15863063304126263,\n",
              "   0.1155070024497807,\n",
              "   0.09722974300570786,\n",
              "   0.08993825114965438,\n",
              "   0.08214524556156248,\n",
              "   0.07159718549009413,\n",
              "   0.06737418829258532,\n",
              "   0.0630772864177823,\n",
              "   0.05784286631122231,\n",
              "   0.055296975947450845],\n",
              "  'valid_loss': [0.445020611679554,\n",
              "   0.43143266570568084,\n",
              "   0.3336475734829903,\n",
              "   0.39070509229898454,\n",
              "   0.37596815447807314,\n",
              "   0.3753319277882576,\n",
              "   0.39195153474807737,\n",
              "   0.3899280695796013,\n",
              "   0.4148231076717377,\n",
              "   0.4671101590037346,\n",
              "   0.37577719094753265,\n",
              "   0.44902252085208894,\n",
              "   0.48366415395736695,\n",
              "   0.38142940596342084,\n",
              "   0.4349351691126823,\n",
              "   0.4561902080774307,\n",
              "   0.374283144402504,\n",
              "   0.4381800763249397,\n",
              "   0.4702612398147583,\n",
              "   0.49551295857429506,\n",
              "   0.27742889786362646,\n",
              "   0.26843400350213054,\n",
              "   0.2728784598827362,\n",
              "   0.27477525190114976,\n",
              "   0.2835668076232076,\n",
              "   0.2755814109209925,\n",
              "   0.2766463681876659,\n",
              "   0.2857860713973641,\n",
              "   0.288745869487524,\n",
              "   0.28298514307141304,\n",
              "   0.37600775475502013,\n",
              "   0.39765059051513674,\n",
              "   0.38134266875982287,\n",
              "   0.42533464567661283,\n",
              "   0.4227616546392441,\n",
              "   0.37355665125846865,\n",
              "   0.3589714109659195,\n",
              "   0.42305661754608154,\n",
              "   0.3894899540185928,\n",
              "   0.43216359377503394,\n",
              "   0.41648665083646774,\n",
              "   0.41947494733929636,\n",
              "   0.47482827951908113,\n",
              "   0.5120783518075943,\n",
              "   0.4353105638861656,\n",
              "   0.4125898102760315,\n",
              "   0.3897325955629349,\n",
              "   0.44729194438457487,\n",
              "   0.44835713720321657,\n",
              "   0.4361024504184723,\n",
              "   0.2747956767976284,\n",
              "   0.2685240683019161,\n",
              "   0.2720258569866419,\n",
              "   0.26815159222483637,\n",
              "   0.264681710896641,\n",
              "   0.2755093263477087,\n",
              "   0.27389654819965364,\n",
              "   0.2693574653826654,\n",
              "   0.2791830378778279,\n",
              "   0.2693709916852415,\n",
              "   0.39301682991981507,\n",
              "   0.3161373514652252,\n",
              "   0.39213719725608825,\n",
              "   0.37707878687381746,\n",
              "   0.40358913493156434,\n",
              "   0.446502927339077,\n",
              "   0.40094965193271637,\n",
              "   0.3972926742196083,\n",
              "   0.39943355021476745,\n",
              "   0.44570843710899355,\n",
              "   0.41265030524730684,\n",
              "   0.3954956883907318,\n",
              "   0.39606044201850893,\n",
              "   0.4882007834196091,\n",
              "   0.45454874344468116,\n",
              "   0.46786845004558564,\n",
              "   0.40299729911088944,\n",
              "   0.38746205883026125,\n",
              "   0.4441994534254074,\n",
              "   0.4348069209098816,\n",
              "   0.274203447163105,\n",
              "   0.2629163083910942,\n",
              "   0.262554563139379,\n",
              "   0.2643008347868919,\n",
              "   0.2694631364941597,\n",
              "   0.2651593653574586,\n",
              "   0.26784609842002394,\n",
              "   0.2613277198076248,\n",
              "   0.30135913898050787,\n",
              "   0.2924967523664236,\n",
              "   0.3545898533821106,\n",
              "   0.3840061910390854,\n",
              "   0.34267830995321275,\n",
              "   0.3630928805828095,\n",
              "   0.37955370533466337,\n",
              "   0.33606634571552274,\n",
              "   0.44334350703954695,\n",
              "   0.3689177916407585,\n",
              "   0.4257809668302536,\n",
              "   0.42625437548160555,\n",
              "   0.4034199438333511,\n",
              "   0.46403091373443606,\n",
              "   0.43054973328113555,\n",
              "   0.4183733963131905,\n",
              "   0.41321967861652376,\n",
              "   0.4583630894899368,\n",
              "   0.4728661587715149,\n",
              "   0.40966051380634305,\n",
              "   0.43310665038228036,\n",
              "   0.4166913940548897,\n",
              "   0.28051051164269447,\n",
              "   0.2742628143578768,\n",
              "   0.2684525733411312,\n",
              "   0.2741292292892933,\n",
              "   0.2826760642796755,\n",
              "   0.2843408197283745,\n",
              "   0.27347555152773856,\n",
              "   0.2839307977795601,\n",
              "   0.2899189811892807,\n",
              "   0.2872998674452305]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJKyrAeCnscj"
      },
      "source": [
        "### Prune sur \"features.40\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z4JbLkH-l5IG",
        "outputId": "74700694-0d96-4ede-d500-37dc58ca2b74"
      },
      "source": [
        "prune_fine_tune(12,[0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.40\n",
            "features.40\n",
            "6809739.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "83.74  % ,  0.3652986422300339  ,  0.363784453356266\n",
            "epoch  1\n",
            "saving weights.... \n",
            "85.42  % ,  0.3625009913682938  ,  0.3371664649128914\n",
            "epoch  2\n",
            "saving weights.... \n",
            "86.87  % ,  0.334758321750164  ,  0.3273284856915474\n",
            "epoch  3\n",
            "saving weights.... \n",
            "86.24  % ,  0.3796747834444046  ,  0.3164309662848711\n",
            "epoch  4\n",
            "saving weights.... \n",
            "84.29  % ,  0.4028608358621597  ,  0.31195530293881896\n",
            "epoch  5\n",
            "saving weights.... \n",
            "85.03  % ,  0.39987493793964385  ,  0.30701902668476105\n",
            "epoch  6\n",
            "saving weights.... \n",
            "86.11  % ,  0.38616906170845033  ,  0.30659070705473424\n",
            "epoch  7\n",
            "saving weights.... \n",
            "86.22  % ,  0.36436377462148667  ,  0.3011259002685547\n",
            "epoch  8\n",
            "saving weights.... \n",
            "86.58  % ,  0.3698863430172205  ,  0.30108107025027275\n",
            "epoch  9\n",
            "saving weights.... \n",
            "85.56  % ,  0.38048521510362626  ,  0.29597196120619773\n",
            "epoch  10\n",
            "saving weights.... \n",
            "85.27  % ,  0.3995002091526985  ,  0.2935586633861065\n",
            "epoch  11\n",
            "saving weights.... \n",
            "85.16  % ,  0.43879242265224455  ,  0.2929565840899944\n",
            "epoch  12\n",
            "saving weights.... \n",
            "84.6  % ,  0.4572175764679909  ,  0.2986148417592049\n",
            "epoch  13\n",
            "saving weights.... \n",
            "86.57  % ,  0.4018332665801048  ,  0.2877674796313047\n",
            "epoch  14\n",
            "saving weights.... \n",
            "86.97  % ,  0.3661221534848213  ,  0.2920510015428066\n",
            "epoch  15\n",
            "saving weights.... \n",
            "86.57  % ,  0.37979006061553955  ,  0.28821502611786126\n",
            "epoch  16\n",
            "saving weights.... \n",
            "86.42  % ,  0.4177395811319351  ,  0.2802938088312745\n",
            "epoch  17\n",
            "saving weights.... \n",
            "84.5  % ,  0.5004160271406174  ,  0.2826227265834808\n",
            "epoch  18\n",
            "saving weights.... \n",
            "86.07  % ,  0.4203548230648041  ,  0.2838340441420674\n",
            "epoch  19\n",
            "saving weights.... \n",
            "87.2  % ,  0.4090287123799324  ,  0.2849051160275936\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.28  % ,  0.2832505022674799  ,  0.15440826106145977\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.52  % ,  0.2651658486574888  ,  0.1127880785215646\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.85  % ,  0.2661430995106697  ,  0.10159137364998459\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.8  % ,  0.27634272849857805  ,  0.08678633862100542\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.86  % ,  0.26659162866473196  ,  0.08168033768981695\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.73  % ,  0.27456233023703097  ,  0.07342577430456877\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.7  % ,  0.26980633396357295  ,  0.06915799516290426\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.75  % ,  0.28606447594016793  ,  0.0605387993209064\n",
            "epoch  8\n",
            "saving weights.... \n",
            "91.04  % ,  0.28653450000435116  ,  0.05912152198003605\n",
            "epoch  9\n",
            "saving weights.... \n",
            "91.15  % ,  0.2885183928489685  ,  0.05965982687594369\n",
            "Finished Training\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "changing module :  features.40\n",
            "features.40\n",
            "6425247.0 510.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "85.16  % ,  0.3603210410833359  ,  0.3717221769273281\n",
            "epoch  1\n",
            "saving weights.... \n",
            "83.31  % ,  0.43097810161113737  ,  0.342645852547884\n",
            "epoch  2\n",
            "saving weights.... \n",
            "85.63  % ,  0.3614657994508743  ,  0.3308918531358242\n",
            "epoch  3\n",
            "saving weights.... \n",
            "84.78  % ,  0.3784205896615982  ,  0.32479703027904033\n",
            "epoch  4\n",
            "saving weights.... \n",
            "86.78  % ,  0.3495366342186928  ,  0.32218738459050655\n",
            "epoch  5\n",
            "saving weights.... \n",
            "86.44  % ,  0.3731925350189209  ,  0.31108728597164154\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.98  % ,  0.36887834451198576  ,  0.30706054975390434\n",
            "epoch  7\n",
            "saving weights.... \n",
            "87.02  % ,  0.3504226750135422  ,  0.3057656193599105\n",
            "epoch  8\n",
            "saving weights.... \n",
            "85.58  % ,  0.40596330823898313  ,  0.3050827283501625\n",
            "epoch  9\n",
            "saving weights.... \n",
            "84.78  % ,  0.43222376096248627  ,  0.3001355744123459\n",
            "epoch  10\n",
            "saving weights.... \n",
            "84.45  % ,  0.45111742639541624  ,  0.29309019438624384\n",
            "epoch  11\n",
            "saving weights.... \n",
            "83.83  % ,  0.475445584487915  ,  0.2936345613539219\n",
            "epoch  12\n",
            "saving weights.... \n",
            "87.12  % ,  0.36773753712177276  ,  0.29521779091060163\n",
            "epoch  13\n",
            "saving weights.... \n",
            "86.83  % ,  0.3910072116613388  ,  0.29319412536025047\n",
            "epoch  14\n",
            "saving weights.... \n",
            "85.8  % ,  0.40461605751514435  ,  0.2843851218700409\n",
            "epoch  15\n",
            "saving weights.... \n",
            "83.97  % ,  0.484806388759613  ,  0.2883792933613062\n",
            "epoch  16\n",
            "saving weights.... \n",
            "85.54  % ,  0.4231666206598282  ,  0.28178793233186006\n",
            "epoch  17\n",
            "saving weights.... \n",
            "85.82  % ,  0.4379344573378563  ,  0.28494139921963213\n",
            "epoch  18\n",
            "saving weights.... \n",
            "85.73  % ,  0.42086072874069214  ,  0.28002016406059266\n",
            "epoch  19\n",
            "saving weights.... \n",
            "84.97  % ,  0.4403946658372879  ,  0.2837989174813032\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "90.09  % ,  0.2683191631197929  ,  0.15235383375287057\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.82  % ,  0.2728955974191427  ,  0.11242449519783258\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.81  % ,  0.2713778224736452  ,  0.10297380566485226\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.98  % ,  0.2636676842093468  ,  0.08519371609278023\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.97  % ,  0.2731557501733303  ,  0.07809004698283971\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.99  % ,  0.270313446174562  ,  0.07317064554952085\n",
            "epoch  6\n",
            "saving weights.... \n",
            "91.36  % ,  0.2738521011829376  ,  0.06864312986861915\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.91  % ,  0.2861718436781317  ,  0.06269952299594879\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.95  % ,  0.2712237183615565  ,  0.05933610156904906\n",
            "epoch  9\n",
            "saving weights.... \n",
            "91.01  % ,  0.2828846381247044  ,  0.05809593645557761\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8c85644e-0710-4671-984d-d3bf99040284\", \"features.40.npy\", 435)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0.3 features.40': {'nbr_param': [510.0, 6809739.0],\n",
              "  'test_acc': [81.45,\n",
              "   83.87,\n",
              "   86.15,\n",
              "   85.34,\n",
              "   85.9,\n",
              "   86.16,\n",
              "   85.47,\n",
              "   86.3,\n",
              "   85.49,\n",
              "   84.39,\n",
              "   86.94,\n",
              "   84.58,\n",
              "   83.76,\n",
              "   86.98,\n",
              "   85.56,\n",
              "   84.68,\n",
              "   86.86,\n",
              "   85.73,\n",
              "   84.84,\n",
              "   84.46,\n",
              "   90.46,\n",
              "   90.74,\n",
              "   90.62,\n",
              "   90.88,\n",
              "   91.03,\n",
              "   90.94,\n",
              "   90.82,\n",
              "   90.76,\n",
              "   90.96,\n",
              "   90.95,\n",
              "   83.39,\n",
              "   84.83,\n",
              "   85.22,\n",
              "   84.36,\n",
              "   84.2,\n",
              "   85.99,\n",
              "   85.64,\n",
              "   85.06,\n",
              "   86.18,\n",
              "   85.07,\n",
              "   85.59,\n",
              "   85.3,\n",
              "   83.05,\n",
              "   83.72,\n",
              "   85.13,\n",
              "   85.88,\n",
              "   86.78,\n",
              "   84.97,\n",
              "   84.54,\n",
              "   85.23,\n",
              "   90.37,\n",
              "   90.71,\n",
              "   90.88,\n",
              "   90.79,\n",
              "   91.04,\n",
              "   90.69,\n",
              "   91.11,\n",
              "   90.89,\n",
              "   90.89,\n",
              "   90.87,\n",
              "   84.6,\n",
              "   86.78,\n",
              "   85.39,\n",
              "   85.95,\n",
              "   84.75,\n",
              "   84.37,\n",
              "   85.27,\n",
              "   85.36,\n",
              "   86.16,\n",
              "   84.9,\n",
              "   86.11,\n",
              "   86.11,\n",
              "   86.44,\n",
              "   83.47,\n",
              "   84.73,\n",
              "   84.59,\n",
              "   85.8,\n",
              "   87.58,\n",
              "   85.14,\n",
              "   86.18,\n",
              "   90.33,\n",
              "   90.59,\n",
              "   90.99,\n",
              "   91.02,\n",
              "   91.2,\n",
              "   91.28,\n",
              "   90.96,\n",
              "   91.09,\n",
              "   91.02,\n",
              "   91.31,\n",
              "   85.36,\n",
              "   85.19,\n",
              "   85.88,\n",
              "   85.78,\n",
              "   85.4,\n",
              "   86.52,\n",
              "   84.98,\n",
              "   86.39,\n",
              "   85.24,\n",
              "   85.33,\n",
              "   85.92,\n",
              "   84.45,\n",
              "   85.22,\n",
              "   85.41,\n",
              "   85.78,\n",
              "   84.66,\n",
              "   84.44,\n",
              "   86.38,\n",
              "   85.59,\n",
              "   85.6,\n",
              "   90.55,\n",
              "   90.9,\n",
              "   91.11,\n",
              "   91.28,\n",
              "   91.21,\n",
              "   91.45,\n",
              "   91.24,\n",
              "   91.32,\n",
              "   91.42,\n",
              "   91.12,\n",
              "   83.74,\n",
              "   85.42,\n",
              "   86.87,\n",
              "   86.24,\n",
              "   84.29,\n",
              "   85.03,\n",
              "   86.11,\n",
              "   86.22,\n",
              "   86.58,\n",
              "   85.56,\n",
              "   85.27,\n",
              "   85.16,\n",
              "   84.6,\n",
              "   86.57,\n",
              "   86.97,\n",
              "   86.57,\n",
              "   86.42,\n",
              "   84.5,\n",
              "   86.07,\n",
              "   87.2,\n",
              "   90.28,\n",
              "   90.52,\n",
              "   90.85,\n",
              "   90.8,\n",
              "   90.86,\n",
              "   90.73,\n",
              "   90.7,\n",
              "   90.75,\n",
              "   91.04,\n",
              "   91.15,\n",
              "   85.16,\n",
              "   83.31,\n",
              "   85.63,\n",
              "   84.78,\n",
              "   86.78,\n",
              "   86.44,\n",
              "   85.98,\n",
              "   87.02,\n",
              "   85.58,\n",
              "   84.78,\n",
              "   84.45,\n",
              "   83.83,\n",
              "   87.12,\n",
              "   86.83,\n",
              "   85.8,\n",
              "   83.97,\n",
              "   85.54,\n",
              "   85.82,\n",
              "   85.73,\n",
              "   84.97,\n",
              "   90.09,\n",
              "   90.82,\n",
              "   90.81,\n",
              "   90.98,\n",
              "   90.97,\n",
              "   90.99,\n",
              "   91.36,\n",
              "   90.91,\n",
              "   90.95,\n",
              "   91.01],\n",
              "  'training_loss': [0.35084924191869793,\n",
              "   0.33106836324334143,\n",
              "   0.3264572918057442,\n",
              "   0.32258193586170675,\n",
              "   0.30834390057921407,\n",
              "   0.3096859028726816,\n",
              "   0.3019952151685953,\n",
              "   0.3045683059573174,\n",
              "   0.2969441183447838,\n",
              "   0.29681066096127035,\n",
              "   0.29954650689661505,\n",
              "   0.2892873609080911,\n",
              "   0.29731301822960376,\n",
              "   0.2883898736402392,\n",
              "   0.28643721699416635,\n",
              "   0.2871512551739812,\n",
              "   0.2908075893908739,\n",
              "   0.2831595894038677,\n",
              "   0.27482943947315214,\n",
              "   0.27688033969402315,\n",
              "   0.1570197295330465,\n",
              "   0.11273897748738528,\n",
              "   0.09747598305270076,\n",
              "   0.0871900948897004,\n",
              "   0.08088943989910186,\n",
              "   0.07286589509807527,\n",
              "   0.06815798930078745,\n",
              "   0.06490975444931536,\n",
              "   0.05689116675425321,\n",
              "   0.05504231601404026,\n",
              "   0.3490093362249434,\n",
              "   0.3341561746329069,\n",
              "   0.3225847297281027,\n",
              "   0.3229009267747402,\n",
              "   0.31207209722101686,\n",
              "   0.3082287725716829,\n",
              "   0.30282032886892557,\n",
              "   0.3000041302204132,\n",
              "   0.297326535987854,\n",
              "   0.29553226284086703,\n",
              "   0.29052219577431676,\n",
              "   0.29814579522311685,\n",
              "   0.2911034922719002,\n",
              "   0.2860803555697203,\n",
              "   0.28511460112929343,\n",
              "   0.2889214350298047,\n",
              "   0.28548475922346117,\n",
              "   0.29005676914751527,\n",
              "   0.28315103187263013,\n",
              "   0.2783119087189436,\n",
              "   0.14971897754147648,\n",
              "   0.10876924117542804,\n",
              "   0.09644272176064551,\n",
              "   0.08653565613180399,\n",
              "   0.07774113832805306,\n",
              "   0.07490336100943387,\n",
              "   0.06530787081457674,\n",
              "   0.06330178949665279,\n",
              "   0.05504229255290702,\n",
              "   0.05482144209183753,\n",
              "   0.35343674695268273,\n",
              "   0.3337914038449526,\n",
              "   0.3252654498666525,\n",
              "   0.3239388402611017,\n",
              "   0.3091171863377094,\n",
              "   0.3079055591315031,\n",
              "   0.30265917486846444,\n",
              "   0.3009394941329956,\n",
              "   0.2980901392430067,\n",
              "   0.30237568849176166,\n",
              "   0.2932415252715349,\n",
              "   0.2921770704388619,\n",
              "   0.2927383310556412,\n",
              "   0.29115306247770784,\n",
              "   0.2895364363715053,\n",
              "   0.2886902376636863,\n",
              "   0.2881756612852216,\n",
              "   0.2859439132660627,\n",
              "   0.2755860184654593,\n",
              "   0.28417954752743246,\n",
              "   0.16052379014119506,\n",
              "   0.11273329152613878,\n",
              "   0.09676752012558282,\n",
              "   0.09003175244890153,\n",
              "   0.07966110332384706,\n",
              "   0.0732280059479177,\n",
              "   0.06837274461966009,\n",
              "   0.06388776531741024,\n",
              "   0.05818172602578998,\n",
              "   0.054275437531806525,\n",
              "   0.36295564119517804,\n",
              "   0.34082623996436595,\n",
              "   0.32982140679657457,\n",
              "   0.3191536955565214,\n",
              "   0.3134048237979412,\n",
              "   0.3092199051529169,\n",
              "   0.30672234110832214,\n",
              "   0.303634811770916,\n",
              "   0.2993334087818861,\n",
              "   0.29577902571707965,\n",
              "   0.2960544077605009,\n",
              "   0.29373334351927044,\n",
              "   0.28747491617798804,\n",
              "   0.2896588575989008,\n",
              "   0.2853454539358616,\n",
              "   0.2792084229826927,\n",
              "   0.2857408996373415,\n",
              "   0.2790056697443128,\n",
              "   0.2799019447401166,\n",
              "   0.27800903359651563,\n",
              "   0.15863063304126263,\n",
              "   0.1155070024497807,\n",
              "   0.09722974300570786,\n",
              "   0.08993825114965438,\n",
              "   0.08214524556156248,\n",
              "   0.07159718549009413,\n",
              "   0.06737418829258532,\n",
              "   0.0630772864177823,\n",
              "   0.05784286631122231,\n",
              "   0.055296975947450845,\n",
              "   0.363784453356266,\n",
              "   0.3371664649128914,\n",
              "   0.3273284856915474,\n",
              "   0.3164309662848711,\n",
              "   0.31195530293881896,\n",
              "   0.30701902668476105,\n",
              "   0.30659070705473424,\n",
              "   0.3011259002685547,\n",
              "   0.30108107025027275,\n",
              "   0.29597196120619773,\n",
              "   0.2935586633861065,\n",
              "   0.2929565840899944,\n",
              "   0.2986148417592049,\n",
              "   0.2877674796313047,\n",
              "   0.2920510015428066,\n",
              "   0.28821502611786126,\n",
              "   0.2802938088312745,\n",
              "   0.2826227265834808,\n",
              "   0.2838340441420674,\n",
              "   0.2849051160275936,\n",
              "   0.15440826106145977,\n",
              "   0.1127880785215646,\n",
              "   0.10159137364998459,\n",
              "   0.08678633862100542,\n",
              "   0.08168033768981695,\n",
              "   0.07342577430456877,\n",
              "   0.06915799516290426,\n",
              "   0.0605387993209064,\n",
              "   0.05912152198003605,\n",
              "   0.05965982687594369,\n",
              "   0.3717221769273281,\n",
              "   0.342645852547884,\n",
              "   0.3308918531358242,\n",
              "   0.32479703027904033,\n",
              "   0.32218738459050655,\n",
              "   0.31108728597164154,\n",
              "   0.30706054975390434,\n",
              "   0.3057656193599105,\n",
              "   0.3050827283501625,\n",
              "   0.3001355744123459,\n",
              "   0.29309019438624384,\n",
              "   0.2936345613539219,\n",
              "   0.29521779091060163,\n",
              "   0.29319412536025047,\n",
              "   0.2843851218700409,\n",
              "   0.2883792933613062,\n",
              "   0.28178793233186006,\n",
              "   0.28494139921963213,\n",
              "   0.28002016406059266,\n",
              "   0.2837989174813032,\n",
              "   0.15235383375287057,\n",
              "   0.11242449519783258,\n",
              "   0.10297380566485226,\n",
              "   0.08519371609278023,\n",
              "   0.07809004698283971,\n",
              "   0.07317064554952085,\n",
              "   0.06864312986861915,\n",
              "   0.06269952299594879,\n",
              "   0.05933610156904906,\n",
              "   0.05809593645557761],\n",
              "  'valid_loss': [0.445020611679554,\n",
              "   0.43143266570568084,\n",
              "   0.3336475734829903,\n",
              "   0.39070509229898454,\n",
              "   0.37596815447807314,\n",
              "   0.3753319277882576,\n",
              "   0.39195153474807737,\n",
              "   0.3899280695796013,\n",
              "   0.4148231076717377,\n",
              "   0.4671101590037346,\n",
              "   0.37577719094753265,\n",
              "   0.44902252085208894,\n",
              "   0.48366415395736695,\n",
              "   0.38142940596342084,\n",
              "   0.4349351691126823,\n",
              "   0.4561902080774307,\n",
              "   0.374283144402504,\n",
              "   0.4381800763249397,\n",
              "   0.4702612398147583,\n",
              "   0.49551295857429506,\n",
              "   0.27742889786362646,\n",
              "   0.26843400350213054,\n",
              "   0.2728784598827362,\n",
              "   0.27477525190114976,\n",
              "   0.2835668076232076,\n",
              "   0.2755814109209925,\n",
              "   0.2766463681876659,\n",
              "   0.2857860713973641,\n",
              "   0.288745869487524,\n",
              "   0.28298514307141304,\n",
              "   0.37600775475502013,\n",
              "   0.39765059051513674,\n",
              "   0.38134266875982287,\n",
              "   0.42533464567661283,\n",
              "   0.4227616546392441,\n",
              "   0.37355665125846865,\n",
              "   0.3589714109659195,\n",
              "   0.42305661754608154,\n",
              "   0.3894899540185928,\n",
              "   0.43216359377503394,\n",
              "   0.41648665083646774,\n",
              "   0.41947494733929636,\n",
              "   0.47482827951908113,\n",
              "   0.5120783518075943,\n",
              "   0.4353105638861656,\n",
              "   0.4125898102760315,\n",
              "   0.3897325955629349,\n",
              "   0.44729194438457487,\n",
              "   0.44835713720321657,\n",
              "   0.4361024504184723,\n",
              "   0.2747956767976284,\n",
              "   0.2685240683019161,\n",
              "   0.2720258569866419,\n",
              "   0.26815159222483637,\n",
              "   0.264681710896641,\n",
              "   0.2755093263477087,\n",
              "   0.27389654819965364,\n",
              "   0.2693574653826654,\n",
              "   0.2791830378778279,\n",
              "   0.2693709916852415,\n",
              "   0.39301682991981507,\n",
              "   0.3161373514652252,\n",
              "   0.39213719725608825,\n",
              "   0.37707878687381746,\n",
              "   0.40358913493156434,\n",
              "   0.446502927339077,\n",
              "   0.40094965193271637,\n",
              "   0.3972926742196083,\n",
              "   0.39943355021476745,\n",
              "   0.44570843710899355,\n",
              "   0.41265030524730684,\n",
              "   0.3954956883907318,\n",
              "   0.39606044201850893,\n",
              "   0.4882007834196091,\n",
              "   0.45454874344468116,\n",
              "   0.46786845004558564,\n",
              "   0.40299729911088944,\n",
              "   0.38746205883026125,\n",
              "   0.4441994534254074,\n",
              "   0.4348069209098816,\n",
              "   0.274203447163105,\n",
              "   0.2629163083910942,\n",
              "   0.262554563139379,\n",
              "   0.2643008347868919,\n",
              "   0.2694631364941597,\n",
              "   0.2651593653574586,\n",
              "   0.26784609842002394,\n",
              "   0.2613277198076248,\n",
              "   0.30135913898050787,\n",
              "   0.2924967523664236,\n",
              "   0.3545898533821106,\n",
              "   0.3840061910390854,\n",
              "   0.34267830995321275,\n",
              "   0.3630928805828095,\n",
              "   0.37955370533466337,\n",
              "   0.33606634571552274,\n",
              "   0.44334350703954695,\n",
              "   0.3689177916407585,\n",
              "   0.4257809668302536,\n",
              "   0.42625437548160555,\n",
              "   0.4034199438333511,\n",
              "   0.46403091373443606,\n",
              "   0.43054973328113555,\n",
              "   0.4183733963131905,\n",
              "   0.41321967861652376,\n",
              "   0.4583630894899368,\n",
              "   0.4728661587715149,\n",
              "   0.40966051380634305,\n",
              "   0.43310665038228036,\n",
              "   0.4166913940548897,\n",
              "   0.28051051164269447,\n",
              "   0.2742628143578768,\n",
              "   0.2684525733411312,\n",
              "   0.2741292292892933,\n",
              "   0.2826760642796755,\n",
              "   0.2843408197283745,\n",
              "   0.27347555152773856,\n",
              "   0.2839307977795601,\n",
              "   0.2899189811892807,\n",
              "   0.2872998674452305,\n",
              "   0.3652986422300339,\n",
              "   0.3625009913682938,\n",
              "   0.334758321750164,\n",
              "   0.3796747834444046,\n",
              "   0.4028608358621597,\n",
              "   0.39987493793964385,\n",
              "   0.38616906170845033,\n",
              "   0.36436377462148667,\n",
              "   0.3698863430172205,\n",
              "   0.38048521510362626,\n",
              "   0.3995002091526985,\n",
              "   0.43879242265224455,\n",
              "   0.4572175764679909,\n",
              "   0.4018332665801048,\n",
              "   0.3661221534848213,\n",
              "   0.37979006061553955,\n",
              "   0.4177395811319351,\n",
              "   0.5004160271406174,\n",
              "   0.4203548230648041,\n",
              "   0.4090287123799324,\n",
              "   0.2832505022674799,\n",
              "   0.2651658486574888,\n",
              "   0.2661430995106697,\n",
              "   0.27634272849857805,\n",
              "   0.26659162866473196,\n",
              "   0.27456233023703097,\n",
              "   0.26980633396357295,\n",
              "   0.28606447594016793,\n",
              "   0.28653450000435116,\n",
              "   0.2885183928489685,\n",
              "   0.3603210410833359,\n",
              "   0.43097810161113737,\n",
              "   0.3614657994508743,\n",
              "   0.3784205896615982,\n",
              "   0.3495366342186928,\n",
              "   0.3731925350189209,\n",
              "   0.36887834451198576,\n",
              "   0.3504226750135422,\n",
              "   0.40596330823898313,\n",
              "   0.43222376096248627,\n",
              "   0.45111742639541624,\n",
              "   0.475445584487915,\n",
              "   0.36773753712177276,\n",
              "   0.3910072116613388,\n",
              "   0.40461605751514435,\n",
              "   0.484806388759613,\n",
              "   0.4231666206598282,\n",
              "   0.4379344573378563,\n",
              "   0.42086072874069214,\n",
              "   0.4403946658372879,\n",
              "   0.2683191631197929,\n",
              "   0.2728955974191427,\n",
              "   0.2713778224736452,\n",
              "   0.2636676842093468,\n",
              "   0.2731557501733303,\n",
              "   0.270313446174562,\n",
              "   0.2738521011829376,\n",
              "   0.2861718436781317,\n",
              "   0.2712237183615565,\n",
              "   0.2828846381247044]},\n",
              " '0.6 features.40': {'nbr_param': [510.0, 6425247.0],\n",
              "  'test_acc': [81.45,\n",
              "   83.87,\n",
              "   86.15,\n",
              "   85.34,\n",
              "   85.9,\n",
              "   86.16,\n",
              "   85.47,\n",
              "   86.3,\n",
              "   85.49,\n",
              "   84.39,\n",
              "   86.94,\n",
              "   84.58,\n",
              "   83.76,\n",
              "   86.98,\n",
              "   85.56,\n",
              "   84.68,\n",
              "   86.86,\n",
              "   85.73,\n",
              "   84.84,\n",
              "   84.46,\n",
              "   90.46,\n",
              "   90.74,\n",
              "   90.62,\n",
              "   90.88,\n",
              "   91.03,\n",
              "   90.94,\n",
              "   90.82,\n",
              "   90.76,\n",
              "   90.96,\n",
              "   90.95,\n",
              "   83.39,\n",
              "   84.83,\n",
              "   85.22,\n",
              "   84.36,\n",
              "   84.2,\n",
              "   85.99,\n",
              "   85.64,\n",
              "   85.06,\n",
              "   86.18,\n",
              "   85.07,\n",
              "   85.59,\n",
              "   85.3,\n",
              "   83.05,\n",
              "   83.72,\n",
              "   85.13,\n",
              "   85.88,\n",
              "   86.78,\n",
              "   84.97,\n",
              "   84.54,\n",
              "   85.23,\n",
              "   90.37,\n",
              "   90.71,\n",
              "   90.88,\n",
              "   90.79,\n",
              "   91.04,\n",
              "   90.69,\n",
              "   91.11,\n",
              "   90.89,\n",
              "   90.89,\n",
              "   90.87,\n",
              "   84.6,\n",
              "   86.78,\n",
              "   85.39,\n",
              "   85.95,\n",
              "   84.75,\n",
              "   84.37,\n",
              "   85.27,\n",
              "   85.36,\n",
              "   86.16,\n",
              "   84.9,\n",
              "   86.11,\n",
              "   86.11,\n",
              "   86.44,\n",
              "   83.47,\n",
              "   84.73,\n",
              "   84.59,\n",
              "   85.8,\n",
              "   87.58,\n",
              "   85.14,\n",
              "   86.18,\n",
              "   90.33,\n",
              "   90.59,\n",
              "   90.99,\n",
              "   91.02,\n",
              "   91.2,\n",
              "   91.28,\n",
              "   90.96,\n",
              "   91.09,\n",
              "   91.02,\n",
              "   91.31,\n",
              "   85.36,\n",
              "   85.19,\n",
              "   85.88,\n",
              "   85.78,\n",
              "   85.4,\n",
              "   86.52,\n",
              "   84.98,\n",
              "   86.39,\n",
              "   85.24,\n",
              "   85.33,\n",
              "   85.92,\n",
              "   84.45,\n",
              "   85.22,\n",
              "   85.41,\n",
              "   85.78,\n",
              "   84.66,\n",
              "   84.44,\n",
              "   86.38,\n",
              "   85.59,\n",
              "   85.6,\n",
              "   90.55,\n",
              "   90.9,\n",
              "   91.11,\n",
              "   91.28,\n",
              "   91.21,\n",
              "   91.45,\n",
              "   91.24,\n",
              "   91.32,\n",
              "   91.42,\n",
              "   91.12,\n",
              "   83.74,\n",
              "   85.42,\n",
              "   86.87,\n",
              "   86.24,\n",
              "   84.29,\n",
              "   85.03,\n",
              "   86.11,\n",
              "   86.22,\n",
              "   86.58,\n",
              "   85.56,\n",
              "   85.27,\n",
              "   85.16,\n",
              "   84.6,\n",
              "   86.57,\n",
              "   86.97,\n",
              "   86.57,\n",
              "   86.42,\n",
              "   84.5,\n",
              "   86.07,\n",
              "   87.2,\n",
              "   90.28,\n",
              "   90.52,\n",
              "   90.85,\n",
              "   90.8,\n",
              "   90.86,\n",
              "   90.73,\n",
              "   90.7,\n",
              "   90.75,\n",
              "   91.04,\n",
              "   91.15,\n",
              "   85.16,\n",
              "   83.31,\n",
              "   85.63,\n",
              "   84.78,\n",
              "   86.78,\n",
              "   86.44,\n",
              "   85.98,\n",
              "   87.02,\n",
              "   85.58,\n",
              "   84.78,\n",
              "   84.45,\n",
              "   83.83,\n",
              "   87.12,\n",
              "   86.83,\n",
              "   85.8,\n",
              "   83.97,\n",
              "   85.54,\n",
              "   85.82,\n",
              "   85.73,\n",
              "   84.97,\n",
              "   90.09,\n",
              "   90.82,\n",
              "   90.81,\n",
              "   90.98,\n",
              "   90.97,\n",
              "   90.99,\n",
              "   91.36,\n",
              "   90.91,\n",
              "   90.95,\n",
              "   91.01],\n",
              "  'training_loss': [0.35084924191869793,\n",
              "   0.33106836324334143,\n",
              "   0.3264572918057442,\n",
              "   0.32258193586170675,\n",
              "   0.30834390057921407,\n",
              "   0.3096859028726816,\n",
              "   0.3019952151685953,\n",
              "   0.3045683059573174,\n",
              "   0.2969441183447838,\n",
              "   0.29681066096127035,\n",
              "   0.29954650689661505,\n",
              "   0.2892873609080911,\n",
              "   0.29731301822960376,\n",
              "   0.2883898736402392,\n",
              "   0.28643721699416635,\n",
              "   0.2871512551739812,\n",
              "   0.2908075893908739,\n",
              "   0.2831595894038677,\n",
              "   0.27482943947315214,\n",
              "   0.27688033969402315,\n",
              "   0.1570197295330465,\n",
              "   0.11273897748738528,\n",
              "   0.09747598305270076,\n",
              "   0.0871900948897004,\n",
              "   0.08088943989910186,\n",
              "   0.07286589509807527,\n",
              "   0.06815798930078745,\n",
              "   0.06490975444931536,\n",
              "   0.05689116675425321,\n",
              "   0.05504231601404026,\n",
              "   0.3490093362249434,\n",
              "   0.3341561746329069,\n",
              "   0.3225847297281027,\n",
              "   0.3229009267747402,\n",
              "   0.31207209722101686,\n",
              "   0.3082287725716829,\n",
              "   0.30282032886892557,\n",
              "   0.3000041302204132,\n",
              "   0.297326535987854,\n",
              "   0.29553226284086703,\n",
              "   0.29052219577431676,\n",
              "   0.29814579522311685,\n",
              "   0.2911034922719002,\n",
              "   0.2860803555697203,\n",
              "   0.28511460112929343,\n",
              "   0.2889214350298047,\n",
              "   0.28548475922346117,\n",
              "   0.29005676914751527,\n",
              "   0.28315103187263013,\n",
              "   0.2783119087189436,\n",
              "   0.14971897754147648,\n",
              "   0.10876924117542804,\n",
              "   0.09644272176064551,\n",
              "   0.08653565613180399,\n",
              "   0.07774113832805306,\n",
              "   0.07490336100943387,\n",
              "   0.06530787081457674,\n",
              "   0.06330178949665279,\n",
              "   0.05504229255290702,\n",
              "   0.05482144209183753,\n",
              "   0.35343674695268273,\n",
              "   0.3337914038449526,\n",
              "   0.3252654498666525,\n",
              "   0.3239388402611017,\n",
              "   0.3091171863377094,\n",
              "   0.3079055591315031,\n",
              "   0.30265917486846444,\n",
              "   0.3009394941329956,\n",
              "   0.2980901392430067,\n",
              "   0.30237568849176166,\n",
              "   0.2932415252715349,\n",
              "   0.2921770704388619,\n",
              "   0.2927383310556412,\n",
              "   0.29115306247770784,\n",
              "   0.2895364363715053,\n",
              "   0.2886902376636863,\n",
              "   0.2881756612852216,\n",
              "   0.2859439132660627,\n",
              "   0.2755860184654593,\n",
              "   0.28417954752743246,\n",
              "   0.16052379014119506,\n",
              "   0.11273329152613878,\n",
              "   0.09676752012558282,\n",
              "   0.09003175244890153,\n",
              "   0.07966110332384706,\n",
              "   0.0732280059479177,\n",
              "   0.06837274461966009,\n",
              "   0.06388776531741024,\n",
              "   0.05818172602578998,\n",
              "   0.054275437531806525,\n",
              "   0.36295564119517804,\n",
              "   0.34082623996436595,\n",
              "   0.32982140679657457,\n",
              "   0.3191536955565214,\n",
              "   0.3134048237979412,\n",
              "   0.3092199051529169,\n",
              "   0.30672234110832214,\n",
              "   0.303634811770916,\n",
              "   0.2993334087818861,\n",
              "   0.29577902571707965,\n",
              "   0.2960544077605009,\n",
              "   0.29373334351927044,\n",
              "   0.28747491617798804,\n",
              "   0.2896588575989008,\n",
              "   0.2853454539358616,\n",
              "   0.2792084229826927,\n",
              "   0.2857408996373415,\n",
              "   0.2790056697443128,\n",
              "   0.2799019447401166,\n",
              "   0.27800903359651563,\n",
              "   0.15863063304126263,\n",
              "   0.1155070024497807,\n",
              "   0.09722974300570786,\n",
              "   0.08993825114965438,\n",
              "   0.08214524556156248,\n",
              "   0.07159718549009413,\n",
              "   0.06737418829258532,\n",
              "   0.0630772864177823,\n",
              "   0.05784286631122231,\n",
              "   0.055296975947450845,\n",
              "   0.363784453356266,\n",
              "   0.3371664649128914,\n",
              "   0.3273284856915474,\n",
              "   0.3164309662848711,\n",
              "   0.31195530293881896,\n",
              "   0.30701902668476105,\n",
              "   0.30659070705473424,\n",
              "   0.3011259002685547,\n",
              "   0.30108107025027275,\n",
              "   0.29597196120619773,\n",
              "   0.2935586633861065,\n",
              "   0.2929565840899944,\n",
              "   0.2986148417592049,\n",
              "   0.2877674796313047,\n",
              "   0.2920510015428066,\n",
              "   0.28821502611786126,\n",
              "   0.2802938088312745,\n",
              "   0.2826227265834808,\n",
              "   0.2838340441420674,\n",
              "   0.2849051160275936,\n",
              "   0.15440826106145977,\n",
              "   0.1127880785215646,\n",
              "   0.10159137364998459,\n",
              "   0.08678633862100542,\n",
              "   0.08168033768981695,\n",
              "   0.07342577430456877,\n",
              "   0.06915799516290426,\n",
              "   0.0605387993209064,\n",
              "   0.05912152198003605,\n",
              "   0.05965982687594369,\n",
              "   0.3717221769273281,\n",
              "   0.342645852547884,\n",
              "   0.3308918531358242,\n",
              "   0.32479703027904033,\n",
              "   0.32218738459050655,\n",
              "   0.31108728597164154,\n",
              "   0.30706054975390434,\n",
              "   0.3057656193599105,\n",
              "   0.3050827283501625,\n",
              "   0.3001355744123459,\n",
              "   0.29309019438624384,\n",
              "   0.2936345613539219,\n",
              "   0.29521779091060163,\n",
              "   0.29319412536025047,\n",
              "   0.2843851218700409,\n",
              "   0.2883792933613062,\n",
              "   0.28178793233186006,\n",
              "   0.28494139921963213,\n",
              "   0.28002016406059266,\n",
              "   0.2837989174813032,\n",
              "   0.15235383375287057,\n",
              "   0.11242449519783258,\n",
              "   0.10297380566485226,\n",
              "   0.08519371609278023,\n",
              "   0.07809004698283971,\n",
              "   0.07317064554952085,\n",
              "   0.06864312986861915,\n",
              "   0.06269952299594879,\n",
              "   0.05933610156904906,\n",
              "   0.05809593645557761],\n",
              "  'valid_loss': [0.445020611679554,\n",
              "   0.43143266570568084,\n",
              "   0.3336475734829903,\n",
              "   0.39070509229898454,\n",
              "   0.37596815447807314,\n",
              "   0.3753319277882576,\n",
              "   0.39195153474807737,\n",
              "   0.3899280695796013,\n",
              "   0.4148231076717377,\n",
              "   0.4671101590037346,\n",
              "   0.37577719094753265,\n",
              "   0.44902252085208894,\n",
              "   0.48366415395736695,\n",
              "   0.38142940596342084,\n",
              "   0.4349351691126823,\n",
              "   0.4561902080774307,\n",
              "   0.374283144402504,\n",
              "   0.4381800763249397,\n",
              "   0.4702612398147583,\n",
              "   0.49551295857429506,\n",
              "   0.27742889786362646,\n",
              "   0.26843400350213054,\n",
              "   0.2728784598827362,\n",
              "   0.27477525190114976,\n",
              "   0.2835668076232076,\n",
              "   0.2755814109209925,\n",
              "   0.2766463681876659,\n",
              "   0.2857860713973641,\n",
              "   0.288745869487524,\n",
              "   0.28298514307141304,\n",
              "   0.37600775475502013,\n",
              "   0.39765059051513674,\n",
              "   0.38134266875982287,\n",
              "   0.42533464567661283,\n",
              "   0.4227616546392441,\n",
              "   0.37355665125846865,\n",
              "   0.3589714109659195,\n",
              "   0.42305661754608154,\n",
              "   0.3894899540185928,\n",
              "   0.43216359377503394,\n",
              "   0.41648665083646774,\n",
              "   0.41947494733929636,\n",
              "   0.47482827951908113,\n",
              "   0.5120783518075943,\n",
              "   0.4353105638861656,\n",
              "   0.4125898102760315,\n",
              "   0.3897325955629349,\n",
              "   0.44729194438457487,\n",
              "   0.44835713720321657,\n",
              "   0.4361024504184723,\n",
              "   0.2747956767976284,\n",
              "   0.2685240683019161,\n",
              "   0.2720258569866419,\n",
              "   0.26815159222483637,\n",
              "   0.264681710896641,\n",
              "   0.2755093263477087,\n",
              "   0.27389654819965364,\n",
              "   0.2693574653826654,\n",
              "   0.2791830378778279,\n",
              "   0.2693709916852415,\n",
              "   0.39301682991981507,\n",
              "   0.3161373514652252,\n",
              "   0.39213719725608825,\n",
              "   0.37707878687381746,\n",
              "   0.40358913493156434,\n",
              "   0.446502927339077,\n",
              "   0.40094965193271637,\n",
              "   0.3972926742196083,\n",
              "   0.39943355021476745,\n",
              "   0.44570843710899355,\n",
              "   0.41265030524730684,\n",
              "   0.3954956883907318,\n",
              "   0.39606044201850893,\n",
              "   0.4882007834196091,\n",
              "   0.45454874344468116,\n",
              "   0.46786845004558564,\n",
              "   0.40299729911088944,\n",
              "   0.38746205883026125,\n",
              "   0.4441994534254074,\n",
              "   0.4348069209098816,\n",
              "   0.274203447163105,\n",
              "   0.2629163083910942,\n",
              "   0.262554563139379,\n",
              "   0.2643008347868919,\n",
              "   0.2694631364941597,\n",
              "   0.2651593653574586,\n",
              "   0.26784609842002394,\n",
              "   0.2613277198076248,\n",
              "   0.30135913898050787,\n",
              "   0.2924967523664236,\n",
              "   0.3545898533821106,\n",
              "   0.3840061910390854,\n",
              "   0.34267830995321275,\n",
              "   0.3630928805828095,\n",
              "   0.37955370533466337,\n",
              "   0.33606634571552274,\n",
              "   0.44334350703954695,\n",
              "   0.3689177916407585,\n",
              "   0.4257809668302536,\n",
              "   0.42625437548160555,\n",
              "   0.4034199438333511,\n",
              "   0.46403091373443606,\n",
              "   0.43054973328113555,\n",
              "   0.4183733963131905,\n",
              "   0.41321967861652376,\n",
              "   0.4583630894899368,\n",
              "   0.4728661587715149,\n",
              "   0.40966051380634305,\n",
              "   0.43310665038228036,\n",
              "   0.4166913940548897,\n",
              "   0.28051051164269447,\n",
              "   0.2742628143578768,\n",
              "   0.2684525733411312,\n",
              "   0.2741292292892933,\n",
              "   0.2826760642796755,\n",
              "   0.2843408197283745,\n",
              "   0.27347555152773856,\n",
              "   0.2839307977795601,\n",
              "   0.2899189811892807,\n",
              "   0.2872998674452305,\n",
              "   0.3652986422300339,\n",
              "   0.3625009913682938,\n",
              "   0.334758321750164,\n",
              "   0.3796747834444046,\n",
              "   0.4028608358621597,\n",
              "   0.39987493793964385,\n",
              "   0.38616906170845033,\n",
              "   0.36436377462148667,\n",
              "   0.3698863430172205,\n",
              "   0.38048521510362626,\n",
              "   0.3995002091526985,\n",
              "   0.43879242265224455,\n",
              "   0.4572175764679909,\n",
              "   0.4018332665801048,\n",
              "   0.3661221534848213,\n",
              "   0.37979006061553955,\n",
              "   0.4177395811319351,\n",
              "   0.5004160271406174,\n",
              "   0.4203548230648041,\n",
              "   0.4090287123799324,\n",
              "   0.2832505022674799,\n",
              "   0.2651658486574888,\n",
              "   0.2661430995106697,\n",
              "   0.27634272849857805,\n",
              "   0.26659162866473196,\n",
              "   0.27456233023703097,\n",
              "   0.26980633396357295,\n",
              "   0.28606447594016793,\n",
              "   0.28653450000435116,\n",
              "   0.2885183928489685,\n",
              "   0.3603210410833359,\n",
              "   0.43097810161113737,\n",
              "   0.3614657994508743,\n",
              "   0.3784205896615982,\n",
              "   0.3495366342186928,\n",
              "   0.3731925350189209,\n",
              "   0.36887834451198576,\n",
              "   0.3504226750135422,\n",
              "   0.40596330823898313,\n",
              "   0.43222376096248627,\n",
              "   0.45111742639541624,\n",
              "   0.475445584487915,\n",
              "   0.36773753712177276,\n",
              "   0.3910072116613388,\n",
              "   0.40461605751514435,\n",
              "   0.484806388759613,\n",
              "   0.4231666206598282,\n",
              "   0.4379344573378563,\n",
              "   0.42086072874069214,\n",
              "   0.4403946658372879,\n",
              "   0.2683191631197929,\n",
              "   0.2728955974191427,\n",
              "   0.2713778224736452,\n",
              "   0.2636676842093468,\n",
              "   0.2731557501733303,\n",
              "   0.270313446174562,\n",
              "   0.2738521011829376,\n",
              "   0.2861718436781317,\n",
              "   0.2712237183615565,\n",
              "   0.2828846381247044]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ReVvCrfrHiH"
      },
      "source": [
        "### Résultats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "15dNTVMGtsoB",
        "outputId": "6c1faf0d-b32f-426f-c0a3-9d1b6869479d"
      },
      "source": [
        "\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "\n",
        "#files.upload()\n",
        "\n",
        "#files.download('checkpoint.pt')\n",
        " \n",
        " \n",
        "list_modules=[ \"features.0\",\n",
        "              \"features.3\",\n",
        "              \"features.7\",\n",
        "              \"features.10\",\n",
        "              \"features.14\",\n",
        "              \"features.17\",\n",
        "              \"features.20\",\n",
        "              \"features.24\",\n",
        "              \"features.27\",\n",
        "              \"features.30\",\n",
        "              \"features.34\",\n",
        "              \"features.37\",\n",
        "              \"features.40\"]\n",
        "list_dico={}\n",
        "for i in list_modules :\n",
        "\n",
        "\n",
        "  try: \n",
        "    current_feature=np.load(i+'.npy', allow_pickle='TRUE')\n",
        "\n",
        "  except :\n",
        "    print(\"chargez \",i)\n",
        "    files.upload()\n",
        "    current_feature=np.load(i+'.npy', allow_pickle='TRUE')\n",
        "\n",
        "  current_feature=current_feature.tolist()\n",
        "\n",
        "  for key, value in current_feature.items() :\n",
        "    list_dico[key]=value\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  print(list_dico)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chargez  features.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ee08d1d4-5787-4794-8c64-008871a8382e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ee08d1d4-5787-4794-8c64-008871a8382e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}}\n",
            "{'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}}\n",
            "chargez  features.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a7b5483a-c2fc-43dd-96ca-942babd7a89b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a7b5483a-c2fc-43dd-96ca-942babd7a89b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.3.npy to features.3.npy\n",
            "{'0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}}\n",
            "{'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}}\n",
            "chargez  features.7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4eee8d7d-f415-4694-919d-768a9d8cf1b8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4eee8d7d-f415-4694-919d-768a9d8cf1b8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.7.npy to features.7.npy\n",
            "{'0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}}\n",
            "{'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}, '0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}}\n",
            "chargez  features.10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-59012892-9d41-4679-aa27-0eed9b0d264a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-59012892-9d41-4679-aa27-0eed9b0d264a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.10.npy to features.10.npy\n",
            "{'0.3 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}, '0.6 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7145631.0]}}\n",
            "{'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}, '0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}, '0.3 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}, '0.6 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7145631.0]}}\n",
            "chargez  features.14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-96dd49e3-8110-4174-b9e3-79a14aed3aaa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-96dd49e3-8110-4174-b9e3-79a14aed3aaa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.14.npy to features.14.npy\n",
            "{'0.3 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7145901.0]}, '0.6 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7097571.0]}}\n",
            "{'test_acc': 90.78, 'nbr_param': [510.0, 7145901.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}, '0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}, '0.3 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}, '0.6 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7145631.0]}, '0.3 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7145901.0]}, '0.6 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7097571.0]}}\n",
            "chargez  features.17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ce90e02c-cea2-473e-b072-55fade8d8ffd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ce90e02c-cea2-473e-b072-55fade8d8ffd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.17.npy to features.17.npy\n",
            "{'0.3 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}}\n",
            "{'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}, '0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}, '0.3 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}, '0.6 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7145631.0]}, '0.3 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7145901.0]}, '0.6 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7097571.0]}, '0.3 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}}\n",
            "chargez  features.20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-201bc897-38d2-4c7b-b467-6c893fdeb639\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-201bc897-38d2-4c7b-b467-6c893fdeb639\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.20.npy to features.20.npy\n",
            "{'0.3 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}}\n",
            "{'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}, '0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}, '0.3 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}, '0.6 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7145631.0]}, '0.3 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7145901.0]}, '0.6 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7097571.0]}, '0.3 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}}\n",
            "chargez  features.24\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0f1d5115-c5f4-4833-9bbb-580a2dfe1af6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0f1d5115-c5f4-4833-9bbb-580a2dfe1af6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.24.npy to features.24.npy\n",
            "{'0.3 features.24': {'test_acc': 91.0, 'nbr_param': [510.0, 7001985.0]}, '0.6 features.24': {'test_acc': 91.08, 'nbr_param': [510.0, 6809739.0]}}\n",
            "{'test_acc': 91.0, 'nbr_param': [510.0, 7001985.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}, '0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}, '0.3 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}, '0.6 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7145631.0]}, '0.3 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7145901.0]}, '0.6 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7097571.0]}, '0.3 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.24': {'test_acc': 91.0, 'nbr_param': [510.0, 7001985.0]}, '0.6 features.24': {'test_acc': 91.08, 'nbr_param': [510.0, 6809739.0]}}\n",
            "chargez  features.27\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9a43bf9a-1cd7-4d00-ac14-ea2eb62b161e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9a43bf9a-1cd7-4d00-ac14-ea2eb62b161e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.27.npy to features.27.npy\n",
            "{'0.3 features.27': {'test_acc': 91.1, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.27': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}}\n",
            "{'test_acc': 91.1, 'nbr_param': [510.0, 6809739.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}, '0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}, '0.3 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}, '0.6 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7145631.0]}, '0.3 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7145901.0]}, '0.6 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7097571.0]}, '0.3 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.24': {'test_acc': 91.0, 'nbr_param': [510.0, 7001985.0]}, '0.6 features.24': {'test_acc': 91.08, 'nbr_param': [510.0, 6809739.0]}, '0.3 features.27': {'test_acc': 91.1, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.27': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}}\n",
            "chargez  features.30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bd1fdb81-e7b9-4189-a379-a5d6aa509b3e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bd1fdb81-e7b9-4189-a379-a5d6aa509b3e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.30.npy to features.30.npy\n",
            "{'0.3 features.30': {'test_acc': 91.37, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.30': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}}\n",
            "{'test_acc': 91.37, 'nbr_param': [510.0, 6809739.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}, '0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}, '0.3 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}, '0.6 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7145631.0]}, '0.3 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7145901.0]}, '0.6 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7097571.0]}, '0.3 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.24': {'test_acc': 91.0, 'nbr_param': [510.0, 7001985.0]}, '0.6 features.24': {'test_acc': 91.08, 'nbr_param': [510.0, 6809739.0]}, '0.3 features.27': {'test_acc': 91.1, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.27': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.30': {'test_acc': 91.37, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.30': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}}\n",
            "chargez  features.34\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-334fab52-12e2-4d91-9af9-ee400db75fca\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-334fab52-12e2-4d91-9af9-ee400db75fca\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.34.npy to features.34 (1).npy\n",
            "{'0.3 features.34': {'test_acc': 91.03, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.34': {'test_acc': 91.11, 'nbr_param': [510.0, 6425247.0]}}\n",
            "{'test_acc': 91.03, 'nbr_param': [510.0, 6809739.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}, '0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}, '0.3 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}, '0.6 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7145631.0]}, '0.3 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7145901.0]}, '0.6 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7097571.0]}, '0.3 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.24': {'test_acc': 91.0, 'nbr_param': [510.0, 7001985.0]}, '0.6 features.24': {'test_acc': 91.08, 'nbr_param': [510.0, 6809739.0]}, '0.3 features.27': {'test_acc': 91.1, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.27': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.30': {'test_acc': 91.37, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.30': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.34': {'test_acc': 91.03, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.34': {'test_acc': 91.11, 'nbr_param': [510.0, 6425247.0]}}\n",
            "chargez  features.37\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d25df70a-165a-4413-bf33-714d88be166e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d25df70a-165a-4413-bf33-714d88be166e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.37.npy to features.37 (1).npy\n",
            "{'0.3 features.37': {'test_acc': 91.31, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.37': {'test_acc': 91.45, 'nbr_param': [510.0, 6425247.0]}}\n",
            "{'test_acc': 91.31, 'nbr_param': [510.0, 6809739.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}, '0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}, '0.3 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}, '0.6 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7145631.0]}, '0.3 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7145901.0]}, '0.6 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7097571.0]}, '0.3 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.24': {'test_acc': 91.0, 'nbr_param': [510.0, 7001985.0]}, '0.6 features.24': {'test_acc': 91.08, 'nbr_param': [510.0, 6809739.0]}, '0.3 features.27': {'test_acc': 91.1, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.27': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.30': {'test_acc': 91.37, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.30': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.34': {'test_acc': 91.03, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.34': {'test_acc': 91.11, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.37': {'test_acc': 91.31, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.37': {'test_acc': 91.45, 'nbr_param': [510.0, 6425247.0]}}\n",
            "chargez  features.40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d689cb42-5643-4f86-b2fa-c524f7436df3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d689cb42-5643-4f86-b2fa-c524f7436df3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving features.40.npy to features.40 (1).npy\n",
            "{'0.3 features.40': {'test_acc': 91.45, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.40': {'test_acc': 91.45, 'nbr_param': [510.0, 6425247.0]}}\n",
            "{'test_acc': 91.45, 'nbr_param': [510.0, 6809739.0]}\n",
            "<class 'dict'>\n",
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}, '0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}, '0.3 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}, '0.6 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7145631.0]}, '0.3 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7145901.0]}, '0.6 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7097571.0]}, '0.3 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.24': {'test_acc': 91.0, 'nbr_param': [510.0, 7001985.0]}, '0.6 features.24': {'test_acc': 91.08, 'nbr_param': [510.0, 6809739.0]}, '0.3 features.27': {'test_acc': 91.1, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.27': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.30': {'test_acc': 91.37, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.30': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.34': {'test_acc': 91.03, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.34': {'test_acc': 91.11, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.37': {'test_acc': 91.31, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.37': {'test_acc': 91.45, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.40': {'test_acc': 91.45, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.40': {'test_acc': 91.45, 'nbr_param': [510.0, 6425247.0]}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zhxUkrhzoQds",
        "outputId": "36b3959f-efe3-401b-edcb-9cdf0bf1dd47"
      },
      "source": [
        "print(list_dico)\n",
        "\n",
        "figure=plt.figure(figsize=(10,10))\n",
        "\n",
        "for key, value in list_dico.items() :\n",
        "  #if key[:3]==str(0.3):\n",
        "\n",
        "  print(key,\" : \",value[\"test_acc\"],\" ---- \",sum(value[\"nbr_param\"]))\n",
        "  plt.scatter(sum(value[\"nbr_param\"]),value[\"test_acc\"])\n",
        "\n",
        "plt.legend(list_dico.keys())\n",
        "plt.xlabel(\"millions of parameters\")\n",
        "plt.ylabel(\"Accuracy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'0.3 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193961.0]}, '0.6 features.0': {'test_acc': 91.01, 'nbr_param': [510.0, 7193691.0]}, '0.3 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7188156.0]}, '0.6 features.3': {'test_acc': 91.01, 'nbr_param': [510.0, 7182081.0]}, '0.3 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7182081.0]}, '0.6 features.7': {'test_acc': 91.35, 'nbr_param': [510.0, 7169931.0]}, '0.3 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7169931.0]}, '0.6 features.10': {'test_acc': 91.26, 'nbr_param': [510.0, 7145631.0]}, '0.3 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7145901.0]}, '0.6 features.14': {'test_acc': 90.78, 'nbr_param': [510.0, 7097571.0]}, '0.3 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.17': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7098108.0]}, '0.6 features.20': {'test_acc': 91.26, 'nbr_param': [510.0, 7001985.0]}, '0.3 features.24': {'test_acc': 91.0, 'nbr_param': [510.0, 7001985.0]}, '0.6 features.24': {'test_acc': 91.08, 'nbr_param': [510.0, 6809739.0]}, '0.3 features.27': {'test_acc': 91.1, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.27': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.30': {'test_acc': 91.37, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.30': {'test_acc': 91.37, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.34': {'test_acc': 91.03, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.34': {'test_acc': 91.11, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.37': {'test_acc': 91.31, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.37': {'test_acc': 91.45, 'nbr_param': [510.0, 6425247.0]}, '0.3 features.40': {'test_acc': 91.45, 'nbr_param': [510.0, 6809739.0]}, '0.6 features.40': {'test_acc': 91.45, 'nbr_param': [510.0, 6425247.0]}}\n",
            "0.3 features.0  :  91.01  ----  7194471.0\n",
            "0.6 features.0  :  91.01  ----  7194201.0\n",
            "0.3 features.3  :  91.01  ----  7188666.0\n",
            "0.6 features.3  :  91.01  ----  7182591.0\n",
            "0.3 features.7  :  91.35  ----  7182591.0\n",
            "0.6 features.7  :  91.35  ----  7170441.0\n",
            "0.3 features.10  :  91.26  ----  7170441.0\n",
            "0.6 features.10  :  91.26  ----  7146141.0\n",
            "0.3 features.14  :  90.78  ----  7146411.0\n",
            "0.6 features.14  :  90.78  ----  7098081.0\n",
            "0.3 features.17  :  91.26  ----  7098618.0\n",
            "0.6 features.17  :  91.26  ----  7002495.0\n",
            "0.3 features.20  :  91.26  ----  7098618.0\n",
            "0.6 features.20  :  91.26  ----  7002495.0\n",
            "0.3 features.24  :  91.0  ----  7002495.0\n",
            "0.6 features.24  :  91.08  ----  6810249.0\n",
            "0.3 features.27  :  91.1  ----  6810249.0\n",
            "0.6 features.27  :  91.37  ----  6425757.0\n",
            "0.3 features.30  :  91.37  ----  6810249.0\n",
            "0.6 features.30  :  91.37  ----  6425757.0\n",
            "0.3 features.34  :  91.03  ----  6810249.0\n",
            "0.6 features.34  :  91.11  ----  6425757.0\n",
            "0.3 features.37  :  91.31  ----  6810249.0\n",
            "0.6 features.37  :  91.45  ----  6425757.0\n",
            "0.3 features.40  :  91.45  ----  6810249.0\n",
            "0.6 features.40  :  91.45  ----  6425757.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq8-BK2Y49Cg"
      },
      "source": [
        "**Dans ce graphique nous pouvons observer les divers compromis entre accuracy et nombre de paramètres. Au vu du nombre de points, la lisibilité peut être difficile, le texte au dessus du graphique affiche les résultats précis du pruning de chacuns des layers.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P03G5Tw7w1L"
      },
      "source": [
        "### Utilisation de ces résultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luRV8TjW71rT"
      },
      "source": [
        "**Nous pensions concatener les divers élagages de layers qui semblaient être intéressants ( Pour chaque layer on regarde quels sont les ratios permettant d'obtenir environ 91% d'accuracy avec le moins de paramètres possible). Cependant cette méthode n'a pas abouti**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znmhi1Pl8S9k"
      },
      "source": [
        "**Nous avons donc essentiellement retenu que nous pouvions hautement pruner les hauts étages 70 à 80% et qu'il vallait mieux éviter de pruner les bas étages 10/20%. Au vu du faible nombre de paramètres économisés en prunant les bas layers, nous avons choisis de ne pas les pruner :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFJ8fkv7oQo4",
        "outputId": "65af9f44-77c1-4f98-ef98-d44c0df4352c"
      },
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "\n",
        "try: \n",
        "  loaded_cpt = torch.load('checkpoint.pt')\n",
        "\n",
        "except :\n",
        "  \n",
        "  #loaded_cpt = torch.load('C:/Users/Malo/Desktop/Cours/Fise_A2/S4/DeepL/prunage de layers sur cifar 10/checkpoint.pt',map_location=torch.device('cpu'))\n",
        "  if True :\n",
        "\n",
        "\n",
        "    print(\"Chargez les poids\")\n",
        "    files.upload()\n",
        "    loaded_cpt = torch.load('checkpoint.pt')\n",
        "net = VGG('VGG16')\n",
        "  \n",
        "if data_int :\n",
        "  net.half()\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(loaded_cpt[\"net\"])\n",
        "#\"features.0\":0.2,\n",
        "              #\"features.3\":0.2,\n",
        "              #\"features.7\":0.3,\n",
        "              #\"features.10\":0.3,\n",
        "              #\"features.14\":0.3,\n",
        "\n",
        "list_modules={ \n",
        "              \"features.17\":0.3,\n",
        "              \"features.20\":0.3,\n",
        "              \"features.24\":0.3,\n",
        "              \"features.27\":0.6,\n",
        "              \"features.30\":0.6,\n",
        "              \"features.34\":0.6,\n",
        "              \"features.37\":0.7,\n",
        "              \"features.40\":0.7}\n",
        "\n",
        "for key, value in list_modules.items() :  \n",
        "\n",
        "\n",
        "  print(\"_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \")\n",
        "  \n",
        "  paramconv1,paramfc1= get_number_param_pruned(mymodel.model)\n",
        "  mymodel.prune_spef_layer(key,value,2)\n",
        "  paramconv2,paramfc2=get_number_param_pruned(mymodel.model)\n",
        "  print(paramconv2,paramfc2)\n",
        "\n",
        "  print(\"parameters losed : \",sum([paramconv2,paramfc2])-sum([ paramconv1,paramfc1]))\n",
        "\n",
        "\n",
        "valid_loss,training_loss,test_accuracy=[],[],[]\n",
        "# entrainement a\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 10,criterion,optimizer,mymodel,scheduler=False ) \n",
        "\n",
        "     \n",
        "    #entrainement b\n",
        "\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,15,criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy,scheduler=False ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "features.17\n",
            "7098108.0 510.0\n",
            "parameters losed :  -96123.0\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "features.20\n",
            "7001985.0 510.0\n",
            "parameters losed :  -96123.0\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "features.24\n",
            "6809739.0 510.0\n",
            "parameters losed :  -192246.0\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "features.27\n",
            "6040755.0 510.0\n",
            "parameters losed :  -768984.0\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "features.30\n",
            "5271771.0 510.0\n",
            "parameters losed :  -768984.0\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "features.34\n",
            "4502787.0 510.0\n",
            "parameters losed :  -768984.0\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "features.37\n",
            "3733803.0 510.0\n",
            "parameters losed :  -768984.0\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "features.40\n",
            "2964819.0 510.0\n",
            "parameters losed :  -768984.0\n",
            "epoch  0\n",
            "saving weights.... \n",
            "83.48  % ,  0.45435957016944883  ,  0.6119027866959572\n",
            "epoch  1\n",
            "saving weights.... \n",
            "84.24  % ,  0.4752444675207138  ,  0.4758216939985752\n",
            "epoch  2\n",
            "saving weights.... \n",
            "80.88  % ,  0.537744377565384  ,  0.42204227139949796\n",
            "epoch  3\n",
            "saving weights.... \n",
            "81.25  % ,  0.49624948415756226  ,  0.40253948540091516\n",
            "epoch  4\n",
            "saving weights.... \n",
            "84.45  % ,  0.4171159926772118  ,  0.38422360047698023\n",
            "epoch  5\n",
            "saving weights.... \n",
            "86.01  % ,  0.4079718331336975  ,  0.37013914722502234\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.19  % ,  0.4444079374670982  ,  0.3660958434164524\n",
            "epoch  7\n",
            "saving weights.... \n",
            "81.85  % ,  0.5164513051271439  ,  0.3561770790100098\n",
            "epoch  8\n",
            "saving weights.... \n",
            "82.7  % ,  0.5102485874891282  ,  0.35067741899192334\n",
            "epoch  9\n",
            "saving weights.... \n",
            "85.52  % ,  0.4191967950582504  ,  0.3476885409384966\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "89.77  % ,  0.282100673019886  ,  0.21139328403174878\n",
            "epoch  1\n",
            "saving weights.... \n",
            "89.91  % ,  0.2645319308280945  ,  0.16715793341100216\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.13  % ,  0.2573323635160923  ,  0.15046967632099986\n",
            "epoch  3\n",
            "saving weights.... \n",
            "89.95  % ,  0.2561647116392851  ,  0.1369008764989674\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.01  % ,  0.26065034468024967  ,  0.12622031763494015\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.25  % ,  0.27894733245968817  ,  0.11842497381903232\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.4  % ,  0.2696252995416522  ,  0.10997208295017481\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.51  % ,  0.26144559914469717  ,  0.11124092546422035\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.08  % ,  0.256799590665102  ,  0.10210813930444419\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.53  % ,  0.27716962262690065  ,  0.09797632329538465\n",
            "epoch  10\n",
            "saving weights.... \n",
            "90.17  % ,  0.2782396710202098  ,  0.0907297325965017\n",
            "epoch  11\n",
            "saving weights.... \n",
            "90.16  % ,  0.2840081512182951  ,  0.08778856888264418\n",
            "epoch  12\n",
            "saving weights.... \n",
            "90.3  % ,  0.277274590331316  ,  0.08547573546804488\n",
            "epoch  13\n",
            "saving weights.... \n",
            "90.5  % ,  0.284479318074882  ,  0.08069781730156392\n",
            "epoch  14\n",
            "saving weights.... \n",
            "90.25  % ,  0.27592954231202604  ,  0.0757790561536327\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRoYZLjibflF",
        "outputId": "e74d1526-c9ec-4b81-f5c7-c45aa451dc41"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,20,criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy,scheduler=False ) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "saving weights.... \n",
            "90.22  % ,  0.28221216628700496  ,  0.07299854901302606\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.48  % ,  0.29496109774708745  ,  0.07162552430070937\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.36  % ,  0.29217873817384243  ,  0.0653748398590833\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.48  % ,  0.3114155019775033  ,  0.06503173639141023\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.37  % ,  0.2940168977454305  ,  0.06459791488032789\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.33  % ,  0.30059139006137847  ,  0.06045272808149457\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.44  % ,  0.30311511041224004  ,  0.06116835808120668\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.53  % ,  0.30273729188740256  ,  0.05570056735472754\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.42  % ,  0.30185889819562434  ,  0.06009721760507673\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.29  % ,  0.3091937478750944  ,  0.05486445414824411\n",
            "epoch  10\n",
            "saving weights.... \n",
            "90.19  % ,  0.3008853645659983  ,  0.05403703319407068\n",
            "epoch  11\n",
            "saving weights.... \n",
            "90.18  % ,  0.3109047457953915  ,  0.052887483596242964\n",
            "epoch  12\n",
            "saving weights.... \n",
            "90.01  % ,  0.32144146934449674  ,  0.05173171101026237\n",
            "epoch  13\n",
            "saving weights.... \n",
            "90.51  % ,  0.3224024125792086  ,  0.05120064543588087\n",
            "epoch  14\n",
            "saving weights.... \n",
            "90.33  % ,  0.3304637829706073  ,  0.04859120678165928\n",
            "epoch  15\n",
            "saving weights.... \n",
            "90.5  % ,  0.30331824037134647  ,  0.04972723860265687\n",
            "epoch  16\n",
            "saving weights.... \n",
            "90.23  % ,  0.3180275060430169  ,  0.047249583535734566\n",
            "epoch  17\n",
            "saving weights.... \n",
            "90.25  % ,  0.31091097383722666  ,  0.04891420973036438\n",
            "epoch  18\n",
            "saving weights.... \n",
            "90.38  % ,  0.32610583756752315  ,  0.046160503162723035\n",
            "epoch  19\n",
            "saving weights.... \n",
            "90.12  % ,  0.33232299412861466  ,  0.04753679615808651\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZXWzH6mdUpa",
        "outputId": "fa03ae70-f4a1-4319-8589-9f0c3a54a275"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.0001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,15,criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy,scheduler=False ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "saving weights.... \n",
            "90.63  % ,  0.30974034922719  ,  0.03493915862971917\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.75  % ,  0.31308472573310137  ,  0.029332946519088\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.82  % ,  0.29563098640553653  ,  0.029516348081082104\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.75  % ,  0.31201216403618454  ,  0.027539601940382272\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.81  % ,  0.3058175583511591  ,  0.026980595519393684\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.99  % ,  0.3084580358508974  ,  0.023788554584654047\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.95  % ,  0.3032103516958654  ,  0.024545495200762525\n",
            "epoch  7\n",
            "saving weights.... \n",
            "90.88  % ,  0.3066167630985379  ,  0.02351048816619441\n",
            "epoch  8\n",
            "saving weights.... \n",
            "91.09  % ,  0.3110354413598776  ,  0.02246576363942586\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.86  % ,  0.3087452600732446  ,  0.02125116143221967\n",
            "epoch  10\n",
            "saving weights.... \n",
            "91.03  % ,  0.3096977744370699  ,  0.021688511567236855\n",
            "epoch  11\n",
            "saving weights.... \n",
            "90.98  % ,  0.3088596345551312  ,  0.02025357852736488\n",
            "epoch  12\n",
            "saving weights.... \n",
            "91.09  % ,  0.30243215083628894  ,  0.02151289670453407\n",
            "epoch  13\n",
            "saving weights.... \n",
            "91.01  % ,  0.3057559465892613  ,  0.021141929371189326\n",
            "epoch  14\n",
            "saving weights.... \n",
            "91.03  % ,  0.31605723102875055  ,  0.01945161826927215\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5S2kLNgbfxU"
      },
      "source": [
        "state = {\n",
        "            'net': mymodel.model.state_dict(),\n",
        "            \n",
        "            'optimizer': optimizer,\n",
        "    }\n",
        "torch.save(state, 'checkpoint_prdim2.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdozCitGOM4a",
        "outputId": "66082ec8-5a19-46fd-bbc6-3f6b3fcbe33a"
      },
      "source": [
        "mymodel.remove_prune()\n",
        "mesure_operation(mymodel.model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "Not implemented for  MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "Not implemented for  MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "Not implemented for  MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "Not implemented for  MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "Not implemented for  MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "1\n",
            "tensor([0.9754])\n",
            "Conv2d: S_c=3, F_in=3, F_out=64, P=1024, params=874, operations=2589037\n",
            "Batch norm: F_in=64 P=1024, params=64, operations=196608\n",
            "ReLU: F_in=64 P=1024, params=0, operations=65536\n",
            "1\n",
            "tensor([0.9905])\n",
            "Conv2d: S_c=3, F_in=64, F_out=64, P=1024, params=18289, operations=56086436\n",
            "Batch norm: F_in=64 P=1024, params=64, operations=196608\n",
            "ReLU: F_in=64 P=1024, params=0, operations=65536\n",
            "1\n",
            "tensor([0.9905])\n",
            "Conv2d: S_c=3, F_in=64, F_out=128, P=256, params=36578, operations=28043218\n",
            "Batch norm: F_in=128 P=256, params=128, operations=98304\n",
            "ReLU: F_in=128 P=256, params=0, operations=32768\n",
            "1\n",
            "tensor([0.9896])\n",
            "Conv2d: S_c=3, F_in=128, F_out=128, P=256, params=73028, operations=56036864\n",
            "Batch norm: F_in=128 P=256, params=128, operations=98304\n",
            "ReLU: F_in=128 P=256, params=0, operations=32768\n",
            "1\n",
            "tensor([0.9842])\n",
            "Conv2d: S_c=3, F_in=128, F_out=256, P=64, params=145246, operations=27863046\n",
            "Batch norm: F_in=256 P=64, params=256, operations=49152\n",
            "ReLU: F_in=256 P=64, params=0, operations=16384\n",
            "1\n",
            "tensor([0.6525])\n",
            "Conv2d: S_c=3, F_in=256, F_out=256, P=64, params=192502, operations=36944348\n",
            "Batch norm: F_in=256 P=64, params=256, operations=49152\n",
            "ReLU: F_in=256 P=64, params=0, operations=16384\n",
            "1\n",
            "tensor([0.6525])\n",
            "Conv2d: S_c=3, F_in=256, F_out=256, P=64, params=192502, operations=36944348\n",
            "Batch norm: F_in=256 P=64, params=256, operations=49152\n",
            "ReLU: F_in=256 P=64, params=0, operations=16384\n",
            "1\n",
            "tensor([0.6525])\n",
            "Conv2d: S_c=3, F_in=256, F_out=512, P=16, params=385004, operations=18472174\n",
            "Batch norm: F_in=512 P=16, params=512, operations=24576\n",
            "ReLU: F_in=512 P=16, params=0, operations=8192\n",
            "1\n",
            "tensor([0.3263])\n",
            "Conv2d: S_c=3, F_in=512, F_out=512, P=16, params=385004, operations=18476182\n",
            "Batch norm: F_in=512 P=16, params=512, operations=24576\n",
            "ReLU: F_in=512 P=16, params=0, operations=8192\n",
            "1\n",
            "tensor([0.3263])\n",
            "Conv2d: S_c=3, F_in=512, F_out=512, P=16, params=385004, operations=18476182\n",
            "Batch norm: F_in=512 P=16, params=512, operations=24576\n",
            "ReLU: F_in=512 P=16, params=0, operations=8192\n",
            "1\n",
            "tensor([0.3263])\n",
            "Conv2d: S_c=3, F_in=512, F_out=512, P=4, params=385004, operations=4619045\n",
            "Batch norm: F_in=512 P=4, params=512, operations=6144\n",
            "ReLU: F_in=512 P=4, params=0, operations=2048\n",
            "1\n",
            "tensor([0.3263])\n",
            "Conv2d: S_c=3, F_in=512, F_out=512, P=4, params=385004, operations=4619045\n",
            "Batch norm: F_in=512 P=4, params=512, operations=6144\n",
            "ReLU: F_in=512 P=4, params=0, operations=2048\n",
            "1\n",
            "tensor([0.3263])\n",
            "Conv2d: S_c=3, F_in=512, F_out=512, P=4, params=385004, operations=4619045\n",
            "Batch norm: F_in=512 P=4, params=512, operations=6144\n",
            "ReLU: F_in=512 P=4, params=0, operations=2048\n",
            "AvgPool: S=1, F_in=512, P=1, params=0, operations=512\n",
            "512\n",
            "Linear: F_in=512, F_out=10, params=2565, operations=7670\n",
            "Flops: 314903072.0, Params: 2975832.0\n",
            "Score flops: 0.3347526736580854 Score Params: 0.20214223084696126\n",
            "Final score: 0.5368949045050466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhVxQlqZ8uzi"
      },
      "source": [
        "Nous avons ici divisé par 5 le nombre de paramètres, et le nombre d'opérations par 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA7xfPz59B_Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhU5o-_Qbjo6"
      },
      "source": [
        "## Prune sur la dim 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGw75EoT9jDG"
      },
      "source": [
        "**Nous avons essayés de réaliser la même expérience en élaguant sur la dimension 0 qui semblait être plus prometteuse. Nous avons aussi modifier notre procédé afin de ne pas retomber sur les problèmes précédents. Durant ce processus nous avons choisis de traiter notre réseau layer par layer : nous élaguons un layer au maximum, et si le résultats est satisfaisant, nous passons au layers suivant en gardant le modèle obtenu. Nous n'avons malheureusement pas eu le temps de finir le procédé.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8GO2dBIbjo7"
      },
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "import io\n",
        "data_int=False\n",
        "def prune_fine_tune(ind_module,ratio_conv=[0.3,0.6]):\n",
        "  \n",
        "  try: \n",
        "    loaded_cpt = torch.load('checkpoint_prdim2_temp.pt')\n",
        "\n",
        "  except :\n",
        "    \n",
        "    #loaded_cpt = torch.load('C:/Users/Malo/Desktop/Cours/Fise_A2/S4/DeepL/prunage de layers sur cifar 10/checkpoint.pt',map_location=torch.device('cpu'))\n",
        "    if True :\n",
        "\n",
        "\n",
        "      print(\"Chargez les poids : checkpoint_prdim2_temp.pt\")\n",
        "      files.upload()\n",
        "      loaded_cpt = torch.load('checkpoint_prdim2_temp.pt')\n",
        "\n",
        "  net = VGG('VGG16')\n",
        "  \n",
        "  if data_int :\n",
        "    net.half()\n",
        "  mymodel = my_network_with_trous(net)\n",
        "  mymodel.model = mymodel.model.to(device)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  list_modules=[ \"features.0\",\n",
        "              \"features.3\",\n",
        "              \"features.7\",\n",
        "              \"features.10\",\n",
        "              \"features.14\",\n",
        "              \"features.17\",\n",
        "              \"features.20\",\n",
        "              \"features.24\",\n",
        "              \"features.27\",\n",
        "              \"features.30\",\n",
        "              \"features.34\",\n",
        "              \"features.37\",\n",
        "              \"features.40\"]\n",
        "  module=list_modules[ind_module]\n",
        "  for ratios in ratio_conv :  \n",
        "    print(\"________________________________________________________\")\n",
        "    print(\"setting ratio to \",ratios)\n",
        "    \n",
        "    valid_loss,training_loss,test_accuracy=[],[],[]\n",
        "    \n",
        "\n",
        "    mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "\n",
        "    mymodel.model.load_state_dict(loaded_cpt[\"net\"])\n",
        "\n",
        "    paramconv1,paramfc1= get_number_param_pruned(mymodel.model)\n",
        "    mymodel.prune_spef_layer(module,ratios,2)\n",
        "    paramconv2,paramfc2=get_number_param_pruned(mymodel.model)\n",
        "    print(paramconv2,paramfc2)\n",
        "    print(\"parameters losed : \",sum([paramconv2,paramfc2])-sum([ paramconv1,paramfc1]))\n",
        "\n",
        "    print(mymodel.model)\n",
        "    # entrainement a\n",
        "\n",
        "    optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "    valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 1,criterion,optimizer,mymodel,scheduler=False ) \n",
        "    #if False :\n",
        "    if not max(test_accuracy)< 15 :\n",
        "      valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 5,criterion,optimizer,mymodel,scheduler=False ) \n",
        "    #entrainement b\n",
        "\n",
        "      optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "      valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,10,criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy,scheduler=False ) \n",
        "    else : \n",
        "      print(\"cut training because useless\")\n",
        "    print(\"On save (o) ou on coupe (n) ?\")\n",
        "    rep=str(input())\n",
        "    if rep==\"o\":\n",
        "      loaded_cpt = torch.load('temp.pt')\n",
        "\n",
        "\n",
        "      mymodel.model.load_state_dict(loaded_cpt[\"net\"])\n",
        "      state = {\n",
        "            'net': mymodel.model.state_dict(),\n",
        "    }\n",
        "      torch.save(state, 'checkpoint_prdim2_temp.pt')\n",
        "      #files.download('checkpoint_prdim2_temp.pt')\n",
        "\n",
        "    elif rep==\"n\" :\n",
        "      return None\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFwdFp1lbjpC"
      },
      "source": [
        "### Prune sur \"features.40\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJU2KHoHbjo8",
        "outputId": "e4f546ca-ae1f-4de8-a73b-8e7621eee0da"
      },
      "source": [
        "prune_fine_tune(12,[0.3,0.4,0.5,0.6,0.8,0.875])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "features.40\n",
            "2964819.0 510.0\n",
            "parameters losed :  0.0\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "epoch  0\n",
            "83.32  % ,  0.4176534989356995  ,  0.4912122260376811\n",
            "Finished Training\n",
            "epoch  0\n",
            "81.44  % ,  0.5020196811676025  ,  0.4113596220970154\n",
            "epoch  1\n",
            "83.71  % ,  0.41751556854248045  ,  0.38284370097517967\n",
            "epoch  2\n",
            "saving weights.... \n",
            "85.46  % ,  0.38854735486507413  ,  0.3702326376080513\n",
            "epoch  3\n",
            "85.26  % ,  0.4086020769119263  ,  0.35739539050757885\n",
            "epoch  4\n",
            "85.12  % ,  0.4162482996940613  ,  0.34322604635059833\n",
            "Finished Training\n",
            "epoch  0\n",
            "saving weights.... \n",
            "89.9  % ,  0.2339164553940296  ,  0.202037260556221\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.38  % ,  0.22937144258022307  ,  0.15962120521068573\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.21  % ,  0.22383170119822024  ,  0.13964514635801314\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.25  % ,  0.21665332035422324  ,  0.12849673682898283\n",
            "epoch  4\n",
            "90.46  % ,  0.22173845601119102  ,  0.11871602490209043\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.55  % ,  0.2099493674531579  ,  0.1073850129518658\n",
            "epoch  6\n",
            "90.75  % ,  0.22328228455334903  ,  0.10182813430055976\n",
            "epoch  7\n",
            "90.7  % ,  0.22289406037032605  ,  0.09587643986977637\n",
            "epoch  8\n",
            "90.86  % ,  0.2227204667881131  ,  0.09043679626584053\n",
            "epoch  9\n",
            "90.48  % ,  0.22312537263333798  ,  0.08618771670721471\n",
            "Finished Training\n",
            "On save (o) ou on coupe (n) ?\n",
            "o\n",
            "________________________________________________________\n",
            "setting ratio to  0.4\n",
            "Pruning....\n",
            "features.40\n",
            "2964819.0 510.0\n",
            "parameters losed :  0.0\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "epoch  0\n",
            "85.32  % ,  0.4072984797477722  ,  0.34074889763705435\n",
            "Finished Training\n",
            "epoch  0\n",
            "83.44  % ,  0.4808131821155548  ,  0.33897253058254717\n",
            "epoch  1\n",
            "84.85  % ,  0.4325557126045227  ,  0.33726727776825427\n",
            "epoch  2\n",
            "85.73  % ,  0.394953879070282  ,  0.32031984081864356\n",
            "epoch  3\n",
            "86.03  % ,  0.4001803243398666  ,  0.3267166309177876\n",
            "epoch  4\n",
            "85.83  % ,  0.4202938421010971  ,  0.3164667635470629\n",
            "Finished Training\n",
            "epoch  0\n",
            "90.08  % ,  0.26176387795805933  ,  0.18024784329757093\n",
            "epoch  1\n",
            "90.41  % ,  0.25124321111738684  ,  0.13883321701958776\n",
            "epoch  2\n",
            "90.84  % ,  0.254289955753088  ,  0.12447180647812783\n",
            "epoch  3\n",
            "90.64  % ,  0.24712965634465217  ,  0.11150071262717247\n",
            "epoch  4\n",
            "90.9  % ,  0.2546123436421156  ,  0.10129508256539703\n",
            "epoch  5\n",
            "90.86  % ,  0.24786310836672784  ,  0.0941374572072178\n",
            "epoch  6\n",
            "91.09  % ,  0.2500939901104197  ,  0.08810443241409957\n",
            "epoch  7\n",
            "91.24  % ,  0.25563143982887265  ,  0.08159392265062779\n",
            "epoch  8\n",
            "90.9  % ,  0.2572379448082298  ,  0.08002789600268007\n",
            "epoch  9\n",
            "90.92  % ,  0.2648420133829117  ,  0.07420947723016143\n",
            "Finished Training\n",
            "On save (o) ou on coupe (n) ?\n",
            "o\n",
            "________________________________________________________\n",
            "setting ratio to  0.5\n",
            "Pruning....\n",
            "features.40\n",
            "2964819.0 510.0\n",
            "parameters losed :  0.0\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "epoch  0\n",
            "83.39  % ,  0.46284153567552566  ,  0.3508827294766903\n",
            "Finished Training\n",
            "epoch  0\n",
            "83.73  % ,  0.4365009696960449  ,  0.33767396740019323\n",
            "epoch  1\n",
            "84.83  % ,  0.4177933972477913  ,  0.3340262473881245\n",
            "epoch  2\n",
            "84.48  % ,  0.4052013752102852  ,  0.3303326396912336\n",
            "epoch  3\n",
            "85.81  % ,  0.4284479075670242  ,  0.32309641278386114\n",
            "epoch  4\n",
            "84.92  % ,  0.42762585422992705  ,  0.31794909779131414\n",
            "Finished Training\n",
            "epoch  0\n",
            "90.43  % ,  0.2603222874045372  ,  0.18509121779799462\n",
            "epoch  1\n",
            "90.84  % ,  0.25175968417823313  ,  0.14092093401774763\n",
            "epoch  2\n",
            "90.89  % ,  0.24981371895968915  ,  0.12810005861967802\n",
            "epoch  3\n",
            "90.79  % ,  0.24784706023931502  ,  0.1159099480420351\n",
            "epoch  4\n",
            "91.04  % ,  0.25101308038830755  ,  0.1040072288621217\n",
            "epoch  5\n",
            "90.98  % ,  0.24058116775751115  ,  0.09769556768760085\n",
            "epoch  6\n",
            "91.21  % ,  0.24983339653909206  ,  0.09240019502341747\n",
            "epoch  7\n",
            "91.05  % ,  0.2529624315187335  ,  0.08149353977050633\n",
            "epoch  8\n",
            "90.89  % ,  0.24919776978939773  ,  0.0815610701488331\n",
            "epoch  9\n",
            "90.97  % ,  0.24862144336998462  ,  0.07407885169070214\n",
            "Finished Training\n",
            "On save (o) ou on coupe (n) ?\n",
            "o\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n",
            "features.40\n",
            "2580327.0 510.0\n",
            "parameters losed :  -384492.0\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "epoch  0\n",
            "10.0  % ,  2.303615251541138  ,  2.30554324798584\n",
            "Finished Training\n",
            "epoch  0\n",
            "10.0  % ,  2.305780228805542  ,  2.3051945487976075\n",
            "epoch  1\n",
            "10.0  % ,  2.3039008693695067  ,  2.304335276222229\n",
            "epoch  2\n",
            "10.0  % ,  2.303633000564575  ,  2.3040430000305174\n",
            "epoch  3\n",
            "10.0  % ,  2.304323560714722  ,  2.303891571998596\n",
            "epoch  4\n",
            "10.0  % ,  2.302937893295288  ,  2.303654242515564\n",
            "Finished Training\n",
            "epoch  0\n",
            "10.0  % ,  2.302747064971924  ,  2.3028787023544313\n",
            "epoch  1\n",
            "10.0  % ,  2.3028492527008058  ,  2.3027339780807496\n",
            "epoch  2\n",
            "10.0  % ,  2.3028319953918457  ,  2.3026997007369996\n",
            "epoch  3\n",
            "10.0  % ,  2.302846704864502  ,  2.30270281829834\n",
            "epoch  4\n",
            "10.0  % ,  2.3029636684417722  ,  2.3027042474746704\n",
            "epoch  5\n",
            "10.0  % ,  2.302805796813965  ,  2.302703360939026\n",
            "epoch  6\n",
            "10.0  % ,  2.3029790496826172  ,  2.302722593688965\n",
            "epoch  7\n",
            "10.0  % ,  2.303060250854492  ,  2.3026646852493284\n",
            "epoch  8\n",
            "10.0  % ,  2.3030253925323487  ,  2.302722145843506\n",
            "epoch  9\n",
            "10.0  % ,  2.3029450393676756  ,  2.3026975078582765\n",
            "Finished Training\n",
            "On save (o) ou on coupe (n) ?\n",
            "n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twSNrHW7bjpC"
      },
      "source": [
        "### Prune sur \"features.37\"\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "95ORqVuAbjpC",
        "outputId": "f055e08c-3c9d-4395-d346-de953803212c"
      },
      "source": [
        "prune_fine_tune(11,[0.3,0.6,0.8,0.875])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "________________________________________________________\n",
            "setting ratio to  0.3\n",
            "Pruning....\n",
            "features.37\n",
            "2964819.0 510.0\n",
            "parameters losed :  0.0\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "  )\n",
            "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n",
            "epoch  0\n",
            "82.6  % ,  0.5019384934663773  ,  0.34285695199519395\n",
            "Finished Training\n",
            "epoch  0\n",
            "84.29  % ,  0.4623634505033493  ,  0.343466997256875\n",
            "epoch  1\n",
            "83.6  % ,  0.45690388317108155  ,  0.33071158923208716\n",
            "epoch  2\n",
            "83.76  % ,  0.4879619478225708  ,  0.3247746751204133\n",
            "epoch  3\n",
            "85.19  % ,  0.43123689603805543  ,  0.32556002151668073\n",
            "epoch  4\n",
            "84.67  % ,  0.42904906985759733  ,  0.32251119320094584\n",
            "Finished Training\n",
            "epoch  0\n",
            "90.46  % ,  0.25517415249347686  ,  0.18544299018234014\n",
            "epoch  1\n",
            "90.87  % ,  0.2511878061413765  ,  0.13854646339714527\n",
            "epoch  2\n",
            "90.86  % ,  0.24234839776158332  ,  0.12711138919480144\n",
            "epoch  3\n",
            "90.89  % ,  0.2522037115037441  ,  0.11216986608318985\n",
            "epoch  4\n",
            "90.86  % ,  0.24413868901729585  ,  0.09977037653476\n",
            "epoch  5\n",
            "90.95  % ,  0.24818986081779004  ,  0.09777721897810697\n",
            "epoch  6\n",
            "91.01  % ,  0.24647046446502208  ,  0.08961605957392603\n",
            "epoch  7\n",
            "91.31  % ,  0.24837813991308214  ,  0.08502617317233235\n",
            "epoch  8\n",
            "90.93  % ,  0.2500235768333077  ,  0.076625944407098\n",
            "epoch  9\n",
            "91.41  % ,  0.2506550777003169  ,  0.07569522731006145\n",
            "Finished Training\n",
            "On save (o) ou on coupe (n) ?\n",
            "o\n",
            "________________________________________________________\n",
            "setting ratio to  0.6\n",
            "Pruning....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-7d46dd6b2da9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprune_fine_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.875\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-384077e32d9c>\u001b[0m in \u001b[0;36mprune_fine_tune\u001b[0;34m(ind_module, ratio_conv)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mmymodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_all_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"fc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"conv\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dim\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmymodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_cpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mparamconv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparamfc1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mget_number_param_pruned\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmymodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1224\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VGG:\n\tMissing key(s) in state_dict: \"features.0.bias\", \"features.0.weight_orig\", \"features.0.weight_mask\", \"features.1.weight\", \"features.1.bias\", \"features.1.running_mean\", \"features.1.running_var\", \"features.3.bias\", \"features.3.weight_orig\", \"features.3.weight_mask\", \"features.4.weight\", \"features.4.bias\", \"features.4.running_mean\", \"features.4.running_var\", \"features.7.bias\", \"features.7.weight_orig\", \"features.7.weight_mask\", \"features.8.weight\", \"features.8.bias\", \"features.8.running_mean\", \"features.8.running_var\", \"features.10.bias\", \"features.10.weight_orig\", \"features.10.weight_mask\", \"features.11.weight\", \"features.11.bias\", \"features.11.running_mean\", \"features.11.running_var\", \"features.14.bias\", \"features.14.weight_orig\", \"features.14.weight_mask\", \"features.15.weight\", \"features.15.bias\", \"features.15.running_mean\", \"features.15.running_var\", \"features.17.bias\", \"features.17.weight_orig\", \"features.17.weight_mask\", \"features.18.weight\", \"features.18.bias\", \"features.18.running_mean\", \"features.18.running_var\", \"features.20.bias\", \"features.20.weight_orig\", \"features.20.weight_mask\", \"features.21.weight\", \"features.21.bias\", \"features.21.running_mean\", \"features.21.running_var\", \"features.24.bias\", \"features.24.weight_orig\", \"features.24.weight_mask\", \"features.25.weight\", \"features.25.bias\", \"features.25.running_mean\", \"features.25.running_var\", \"features.27.bias\", \"features.27.weight_orig\", \"features.27.weight_mask\", \"features.28.weight\", \"features.28.bias\", \"fe...\n\tUnexpected key(s) in state_dict: \"net\". "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLVmTjbh9CCK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgdNipd79CE2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4rcSSAO9CHr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woxjzWPU9CKO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDRw2kbP_BCj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oul7SHSRqJN"
      },
      "source": [
        "# Quantization et mesure des paramètres de différents réseaux.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9flf3fNtBDRG"
      },
      "source": [
        "**Ce code ci correspond à l'étude que j'ai réalisé pour Maxence afin de mesurer le nombre de paramètres des réseaux obtenus à travers l'élagage de mobile net. Nous avons aussi essayé de rajouter de la quantization à nos réseaux afin d'optimiser encore plus leur taille.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCMWTpyZT-JL"
      },
      "source": [
        "## class BC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxalpNnHByQ5"
      },
      "source": [
        "**Cette classe BC ( Binary Connect) a pour but de transformer les poids du réseau en poids binaires**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mfWFz9NSEYf"
      },
      "source": [
        "### See http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-b\n",
        "### for a complete description of the algotihm \n",
        "\n",
        "\n",
        "#  \n",
        "import torch.nn as nn\n",
        "import numpy\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class BC():\n",
        "    def __init__(self, model):\n",
        "\n",
        "        # First we need to \n",
        "        # count the number of Conv2d and Linear\n",
        "        # This will be used next in order to build a list of all \n",
        "        # parameters of the model \n",
        "\n",
        "        count_targets = 0\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                count_targets = count_targets + 1\n",
        "\n",
        "        start_range = 0\n",
        "        end_range = count_targets-1\n",
        "        self.bin_range = numpy.linspace(start_range,\n",
        "                end_range, end_range-start_range+1)\\\n",
        "                        .astype('int').tolist()\n",
        "\n",
        "        # Now we can initialize the list of parameters\n",
        "\n",
        "        self.num_of_params = len(self.bin_range)\n",
        "        self.saved_params = [] # This will be used to save the full precision weights\n",
        "        \n",
        "        self.target_modules = [] # this will contain the list of modules to be modified\n",
        "\n",
        "        self.model = model # this contains the model that will be trained and quantified\n",
        "\n",
        "        ### This builds the initial copy of all parameters and target modules\n",
        "        index = -1\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                index = index + 1\n",
        "                if index in self.bin_range:\n",
        "                    tmp = m.weight.data.clone()\n",
        "                    self.saved_params.append(tmp)\n",
        "                    self.target_modules.append(m.weight)\n",
        "\n",
        "\n",
        "    def save_params(self):\n",
        "\n",
        "        ### This loop goes through the list of target modules, and saves the corresponding weights into the list of saved_parameters\n",
        "\n",
        "        for index in range(self.num_of_params):\n",
        "            self.saved_params[index].copy_(self.target_modules[index].data)\n",
        "            \n",
        "\n",
        "    def binarization(self):\n",
        "\n",
        "        ### To be completed\n",
        "\n",
        "        ### (1) Save the current full precision parameters using the save_params method\n",
        "        self.save_params()\n",
        "\n",
        "        \n",
        "        \n",
        "        ### (2) Binarize the weights in the model, by iterating through the list of target modules and overwrite the values with their binary version\n",
        "        #for i in self.target_modules :\n",
        "          #print(i)\n",
        "          #i=torch.div(i,torch.abs(i))\n",
        "          #current_torch_tensor=self.model[i].weight.data\n",
        "          #new_binary_weights=torch.div(current_torch_tensor,torch.abs(current_torch_tensor))\n",
        "          #self.model[i].weight.data.copy_(new_binary_weights)\n",
        "          # Rentrer les poids dans le réseau ?\n",
        "\n",
        "        for index in range(self.num_of_params):\n",
        "            #self.target_modules[index].data.copy_(torch.where(self.target_modules[index]>0, 1, -1))\n",
        "            self.target_modules[index].data.copy_(self.target_modules[index].data.sign())\n",
        "        #J'ai une erreur c'est la même chose que ce que t'avais fais au dessus, d'ailleurs je suis chaud on demande à un prof dans la journée de\n",
        "        #vendredi\n",
        "        #for module in range(self.num_of_params) :\n",
        "        #  self.model[module].weight.data.copy_(torch.div(self.model.module.weight.data,abs(self.model.module.weight.data)))\n",
        "            \n",
        "          \n",
        "          \n",
        "        \n",
        "\n",
        "    def restore(self):\n",
        "\n",
        "        ### restore the copy from self.saved_params into the model \n",
        "\n",
        "        for index in range(self.num_of_params):\n",
        "            self.target_modules[index].data.copy_(self.saved_params[index])\n",
        "      \n",
        "    def clip(self):\n",
        "\n",
        "        ## To be completed \n",
        "        ## Clip all parameters to the range [-1,1] using Hard Tanh \n",
        "        ## you can use the nn.Hardtanh function\n",
        "        clip_scale=[]\n",
        "        m=nn.Hardtanh(-1, 1)\n",
        "        for index in range(self.num_of_params):\n",
        "            clip_scale.append(m(Variable(self.target_modules[index].data)))\n",
        "        for index in range(self.num_of_params):\n",
        "            self.target_modules[index].data.copy_(clip_scale[index].data)\n",
        "            #a=self.target_modules[index].data            \n",
        "            #self.target_modules[index].data.copy_(hth(a))\n",
        "        #self.model.weight.data.copy_(nn.Hardtanh(model.weight.data))\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        ### This function is used so that the model can be used while training\n",
        "        out = self.model(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SRVMoRcUWz7"
      },
      "source": [
        "## training func BC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hudpGJ8wRwdQ"
      },
      "source": [
        "\n",
        "\n",
        "def trainingwithBC(trainloader,validloader,testloader,n_epochs,criterion,optimizer,mymodel,valid_losses=[],training_losses=[],test_accuracys=[],scheduler=True) : \n",
        "  \n",
        "  \n",
        "  mymodelbc = BC(net)\n",
        "  mymodelbc.model = mymodelbc.model.to(device)\n",
        "  \n",
        "  \n",
        "  for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "      train_loss,valid_loss=0,0\n",
        "      \n",
        "      print(\"epoch \",epoch)\n",
        "\n",
        "      running_loss = 0.0\n",
        "      mymodelbc.model.train()\n",
        "\n",
        "      for data,label in trainloader:\n",
        "          \n",
        "          if data_int :\n",
        "              data=data.half()\n",
        "          # zero the parameter gradients\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          mymodelbc.binarization()\n",
        "          #outputs = net(inputs.to(device))\n",
        "          outputs = mymodelbc.forward(data.to(device))\n",
        "          #print(\"outputs\",outputs)\n",
        "          \n",
        "          #print(\"labels\",labels)\n",
        "          loss = criterion(outputs, label.to(device))\n",
        "\n",
        "          loss.backward()\n",
        "          mymodelbc.restore()\n",
        "\n",
        "          optimizer.step()\n",
        "          mymodelbc.clip()\n",
        "          # print statistics\n",
        "          train_loss += loss.item()*data.size(0)\n",
        "\n",
        "          \n",
        "      mymodelbc.model.eval()\n",
        "      for data,label in validloader:\n",
        "\n",
        "\n",
        "          if data_int :\n",
        "              data=data.half()\n",
        "          \n",
        "          outputs = mymodelbc.forward(data.to(device))\n",
        "\n",
        "          loss = criterion(outputs, label.to(device))\n",
        "\n",
        "          # print statistics\n",
        "          valid_loss += loss.item()*data.size(0)\n",
        "\n",
        "      train_loss /= len(trainloader.sampler)\n",
        "      valid_loss /= len(validloader.sampler)\n",
        "      training_losses.append(train_loss)\n",
        "     \n",
        "\n",
        "      #lr_scheduler.step(running_loss/i)        \n",
        "      valid_losses.append(valid_loss)\n",
        "      \n",
        "\n",
        "      if valid_loss==min(valid_losses) :\n",
        "        print(\"saving weights.... \")\n",
        "        state = {'net': mymodelbc.model.state_dict()}\n",
        "\n",
        "\n",
        "        torch.save(state, 'temp.pt')\n",
        "      \n",
        "      test_accuracy=eval_accuracy(mymodelbc.model,testloader )\n",
        "\n",
        "      print(test_accuracy,\" % , \",valid_losses[-1],\" , \",training_losses[-1])\n",
        "      test_accuracys.append(test_accuracy)\n",
        "\n",
        "  #print(training_losses)\n",
        "  print('Finished Training')\n",
        "  return valid_losses,training_losses, test_accuracys, mymodelbc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhsA6_0lW-eL"
      },
      "source": [
        "## Mobile Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VOYTHDBCAFj"
      },
      "source": [
        "Ici on met en place mobile net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPP0ny-KW6O8"
      },
      "source": [
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BaseBlock(nn.Module):\n",
        "    alpha = 1\n",
        "\n",
        "    def __init__(self, input_channel, output_channel, t = 6, downsample = False):\n",
        "        \"\"\"\n",
        "            t:  expansion factor, t*input_channel is channel of expansion layer\n",
        "            alpha:  width multiplier, to get thinner models\n",
        "            rho:    resolution multiplier, to get reduced representation\n",
        "        \"\"\" \n",
        "        super(BaseBlock, self).__init__()\n",
        "        self.stride = 2 if downsample else 1\n",
        "        self.downsample = downsample\n",
        "        self.shortcut = (not downsample) and (input_channel == output_channel) \n",
        "\n",
        "        # apply alpha\n",
        "        input_channel = int(self.alpha * input_channel)\n",
        "        output_channel = int(self.alpha * output_channel)\n",
        "        \n",
        "        # for main path:\n",
        "        c  = t * input_channel\n",
        "        # 1x1   point wise conv\n",
        "        self.conv1 = nn.Conv2d(input_channel, c, kernel_size = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(c)\n",
        "        # 3x3   depth wise conv\n",
        "        self.conv2 = nn.Conv2d(c, c, kernel_size = 3, stride = self.stride, padding = 1, groups = c, bias = False)\n",
        "        self.bn2 = nn.BatchNorm2d(c)\n",
        "        # 1x1   point wise conv\n",
        "        self.conv3 = nn.Conv2d(c, output_channel, kernel_size = 1, bias = False)\n",
        "        self.bn3 = nn.BatchNorm2d(output_channel)\n",
        "        \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # main path\n",
        "        x = F.relu6(self.bn1(self.conv1(inputs)), inplace = True)\n",
        "        x = F.relu6(self.bn2(self.conv2(x)), inplace = True)\n",
        "        x = self.bn3(self.conv3(x))\n",
        "\n",
        "        # shortcut path\n",
        "        x = x + inputs if self.shortcut else x\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(self, output_size, alpha = 1):\n",
        "        super(MobileNetV2, self).__init__()\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # first conv layer \n",
        "        self.conv0 = nn.Conv2d(3, int(32*alpha), kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.bn0 = nn.BatchNorm2d(int(32*alpha))\n",
        "\n",
        "        # build bottlenecks\n",
        "        BaseBlock.alpha = alpha\n",
        "        self.bottlenecks = nn.Sequential(\n",
        "            BaseBlock(32, 16, t = 1, downsample = False),\n",
        "            BaseBlock(16, 24, downsample = False),\n",
        "            BaseBlock(24, 24),\n",
        "            BaseBlock(24, 32, downsample = False),\n",
        "            BaseBlock(32, 32),\n",
        "            BaseBlock(32, 32),\n",
        "            BaseBlock(32, 64, downsample = True),\n",
        "            BaseBlock(64, 64),\n",
        "            BaseBlock(64, 64),\n",
        "            BaseBlock(64, 64),\n",
        "            BaseBlock(64, 96, downsample = False),\n",
        "            BaseBlock(96, 96),\n",
        "            BaseBlock(96, 96),\n",
        "            BaseBlock(96, 160, downsample = True),\n",
        "            BaseBlock(160, 160),\n",
        "            BaseBlock(160, 160),\n",
        "            BaseBlock(160, 320, downsample = False))\n",
        "\n",
        "        # last conv layers and fc layer\n",
        "        self.conv1 = nn.Conv2d(int(320*alpha), 1280, kernel_size = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(1280)\n",
        "        self.fc = nn.Linear(1280, output_size)\n",
        "\n",
        "        # weights init\n",
        "        self.weights_init()\n",
        "\n",
        "\n",
        "    def weights_init(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        # first conv layer\n",
        "        x = F.relu6(self.bn0(self.conv0(inputs)), inplace = True)\n",
        "        # assert x.shape[1:] == torch.Size([32, 32, 32])\n",
        "\n",
        "        # bottlenecks\n",
        "        x = self.bottlenecks(x)\n",
        "        # assert x.shape[1:] == torch.Size([320, 8, 8])\n",
        "\n",
        "        # last conv layer\n",
        "        x = F.relu6(self.bn1(self.conv1(x)), inplace = True)\n",
        "        # assert x.shape[1:] == torch.Size([1280,8,8])\n",
        "\n",
        "        # global pooling and fc (in place of conv 1x1 in paper)\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC-ocrUWUaQd"
      },
      "source": [
        "## Calculs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shQHAPqtRwfv",
        "outputId": "6431cdc6-6a93-4e8d-9d5b-2b6293127951"
      },
      "source": [
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodeltrous = my_network_with_trous(net)\n",
        "mymodeltrous.model = mymodeltrous.model.to(device)\n",
        "\n",
        "#mymodel.remove_prune()\n",
        "mesure_operation(mymodeltrous.model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=3, F_out=32, P=1024, params=864, operations=1736704\n",
            "Batch norm: F_in=32 P=1024, params=64, operations=98304\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=32, F_out=32, P=1024, params=1024, operations=2064384\n",
            "Batch norm: F_in=32 P=1024, params=64, operations=98304\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=32, F_out=32, P=1024, params=288, operations=18841600\n",
            "Batch norm: F_in=32 P=1024, params=64, operations=98304\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=32, F_out=16, P=1024, params=512, operations=1032192\n",
            "Batch norm: F_in=16 P=1024, params=32, operations=49152\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=16, F_out=96, P=1024, params=1536, operations=3047424\n",
            "Batch norm: F_in=96 P=1024, params=192, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=96, F_out=96, P=1024, params=864, operations=169771008\n",
            "Batch norm: F_in=96 P=1024, params=192, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=96, F_out=24, P=1024, params=2304, operations=4694016\n",
            "Batch norm: F_in=24 P=1024, params=48, operations=73728\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=3456, operations=6930432\n",
            "Batch norm: F_in=144 P=1024, params=288, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=1296, operations=382058496\n",
            "Batch norm: F_in=144 P=1024, params=288, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=144, F_out=24, P=1024, params=3456, operations=7053312\n",
            "Batch norm: F_in=24 P=1024, params=48, operations=73728\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=3456, operations=6930432\n",
            "Batch norm: F_in=144 P=1024, params=288, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=1296, operations=382058496\n",
            "Batch norm: F_in=144 P=1024, params=288, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=144, F_out=32, P=1024, params=4608, operations=9404416\n",
            "Batch norm: F_in=32 P=1024, params=64, operations=98304\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=6144, operations=12386304\n",
            "Batch norm: F_in=192 P=1024, params=384, operations=589824\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=1728, operations=679280640\n",
            "Batch norm: F_in=192 P=1024, params=384, operations=589824\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=6144, operations=12550144\n",
            "Batch norm: F_in=32 P=1024, params=64, operations=98304\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=6144, operations=12386304\n",
            "Batch norm: F_in=192 P=1024, params=384, operations=589824\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=1728, operations=679280640\n",
            "Batch norm: F_in=192 P=1024, params=384, operations=589824\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=6144, operations=12550144\n",
            "Batch norm: F_in=32 P=1024, params=64, operations=98304\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=6144, operations=12386304\n",
            "Batch norm: F_in=192 P=1024, params=384, operations=589824\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=1728, operations=169820160\n",
            "Batch norm: F_in=192 P=256, params=384, operations=147456\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=192, F_out=64, P=256, params=12288, operations=6275072\n",
            "Batch norm: F_in=64 P=256, params=128, operations=49152\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=24576, operations=12484608\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=3456, operations=679378944\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=24576, operations=12566528\n",
            "Batch norm: F_in=64 P=256, params=128, operations=49152\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=24576, operations=12484608\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=3456, operations=679378944\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=24576, operations=12566528\n",
            "Batch norm: F_in=64 P=256, params=128, operations=49152\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=24576, operations=12484608\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=3456, operations=679378944\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=24576, operations=12566528\n",
            "Batch norm: F_in=64 P=256, params=128, operations=49152\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=24576, operations=12484608\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=3456, operations=679378944\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=384, F_out=96, P=256, params=36864, operations=18849792\n",
            "Batch norm: F_in=96 P=256, params=192, operations=73728\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=55296, operations=28164096\n",
            "Batch norm: F_in=576 P=256, params=1152, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=5184, operations=1528676352\n",
            "Batch norm: F_in=576 P=256, params=1152, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=55296, operations=28286976\n",
            "Batch norm: F_in=96 P=256, params=192, operations=73728\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=55296, operations=28164096\n",
            "Batch norm: F_in=576 P=256, params=1152, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=5184, operations=1528676352\n",
            "Batch norm: F_in=576 P=256, params=1152, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=55296, operations=28286976\n",
            "Batch norm: F_in=96 P=256, params=192, operations=73728\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=55296, operations=28164096\n",
            "Batch norm: F_in=576 P=256, params=1152, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=5184, operations=382169088\n",
            "Batch norm: F_in=576 P=64, params=1152, operations=110592\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=576, F_out=160, P=64, params=92160, operations=11786240\n",
            "Batch norm: F_in=160 P=64, params=320, operations=30720\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=153600, operations=19599360\n",
            "Batch norm: F_in=960 P=64, params=1920, operations=184320\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=8640, operations=1061621760\n",
            "Batch norm: F_in=960 P=64, params=1920, operations=184320\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=153600, operations=19650560\n",
            "Batch norm: F_in=160 P=64, params=320, operations=30720\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=153600, operations=19599360\n",
            "Batch norm: F_in=960 P=64, params=1920, operations=184320\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=8640, operations=1061621760\n",
            "Batch norm: F_in=960 P=64, params=1920, operations=184320\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=153600, operations=19650560\n",
            "Batch norm: F_in=160 P=64, params=320, operations=30720\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=153600, operations=19599360\n",
            "Batch norm: F_in=960 P=64, params=1920, operations=184320\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=8640, operations=1061621760\n",
            "Batch norm: F_in=960 P=64, params=1920, operations=184320\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=960, F_out=320, P=64, params=307200, operations=39301120\n",
            "Batch norm: F_in=320 P=64, params=640, operations=61440\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=320, F_out=1280, P=64, params=409600, operations=52346880\n",
            "Batch norm: F_in=1280 P=64, params=2560, operations=245760\n",
            "1280\n",
            "Linear: F_in=1280, F_out=10, params=25620, operations=38380\n",
            "Flops: 12396413952.0, Params: 2250580.0\n",
            "Score flops: 14.857341151130788 Score Params: 0.4028257837282783\n",
            "Final score: 15.260166934859067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GJFeQjFCQjm"
      },
      "source": [
        "**La fonction profile ne semble pas bien marcher avec notre fonction profile. Nous avons donc calculé à la main le nombre de paramètres et d'opérations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwt7huxQeNFe",
        "outputId": "62cbd1a4-72dd-4f47-e775-2cb4a0c0cd63"
      },
      "source": [
        "paramconv2,paramfc2=get_number_param_pruned(mymodeltrous.model)\n",
        "print(paramconv2,paramfc2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12383528960.0 38380.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "SJ6noFWaRwm6",
        "outputId": "52314221-2df8-423b-bc2e-ff22098d7705"
      },
      "source": [
        "data_int=False\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('mnet_cifar10_kd_45k.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "paramconv2,paramfc2=get_number_param_pruned(mymodel.model)\n",
        "print(paramconv2,paramfc2)\n",
        "print(\"_____________________________\")\n",
        "mymodel.remove_prune()\n",
        "mesure_operation(mymodel.model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-627f5749b60a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmymodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune_all_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"fc\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"conv\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"dim\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmymodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnet_cifar10_kd_45k.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mparamconv2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparamfc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_number_param_pruned\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmymodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'mnet_cifar10_kd_45k.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgYj-4G4_QLz",
        "outputId": "0463aae7-d537-40aa-e2ed-5f83c33c2bff"
      },
      "source": [
        "data_int=False\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "net.load_state_dict(torch.load('mnet_cifar10_kd_45k.pt'))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V69BOo8g_QO9",
        "outputId": "89fc7346-b3a3-45ec-9148-9a27ac753ce4"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss, test_accuracys,mymodelbc =trainingwithBC(c10trainloader,c10validloader,c10testloader,10,criterion,optimizer,mymodel.model,scheduler=True)\n",
        "\n",
        "#c10trainloader,c10validloader,c10testloader, 5,criterion,optimizer,mymodel,scheduler=False\n",
        "\n",
        "#valid_loss,training_loss = trainingwithBC(trainloader,validloader,testloader,n_epochs,criterion,optimizer,mymodel,valid_losses=[],training_losses=[],test_accuracys=[],scheduler=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "saving weights.... \n",
            "72.88  % ,  0.7822838376998902  ,  0.9869044678688049\n",
            "epoch  1\n",
            "saving weights.... \n",
            "73.82  % ,  0.7524137049198151  ,  0.765453828883171\n",
            "epoch  2\n",
            "saving weights.... \n",
            "76.59  % ,  0.6903407567977905  ,  0.7132915166378021\n",
            "epoch  3\n",
            "73.96  % ,  0.7751262371063232  ,  0.6784958131432534\n",
            "epoch  4\n",
            "75.57  % ,  0.7102547716379166  ,  0.6654510382413864\n",
            "epoch  5\n",
            "75.95  % ,  0.7130450839042664  ,  0.6480537532329559\n",
            "epoch  6\n",
            "saving weights.... \n",
            "78.3  % ,  0.6415948032379151  ,  0.6328354204416275\n",
            "epoch  7\n",
            "77.59  % ,  0.666321591091156  ,  0.6202680255770683\n",
            "epoch  8\n",
            "76.9  % ,  0.6593455308914185  ,  0.6202920879483222\n",
            "epoch  9\n",
            "74.37  % ,  0.7066953675746918  ,  0.6117789582133293\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwVT7aMSs8r_",
        "outputId": "e311558a-a729-47bc-b930-997043e95088"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss, test_accuracys,mymodelbc =trainingwithBC(c10trainloader,c10validloader,c10testloader,20,criterion,optimizer,mymodel.model,scheduler=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "saving weights.... \n",
            "83.71  % ,  0.4738486004829407  ,  0.5123840938329697\n",
            "epoch  1\n",
            "saving weights.... \n",
            "84.07  % ,  0.4545268725395203  ,  0.4767132646083832\n",
            "epoch  2\n",
            "saving weights.... \n",
            "84.27  % ,  0.45004876613616945  ,  0.4630650124490261\n",
            "epoch  3\n",
            "saving weights.... \n",
            "84.16  % ,  0.44247387070655825  ,  0.4566440619289875\n",
            "epoch  4\n",
            "84.5  % ,  0.4457044666290283  ,  0.44601684743762016\n",
            "epoch  5\n",
            "saving weights.... \n",
            "84.74  % ,  0.44216122627258303  ,  0.4379204518556595\n",
            "epoch  6\n",
            "saving weights.... \n",
            "84.75  % ,  0.44173378405570984  ,  0.43792219467163085\n",
            "epoch  7\n",
            "saving weights.... \n",
            "84.87  % ,  0.43764058656692506  ,  0.4338350251555443\n",
            "epoch  8\n",
            "84.81  % ,  0.43778069727420804  ,  0.4279998685121536\n",
            "epoch  9\n",
            "saving weights.... \n",
            "85.15  % ,  0.4298067883014679  ,  0.42163462193012236\n",
            "epoch  10\n",
            "84.84  % ,  0.43408133068084714  ,  0.4212979472666979\n",
            "epoch  11\n",
            "84.48  % ,  0.43332903821468355  ,  0.41755607865452765\n",
            "epoch  12\n",
            "saving weights.... \n",
            "84.92  % ,  0.42847662844657897  ,  0.4193581970334053\n",
            "epoch  13\n",
            "84.78  % ,  0.43101739897727964  ,  0.4115982505619526\n",
            "epoch  14\n",
            "84.87  % ,  0.42856625938415527  ,  0.41509912001490595\n",
            "epoch  15\n",
            "85.17  % ,  0.4330684029102325  ,  0.4089423875153065\n",
            "epoch  16\n",
            "85.15  % ,  0.43883541350364685  ,  0.40610577748417853\n",
            "epoch  17\n",
            "saving weights.... \n",
            "85.55  % ,  0.4218561217069626  ,  0.405791447943449\n",
            "epoch  18\n",
            "85.14  % ,  0.43173322291374205  ,  0.4087063552737236\n",
            "epoch  19\n",
            "85.09  % ,  0.42652653069496155  ,  0.39763786203861234\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAbY5qjt7LuU",
        "outputId": "ac2bb57c-48bf-496f-ee46-d76fad1d8b59"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.0001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss, test_accuracys,mymodelbc =trainingwithBC(c10trainloader,c10validloader,c10testloader,20,criterion,optimizer,mymodel.model,scheduler=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "saving weights.... \n",
            "85.6  % ,  0.41475571715831755  ,  0.3807285984933376\n",
            "epoch  1\n",
            "85.88  % ,  0.42202863163948057  ,  0.3747621378779411\n",
            "epoch  2\n",
            "saving weights.... \n",
            "85.7  % ,  0.40774750497341156  ,  0.3756208316862583\n",
            "epoch  3\n",
            "85.78  % ,  0.4120748637020588  ,  0.37337535467147825\n",
            "epoch  4\n",
            "saving weights.... \n",
            "85.75  % ,  0.405747878241539  ,  0.3664504172325134\n",
            "epoch  5\n",
            "85.81  % ,  0.4090512107133865  ,  0.3736821874260902\n",
            "epoch  6\n",
            "85.91  % ,  0.4131540148735046  ,  0.368728474843502\n",
            "epoch  7\n",
            "85.73  % ,  0.40760580666065216  ,  0.3680179129362106\n",
            "epoch  8\n",
            "85.91  % ,  0.4081586042642593  ,  0.3676912004947662\n",
            "epoch  9\n",
            "85.7  % ,  0.4068161660194397  ,  0.36449916853010655\n",
            "epoch  10\n",
            "86.06  % ,  0.41234762091636656  ,  0.36458342278599737\n",
            "epoch  11\n",
            "85.7  % ,  0.411593772482872  ,  0.3600827035307884\n",
            "epoch  12\n",
            "saving weights.... \n",
            "85.7  % ,  0.40499977898597717  ,  0.3638759257912636\n",
            "epoch  13\n",
            "saving weights.... \n",
            "85.76  % ,  0.40333805665969846  ,  0.3637527884632349\n",
            "epoch  14\n",
            "85.74  % ,  0.40847361558675765  ,  0.3626600576877594\n",
            "epoch  15\n",
            "85.99  % ,  0.407982008767128  ,  0.36310999971032143\n",
            "epoch  16\n",
            "85.62  % ,  0.4124817607164383  ,  0.36571626816391944\n",
            "epoch  17\n",
            "85.66  % ,  0.4092447475194931  ,  0.36345777394175527\n",
            "epoch  18\n",
            "85.92  % ,  0.4110598439455032  ,  0.36048928324878216\n",
            "epoch  19\n",
            "85.63  % ,  0.4164539902687073  ,  0.3620607698619366\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD6C4A6rYeA_",
        "outputId": "cd69fd42-fcbd-4b12-caac-887544884822"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.00005, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss, test_accuracys,mymodelbc =trainingwithBC(c10trainloader,c10validloader,c10testloader,1,criterion,optimizer,mymodel.model,scheduler=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "saving weights.... \n",
            "85.97  % ,  0.4078996299028397  ,  0.35855594984889033\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_dF2m-LVk0m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "nXBwzArdVlEV",
        "outputId": "6334f2d3-e68d-473a-8934-277c2f878c35"
      },
      "source": [
        "mymodelbc.restore()\n",
        "mymodelbc.save_params()\n",
        "mymodelbc.binarization() ## This binarizes all weights in the model\n",
        "mymodelbc.clip()\n",
        "\n",
        "test_accuracy=eval_accuracy(mymodelbc.model,testloader )\n",
        "print(test_accuracy)\n",
        "#\n",
        "\n",
        "\"\"\"\n",
        "  for name, module in model.named_modules():\n",
        "    \n",
        "\n",
        "      l=list(module.named_buffers())\n",
        "\n",
        "      total_conv+=torch.sum(l[0][1].type(torch.DoubleTensor)).item()\n",
        "      #print(len(l[0][1]))\n",
        "       \n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "\n",
        "      l=list(module.named_buffers())\n",
        "      #print(torch.sum(l[0][1]).item())\n",
        "\n",
        "      total_fc+=torch.sum(l[0][1]).item()\n",
        "\n",
        "pp=0\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65.225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n  for name, module in model.named_modules():\\n    \\n\\n      l=list(module.named_buffers())\\n\\n      total_conv+=torch.sum(l[0][1].type(torch.DoubleTensor)).item()\\n      #print(len(l[0][1]))\\n       \\n    elif isinstance(module, torch.nn.Linear):\\n\\n      l=list(module.named_buffers())\\n      #print(torch.sum(l[0][1]).item())\\n\\n      total_fc+=torch.sum(l[0][1]).item()\\n\\npp=0\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ximFu0Gpajdd",
        "outputId": "3f08b439-b91b-4245-e53f-55f3dd048e31"
      },
      "source": [
        "\n",
        "\n",
        "for name, module in mymodelbc.model.named_modules():\n",
        "  print(name)\n",
        "  if isinstance(module, torch.nn.Conv2d):\n",
        "    l=list(module.named_parameters())\n",
        "    #total_conv+=torch.sum(l[0][1].type(torch.DoubleTensor)).item()\n",
        "    print(l[0][1])\n",
        "    a=torch.where(l[0][1] > 0.8, 1., 0.)\n",
        "    #print(torch.sum(a))\n",
        "   \n",
        "    #print(torch.sum(l[0][1]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "\n",
            "         [[-1.3242e-11]],\n",
            "\n",
            "         [[-2.1880e-06]],\n",
            "\n",
            "         [[ 4.1464e-11]],\n",
            "\n",
            "         [[-3.9289e-11]],\n",
            "\n",
            "         [[ 3.5524e-11]],\n",
            "\n",
            "         [[-5.0804e-08]],\n",
            "\n",
            "         [[ 7.1862e-07]],\n",
            "\n",
            "         [[-1.2582e-11]],\n",
            "\n",
            "         [[ 1.0913e-11]],\n",
            "\n",
            "         [[-1.1381e-04]],\n",
            "\n",
            "         [[ 2.0323e-03]],\n",
            "\n",
            "         [[-1.5678e-09]],\n",
            "\n",
            "         [[-1.9779e-12]],\n",
            "\n",
            "         [[ 1.1849e-06]]],\n",
            "\n",
            "\n",
            "        [[[-2.8542e-11]],\n",
            "\n",
            "         [[-9.4109e-10]],\n",
            "\n",
            "         [[ 7.3518e-08]],\n",
            "\n",
            "         [[ 2.9445e-11]],\n",
            "\n",
            "         [[ 1.9720e-11]],\n",
            "\n",
            "         [[-3.1382e-11]],\n",
            "\n",
            "         [[-2.1551e-08]],\n",
            "\n",
            "         [[-5.3991e-09]],\n",
            "\n",
            "         [[ 1.7716e-08]],\n",
            "\n",
            "         [[-7.7827e-12]],\n",
            "\n",
            "         [[-2.6924e-08]],\n",
            "\n",
            "         [[-1.0180e-08]],\n",
            "\n",
            "         [[ 8.9153e-12]],\n",
            "\n",
            "         [[ 2.5994e-08]],\n",
            "\n",
            "         [[ 8.1400e-09]],\n",
            "\n",
            "         [[-1.0876e-10]],\n",
            "\n",
            "         [[ 9.2376e-11]],\n",
            "\n",
            "         [[-7.3939e-09]],\n",
            "\n",
            "         [[ 4.3737e-11]],\n",
            "\n",
            "         [[-2.7187e-08]],\n",
            "\n",
            "         [[-1.5042e-11]],\n",
            "\n",
            "         [[-3.8193e-11]],\n",
            "\n",
            "         [[ 1.7594e-11]],\n",
            "\n",
            "         [[-1.1007e-08]],\n",
            "\n",
            "         [[-3.4291e-08]],\n",
            "\n",
            "         [[-1.6724e-11]],\n",
            "\n",
            "         [[ 2.3847e-12]],\n",
            "\n",
            "         [[-1.0222e-08]],\n",
            "\n",
            "         [[-1.3935e-08]],\n",
            "\n",
            "         [[ 2.1468e-10]],\n",
            "\n",
            "         [[-5.6538e-12]],\n",
            "\n",
            "         [[ 3.9795e-08]]],\n",
            "\n",
            "\n",
            "        [[[-4.6566e-11]],\n",
            "\n",
            "         [[ 8.3669e-10]],\n",
            "\n",
            "         [[ 1.0169e-08]],\n",
            "\n",
            "         [[-2.4799e-11]],\n",
            "\n",
            "         [[ 3.1540e-11]],\n",
            "\n",
            "         [[ 1.6871e-11]],\n",
            "\n",
            "         [[-5.3368e-07]],\n",
            "\n",
            "         [[ 6.4265e-08]],\n",
            "\n",
            "         [[-4.2754e-07]],\n",
            "\n",
            "         [[ 3.0619e-11]],\n",
            "\n",
            "         [[ 1.2563e-08]],\n",
            "\n",
            "         [[-7.5785e-08]],\n",
            "\n",
            "         [[ 1.6433e-11]],\n",
            "\n",
            "         [[ 3.4452e-07]],\n",
            "\n",
            "         [[-7.0920e-07]],\n",
            "\n",
            "         [[ 1.8476e-10]],\n",
            "\n",
            "         [[ 3.2092e-10]],\n",
            "\n",
            "         [[-3.5939e-06]],\n",
            "\n",
            "         [[ 6.2830e-12]],\n",
            "\n",
            "         [[-4.7810e-07]],\n",
            "\n",
            "         [[-7.3919e-11]],\n",
            "\n",
            "         [[-1.4042e-11]],\n",
            "\n",
            "         [[ 5.4855e-11]],\n",
            "\n",
            "         [[ 1.5233e-07]],\n",
            "\n",
            "         [[-8.5397e-09]],\n",
            "\n",
            "         [[-3.2213e-11]],\n",
            "\n",
            "         [[ 9.6189e-12]],\n",
            "\n",
            "         [[ 1.8783e-06]],\n",
            "\n",
            "         [[-1.3312e-06]],\n",
            "\n",
            "         [[-1.9313e-09]],\n",
            "\n",
            "         [[ 1.1793e-11]],\n",
            "\n",
            "         [[-1.5564e-08]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4218e-12]],\n",
            "\n",
            "         [[ 9.7463e-10]],\n",
            "\n",
            "         [[-1.4957e-08]],\n",
            "\n",
            "         [[-1.7570e-10]],\n",
            "\n",
            "         [[ 8.5645e-12]],\n",
            "\n",
            "         [[ 2.6643e-11]],\n",
            "\n",
            "         [[ 4.7520e-04]],\n",
            "\n",
            "         [[-3.4454e-06]],\n",
            "\n",
            "         [[-5.0009e-07]],\n",
            "\n",
            "         [[-5.0086e-12]],\n",
            "\n",
            "         [[ 2.3352e-08]],\n",
            "\n",
            "         [[ 1.0863e-03]],\n",
            "\n",
            "         [[ 4.2614e-12]],\n",
            "\n",
            "         [[ 3.5735e-07]],\n",
            "\n",
            "         [[-8.9327e-07]],\n",
            "\n",
            "         [[-2.0332e-11]],\n",
            "\n",
            "         [[ 7.5120e-11]],\n",
            "\n",
            "         [[-2.7693e-03]],\n",
            "\n",
            "         [[ 1.8666e-12]],\n",
            "\n",
            "         [[-1.9442e-07]],\n",
            "\n",
            "         [[-1.1886e-10]],\n",
            "\n",
            "         [[ 2.2529e-11]],\n",
            "\n",
            "         [[ 6.1088e-13]],\n",
            "\n",
            "         [[ 7.4479e-08]],\n",
            "\n",
            "         [[-6.3585e-07]],\n",
            "\n",
            "         [[ 5.9003e-12]],\n",
            "\n",
            "         [[-1.3747e-11]],\n",
            "\n",
            "         [[ 4.7703e-02]],\n",
            "\n",
            "         [[-9.3331e-01]],\n",
            "\n",
            "         [[-1.7414e-09]],\n",
            "\n",
            "         [[ 1.9922e-11]],\n",
            "\n",
            "         [[ 8.7041e-07]]],\n",
            "\n",
            "\n",
            "        [[[ 5.5322e-13]],\n",
            "\n",
            "         [[ 5.5731e-10]],\n",
            "\n",
            "         [[-2.5876e-08]],\n",
            "\n",
            "         [[-3.0005e-10]],\n",
            "\n",
            "         [[ 1.9352e-12]],\n",
            "\n",
            "         [[ 8.5838e-11]],\n",
            "\n",
            "         [[-5.0716e-04]],\n",
            "\n",
            "         [[-2.5004e-06]],\n",
            "\n",
            "         [[-1.4129e-06]],\n",
            "\n",
            "         [[-2.6374e-11]],\n",
            "\n",
            "         [[-2.8835e-09]],\n",
            "\n",
            "         [[ 1.4743e-01]],\n",
            "\n",
            "         [[ 1.1115e-11]],\n",
            "\n",
            "         [[-1.8162e-06]],\n",
            "\n",
            "         [[ 1.5420e-06]],\n",
            "\n",
            "         [[-1.3366e-10]],\n",
            "\n",
            "         [[-5.9696e-10]],\n",
            "\n",
            "         [[-5.2359e-03]],\n",
            "\n",
            "         [[ 2.9455e-12]],\n",
            "\n",
            "         [[-6.9432e-07]],\n",
            "\n",
            "         [[-1.1321e-10]],\n",
            "\n",
            "         [[ 3.6258e-11]],\n",
            "\n",
            "         [[-2.0844e-11]],\n",
            "\n",
            "         [[-1.7763e-07]],\n",
            "\n",
            "         [[ 1.2946e-06]],\n",
            "\n",
            "         [[-2.8322e-11]],\n",
            "\n",
            "         [[ 3.1237e-11]],\n",
            "\n",
            "         [[-8.4401e-01]],\n",
            "\n",
            "         [[-4.5240e-02]],\n",
            "\n",
            "         [[ 1.8036e-09]],\n",
            "\n",
            "         [[ 4.5689e-11]],\n",
            "\n",
            "         [[ 2.0759e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 2.4745e-11]],\n",
            "\n",
            "         [[ 3.5288e-10]],\n",
            "\n",
            "         [[-2.4671e-08]],\n",
            "\n",
            "         [[-7.8372e-11]],\n",
            "\n",
            "         [[-4.0572e-11]],\n",
            "\n",
            "         [[ 4.9668e-11]],\n",
            "\n",
            "         [[ 2.2667e-04]],\n",
            "\n",
            "         [[-2.0301e-06]],\n",
            "\n",
            "         [[-1.5829e-07]],\n",
            "\n",
            "         [[-1.6363e-12]],\n",
            "\n",
            "         [[ 2.6320e-08]],\n",
            "\n",
            "         [[-1.9911e-04]],\n",
            "\n",
            "         [[ 4.8284e-12]],\n",
            "\n",
            "         [[ 7.5287e-06]],\n",
            "\n",
            "         [[-3.2964e-06]],\n",
            "\n",
            "         [[ 1.4979e-10]],\n",
            "\n",
            "         [[ 2.6639e-10]],\n",
            "\n",
            "         [[ 7.1946e-05]],\n",
            "\n",
            "         [[-3.5829e-11]],\n",
            "\n",
            "         [[ 3.0375e-07]],\n",
            "\n",
            "         [[-9.0378e-11]],\n",
            "\n",
            "         [[-3.2850e-12]],\n",
            "\n",
            "         [[-8.3322e-12]],\n",
            "\n",
            "         [[ 2.9416e-08]],\n",
            "\n",
            "         [[ 2.0780e-07]],\n",
            "\n",
            "         [[-3.2453e-11]],\n",
            "\n",
            "         [[-8.4905e-12]],\n",
            "\n",
            "         [[-3.5645e-05]],\n",
            "\n",
            "         [[-5.3038e-04]],\n",
            "\n",
            "         [[-1.4459e-09]],\n",
            "\n",
            "         [[ 2.3392e-11]],\n",
            "\n",
            "         [[-4.9781e-08]]],\n",
            "\n",
            "\n",
            "        [[[-5.4314e-11]],\n",
            "\n",
            "         [[ 1.2857e-09]],\n",
            "\n",
            "         [[ 7.5805e-09]],\n",
            "\n",
            "         [[-2.6308e-10]],\n",
            "\n",
            "         [[-6.1241e-12]],\n",
            "\n",
            "         [[ 3.8612e-11]],\n",
            "\n",
            "         [[-1.0914e-05]],\n",
            "\n",
            "         [[-1.6970e-06]],\n",
            "\n",
            "         [[-2.4519e-07]],\n",
            "\n",
            "         [[-1.8181e-12]],\n",
            "\n",
            "         [[ 6.4790e-09]],\n",
            "\n",
            "         [[ 1.0932e-05]],\n",
            "\n",
            "         [[ 2.7929e-11]],\n",
            "\n",
            "         [[-2.2968e-06]],\n",
            "\n",
            "         [[ 1.2238e-06]],\n",
            "\n",
            "         [[-4.7109e-11]],\n",
            "\n",
            "         [[-5.6531e-10]],\n",
            "\n",
            "         [[ 7.8865e-06]],\n",
            "\n",
            "         [[-1.1705e-11]],\n",
            "\n",
            "         [[-5.6621e-07]],\n",
            "\n",
            "         [[-1.3508e-10]],\n",
            "\n",
            "         [[ 1.9160e-11]],\n",
            "\n",
            "         [[ 8.6238e-13]],\n",
            "\n",
            "         [[-1.3579e-07]],\n",
            "\n",
            "         [[ 6.6259e-07]],\n",
            "\n",
            "         [[-6.1213e-11]],\n",
            "\n",
            "         [[-3.6783e-12]],\n",
            "\n",
            "         [[-5.1557e-05]],\n",
            "\n",
            "         [[-1.0820e-05]],\n",
            "\n",
            "         [[ 3.1048e-10]],\n",
            "\n",
            "         [[ 3.2280e-11]],\n",
            "\n",
            "         [[ 1.3096e-06]]],\n",
            "\n",
            "\n",
            "        [[[-2.1858e-11]],\n",
            "\n",
            "         [[ 4.9100e-10]],\n",
            "\n",
            "         [[ 6.5663e-09]],\n",
            "\n",
            "         [[-1.3847e-10]],\n",
            "\n",
            "         [[-3.0646e-12]],\n",
            "\n",
            "         [[-2.3666e-11]],\n",
            "\n",
            "         [[ 5.3631e-05]],\n",
            "\n",
            "         [[-9.3362e-07]],\n",
            "\n",
            "         [[-3.7447e-06]],\n",
            "\n",
            "         [[ 2.1482e-11]],\n",
            "\n",
            "         [[ 1.5493e-08]],\n",
            "\n",
            "         [[-3.3453e-05]],\n",
            "\n",
            "         [[ 9.0416e-12]],\n",
            "\n",
            "         [[ 3.8781e-06]],\n",
            "\n",
            "         [[ 2.2508e-06]],\n",
            "\n",
            "         [[-5.6629e-11]],\n",
            "\n",
            "         [[-2.8186e-11]],\n",
            "\n",
            "         [[-1.9007e-05]],\n",
            "\n",
            "         [[-2.9469e-12]],\n",
            "\n",
            "         [[-5.3173e-07]],\n",
            "\n",
            "         [[-1.1097e-10]],\n",
            "\n",
            "         [[ 1.5591e-11]],\n",
            "\n",
            "         [[ 1.3106e-11]],\n",
            "\n",
            "         [[ 4.8460e-08]],\n",
            "\n",
            "         [[ 1.6235e-06]],\n",
            "\n",
            "         [[-2.2062e-12]],\n",
            "\n",
            "         [[-1.8824e-11]],\n",
            "\n",
            "         [[ 1.5246e-05]],\n",
            "\n",
            "         [[-1.4632e-05]],\n",
            "\n",
            "         [[-1.5204e-10]],\n",
            "\n",
            "         [[-4.3193e-11]],\n",
            "\n",
            "         [[-5.5050e-07]]],\n",
            "\n",
            "\n",
            "        [[[-1.1416e-11]],\n",
            "\n",
            "         [[-5.3252e-10]],\n",
            "\n",
            "         [[ 5.3372e-08]],\n",
            "\n",
            "         [[ 1.6875e-10]],\n",
            "\n",
            "         [[ 1.3864e-11]],\n",
            "\n",
            "         [[-3.2796e-11]],\n",
            "\n",
            "         [[-8.8575e-08]],\n",
            "\n",
            "         [[ 1.4079e-09]],\n",
            "\n",
            "         [[ 1.6822e-08]],\n",
            "\n",
            "         [[-1.7423e-11]],\n",
            "\n",
            "         [[-1.0435e-08]],\n",
            "\n",
            "         [[-7.7851e-09]],\n",
            "\n",
            "         [[-1.1396e-11]],\n",
            "\n",
            "         [[-2.0172e-09]],\n",
            "\n",
            "         [[-2.7230e-08]],\n",
            "\n",
            "         [[-1.0209e-10]],\n",
            "\n",
            "         [[-4.4863e-11]],\n",
            "\n",
            "         [[-3.3514e-08]],\n",
            "\n",
            "         [[-1.8873e-11]],\n",
            "\n",
            "         [[-1.5311e-08]],\n",
            "\n",
            "         [[ 8.2255e-11]],\n",
            "\n",
            "         [[-4.2767e-11]],\n",
            "\n",
            "         [[-2.1484e-11]],\n",
            "\n",
            "         [[ 4.7647e-09]],\n",
            "\n",
            "         [[ 1.3447e-08]],\n",
            "\n",
            "         [[ 2.3933e-12]],\n",
            "\n",
            "         [[-3.4767e-11]],\n",
            "\n",
            "         [[ 3.5242e-09]],\n",
            "\n",
            "         [[ 9.0304e-08]],\n",
            "\n",
            "         [[-2.8506e-10]],\n",
            "\n",
            "         [[ 2.1973e-11]],\n",
            "\n",
            "         [[ 2.9762e-08]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2436e-11]],\n",
            "\n",
            "         [[ 9.8707e-10]],\n",
            "\n",
            "         [[-8.2875e-08]],\n",
            "\n",
            "         [[-6.8562e-12]],\n",
            "\n",
            "         [[ 5.2856e-11]],\n",
            "\n",
            "         [[-4.9007e-11]],\n",
            "\n",
            "         [[-2.2727e-08]],\n",
            "\n",
            "         [[-1.8129e-09]],\n",
            "\n",
            "         [[ 1.9689e-08]],\n",
            "\n",
            "         [[ 2.7594e-11]],\n",
            "\n",
            "         [[-7.0278e-10]],\n",
            "\n",
            "         [[ 7.9911e-09]],\n",
            "\n",
            "         [[-1.8314e-11]],\n",
            "\n",
            "         [[ 1.9050e-08]],\n",
            "\n",
            "         [[-5.9791e-10]],\n",
            "\n",
            "         [[-2.2042e-10]],\n",
            "\n",
            "         [[-1.7177e-10]],\n",
            "\n",
            "         [[-2.0568e-08]],\n",
            "\n",
            "         [[ 5.2490e-12]],\n",
            "\n",
            "         [[-2.9384e-09]],\n",
            "\n",
            "         [[-8.1177e-12]],\n",
            "\n",
            "         [[-5.6194e-11]],\n",
            "\n",
            "         [[-2.1798e-11]],\n",
            "\n",
            "         [[-4.8695e-09]],\n",
            "\n",
            "         [[ 4.1209e-09]],\n",
            "\n",
            "         [[ 4.3427e-11]],\n",
            "\n",
            "         [[ 2.9912e-12]],\n",
            "\n",
            "         [[ 4.7996e-08]],\n",
            "\n",
            "         [[-8.3257e-08]],\n",
            "\n",
            "         [[-2.3361e-10]],\n",
            "\n",
            "         [[ 2.2703e-11]],\n",
            "\n",
            "         [[-3.4954e-08]]],\n",
            "\n",
            "\n",
            "        [[[ 8.3312e-12]],\n",
            "\n",
            "         [[-1.3145e-09]],\n",
            "\n",
            "         [[-1.3686e-08]],\n",
            "\n",
            "         [[ 4.8571e-10]],\n",
            "\n",
            "         [[ 1.1853e-11]],\n",
            "\n",
            "         [[-6.5388e-11]],\n",
            "\n",
            "         [[-2.8220e-08]],\n",
            "\n",
            "         [[-2.3799e-08]],\n",
            "\n",
            "         [[ 1.0916e-07]],\n",
            "\n",
            "         [[ 1.2838e-11]],\n",
            "\n",
            "         [[-1.8783e-08]],\n",
            "\n",
            "         [[-2.2250e-07]],\n",
            "\n",
            "         [[-8.0245e-12]],\n",
            "\n",
            "         [[ 1.1151e-07]],\n",
            "\n",
            "         [[-1.2221e-07]],\n",
            "\n",
            "         [[ 8.6733e-11]],\n",
            "\n",
            "         [[ 2.4286e-10]],\n",
            "\n",
            "         [[ 6.0556e-08]],\n",
            "\n",
            "         [[-1.9376e-11]],\n",
            "\n",
            "         [[-1.8090e-08]],\n",
            "\n",
            "         [[ 1.1507e-10]],\n",
            "\n",
            "         [[ 3.3812e-11]],\n",
            "\n",
            "         [[-9.1796e-11]],\n",
            "\n",
            "         [[ 2.5208e-08]],\n",
            "\n",
            "         [[-8.0216e-08]],\n",
            "\n",
            "         [[ 3.8031e-11]],\n",
            "\n",
            "         [[-6.8441e-12]],\n",
            "\n",
            "         [[ 3.0770e-07]],\n",
            "\n",
            "         [[ 4.2385e-07]],\n",
            "\n",
            "         [[ 9.2600e-10]],\n",
            "\n",
            "         [[-2.5513e-11]],\n",
            "\n",
            "         [[-1.0011e-07]]],\n",
            "\n",
            "\n",
            "        [[[-2.7721e-12]],\n",
            "\n",
            "         [[ 1.8276e-09]],\n",
            "\n",
            "         [[-5.2811e-08]],\n",
            "\n",
            "         [[-2.6098e-10]],\n",
            "\n",
            "         [[-1.2821e-11]],\n",
            "\n",
            "         [[ 5.8869e-11]],\n",
            "\n",
            "         [[-2.5746e-08]],\n",
            "\n",
            "         [[ 2.7678e-09]],\n",
            "\n",
            "         [[ 4.7792e-08]],\n",
            "\n",
            "         [[ 2.6927e-11]],\n",
            "\n",
            "         [[ 3.1148e-08]],\n",
            "\n",
            "         [[ 1.9721e-08]],\n",
            "\n",
            "         [[-1.7207e-11]],\n",
            "\n",
            "         [[ 2.8021e-08]],\n",
            "\n",
            "         [[ 2.8383e-08]],\n",
            "\n",
            "         [[-5.6086e-11]],\n",
            "\n",
            "         [[-1.1266e-10]],\n",
            "\n",
            "         [[ 2.4590e-08]],\n",
            "\n",
            "         [[-8.9904e-12]],\n",
            "\n",
            "         [[ 3.4512e-08]],\n",
            "\n",
            "         [[-6.5628e-11]],\n",
            "\n",
            "         [[-1.3292e-11]],\n",
            "\n",
            "         [[ 1.2685e-11]],\n",
            "\n",
            "         [[ 1.2366e-08]],\n",
            "\n",
            "         [[ 1.8666e-08]],\n",
            "\n",
            "         [[ 2.2079e-11]],\n",
            "\n",
            "         [[ 2.4518e-11]],\n",
            "\n",
            "         [[ 1.0321e-07]],\n",
            "\n",
            "         [[-1.6480e-08]],\n",
            "\n",
            "         [[-2.6541e-09]],\n",
            "\n",
            "         [[ 9.6746e-12]],\n",
            "\n",
            "         [[-7.0873e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 4.0065e-12]],\n",
            "\n",
            "         [[ 2.0531e-10]],\n",
            "\n",
            "         [[-6.3247e-09]],\n",
            "\n",
            "         [[-1.1825e-10]],\n",
            "\n",
            "         [[-1.0183e-11]],\n",
            "\n",
            "         [[ 5.9856e-13]],\n",
            "\n",
            "         [[-7.8704e-09]],\n",
            "\n",
            "         [[ 9.3094e-10]],\n",
            "\n",
            "         [[-6.3842e-09]],\n",
            "\n",
            "         [[-6.4656e-12]],\n",
            "\n",
            "         [[ 5.6753e-09]],\n",
            "\n",
            "         [[-2.9395e-09]],\n",
            "\n",
            "         [[-7.0205e-12]],\n",
            "\n",
            "         [[ 6.7344e-09]],\n",
            "\n",
            "         [[-2.6858e-09]],\n",
            "\n",
            "         [[ 1.0328e-12]],\n",
            "\n",
            "         [[-3.8978e-13]],\n",
            "\n",
            "         [[-6.2339e-09]],\n",
            "\n",
            "         [[-6.1740e-12]],\n",
            "\n",
            "         [[ 1.7532e-09]],\n",
            "\n",
            "         [[-2.9273e-12]],\n",
            "\n",
            "         [[-1.0544e-11]],\n",
            "\n",
            "         [[ 3.2563e-11]],\n",
            "\n",
            "         [[-7.6253e-09]],\n",
            "\n",
            "         [[-7.6524e-09]],\n",
            "\n",
            "         [[ 1.3041e-11]],\n",
            "\n",
            "         [[ 7.2358e-12]],\n",
            "\n",
            "         [[-1.1661e-08]],\n",
            "\n",
            "         [[-5.5779e-08]],\n",
            "\n",
            "         [[-3.1301e-10]],\n",
            "\n",
            "         [[ 2.3872e-11]],\n",
            "\n",
            "         [[ 4.8137e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6292e-11]],\n",
            "\n",
            "         [[ 6.9456e-10]],\n",
            "\n",
            "         [[-1.0832e-09]],\n",
            "\n",
            "         [[-2.4148e-10]],\n",
            "\n",
            "         [[ 6.7490e-13]],\n",
            "\n",
            "         [[ 1.3369e-11]],\n",
            "\n",
            "         [[-6.8855e-06]],\n",
            "\n",
            "         [[ 4.8346e-07]],\n",
            "\n",
            "         [[-1.1006e-07]],\n",
            "\n",
            "         [[ 1.4308e-11]],\n",
            "\n",
            "         [[ 1.7832e-08]],\n",
            "\n",
            "         [[ 1.3795e-05]],\n",
            "\n",
            "         [[ 7.4297e-12]],\n",
            "\n",
            "         [[-1.4123e-06]],\n",
            "\n",
            "         [[-3.7111e-07]],\n",
            "\n",
            "         [[ 4.5866e-11]],\n",
            "\n",
            "         [[ 5.5468e-10]],\n",
            "\n",
            "         [[-6.0821e-05]],\n",
            "\n",
            "         [[ 2.6562e-11]],\n",
            "\n",
            "         [[-8.0271e-07]],\n",
            "\n",
            "         [[-9.1245e-11]],\n",
            "\n",
            "         [[-2.7354e-11]],\n",
            "\n",
            "         [[ 1.2226e-10]],\n",
            "\n",
            "         [[ 1.2003e-07]],\n",
            "\n",
            "         [[-9.5889e-07]],\n",
            "\n",
            "         [[ 8.0718e-12]],\n",
            "\n",
            "         [[ 5.1558e-11]],\n",
            "\n",
            "         [[ 3.8565e-05]],\n",
            "\n",
            "         [[-3.0776e-05]],\n",
            "\n",
            "         [[-2.2637e-09]],\n",
            "\n",
            "         [[ 4.9513e-11]],\n",
            "\n",
            "         [[-2.3416e-08]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.0.bn3\n",
            "bottlenecks.1\n",
            "bottlenecks.1.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[-2.5537e-07]],\n",
            "\n",
            "         [[ 7.5221e-06]],\n",
            "\n",
            "         [[-2.9505e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0717e-08]],\n",
            "\n",
            "         [[ 1.4290e-08]],\n",
            "\n",
            "         [[ 1.0272e-05]]],\n",
            "\n",
            "\n",
            "        [[[-2.0004e-07]],\n",
            "\n",
            "         [[-2.0940e-05]],\n",
            "\n",
            "         [[ 3.8390e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.5570e-08]],\n",
            "\n",
            "         [[-6.9037e-09]],\n",
            "\n",
            "         [[-1.0335e-05]]],\n",
            "\n",
            "\n",
            "        [[[ 9.4928e-11]],\n",
            "\n",
            "         [[ 2.1108e-11]],\n",
            "\n",
            "         [[ 1.5252e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.1100e-10]],\n",
            "\n",
            "         [[ 2.4245e-11]],\n",
            "\n",
            "         [[ 1.2817e-10]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-4.5767e-08]],\n",
            "\n",
            "         [[ 1.0442e-04]],\n",
            "\n",
            "         [[-8.1699e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.7624e-09]],\n",
            "\n",
            "         [[ 1.3342e-08]],\n",
            "\n",
            "         [[ 6.5015e-06]]],\n",
            "\n",
            "\n",
            "        [[[-1.2747e-07]],\n",
            "\n",
            "         [[ 2.5079e-04]],\n",
            "\n",
            "         [[ 2.5781e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.3219e-09]],\n",
            "\n",
            "         [[-1.1832e-08]],\n",
            "\n",
            "         [[-8.2038e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 6.5773e-07]],\n",
            "\n",
            "         [[ 4.0972e-03]],\n",
            "\n",
            "         [[ 2.8257e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.4709e-08]],\n",
            "\n",
            "         [[ 2.0964e-09]],\n",
            "\n",
            "         [[-3.0440e-05]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.1.bn1\n",
            "bottlenecks.1.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[-4.9218e-07,  2.0661e-05,  1.4491e-05],\n",
            "          [ 4.0957e-05,  4.4423e-05, -6.4164e-05],\n",
            "          [-2.1198e-05, -1.8571e-05, -5.3911e-07]]],\n",
            "\n",
            "\n",
            "        [[[-2.2493e-06,  4.8318e-05, -3.8343e-05],\n",
            "          [ 4.1604e-05, -3.4729e-05,  2.8368e-05],\n",
            "          [-1.8577e-05, -4.3497e-06, -3.6720e-05]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0555e-11,  7.0348e-12,  9.7684e-11],\n",
            "          [ 5.8976e-11,  6.3088e-11,  2.1614e-10],\n",
            "          [-1.2189e-11,  4.8368e-11,  2.4886e-11]]],\n",
            "\n",
            "\n",
            "        [[[-2.1038e-11,  5.3249e-11,  5.3196e-11],\n",
            "          [ 1.1055e-10,  1.8305e-10,  1.1453e-10],\n",
            "          [ 3.9211e-11,  4.6229e-11,  4.1375e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 3.3737e-11,  4.1495e-11, -2.9027e-11],\n",
            "          [-8.4244e-11,  9.8467e-12,  7.6842e-11],\n",
            "          [-1.6887e-10,  6.9756e-11,  1.9132e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3025e-05,  6.6307e-04, -8.0149e-05],\n",
            "          [ 2.0993e-04,  2.2076e-04,  6.8914e-06],\n",
            "          [-3.9595e-04, -2.8388e-04, -2.1518e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3290e-01,  3.5428e-01, -1.7712e-02],\n",
            "          [ 1.5189e-01,  7.1038e-01, -1.0943e-01],\n",
            "          [-2.3376e-01, -1.9541e-01, -2.3495e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 3.8337e-11, -5.4469e-11, -6.0647e-11],\n",
            "          [-8.2566e-12, -1.1830e-10, -6.7153e-11],\n",
            "          [-1.9902e-11, -3.5166e-11, -1.5382e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3450e-07, -4.4054e-07, -2.8589e-07],\n",
            "          [ 4.3015e-07, -1.4488e-07, -2.9334e-07],\n",
            "          [ 3.0712e-07,  6.0689e-07,  8.5365e-08]]],\n",
            "\n",
            "\n",
            "        [[[-1.3942e-07, -1.0072e-07,  8.0782e-07],\n",
            "          [-3.2841e-07,  1.2167e-07, -1.0781e-06],\n",
            "          [-2.9625e-07, -1.6595e-07, -6.7752e-07]]],\n",
            "\n",
            "\n",
            "        [[[ 4.2005e-01,  1.0399e-01,  8.1967e-02],\n",
            "          [-2.2438e-01,  1.4139e+00, -3.1533e-01],\n",
            "          [-1.1743e-01, -3.8764e-01, -7.6607e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.7786e-06,  5.8910e-06,  3.7642e-06],\n",
            "          [-3.7250e-07,  8.5106e-06,  1.1451e-05],\n",
            "          [-2.5714e-06, -2.2718e-06, -2.8165e-06]]],\n",
            "\n",
            "\n",
            "        [[[-8.8303e-08, -1.1956e-06,  9.1777e-07],\n",
            "          [ 1.0388e-06,  2.3377e-06, -1.8126e-06],\n",
            "          [-2.3059e-07,  2.2058e-07,  7.1752e-07]]],\n",
            "\n",
            "\n",
            "        [[[-2.5166e-11,  1.4677e-11, -1.4417e-11],\n",
            "          [ 3.6757e-11,  1.0039e-10,  3.0068e-11],\n",
            "          [ 4.2968e-11, -4.0203e-12,  3.1952e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 2.6181e-05,  2.0041e-04, -1.2341e-04],\n",
            "          [ 6.4347e-05,  2.0746e-04, -7.7568e-06],\n",
            "          [-4.2694e-05,  6.2341e-07, -3.3473e-05]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0131e-03,  2.2792e-05,  1.5288e-03],\n",
            "          [-5.6038e-04, -1.1843e-03, -1.3856e-03],\n",
            "          [ 2.8880e-03,  1.3636e-03,  6.3329e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 2.7674e-11, -2.0304e-11,  4.8048e-11],\n",
            "          [ 5.8335e-11,  4.1894e-11,  3.6388e-11],\n",
            "          [-2.8756e-12, -7.0759e-12, -3.0039e-12]]],\n",
            "\n",
            "\n",
            "        [[[-1.0116e-07, -2.6697e-07, -4.0777e-08],\n",
            "          [-1.0316e-07, -3.9006e-07, -3.9394e-07],\n",
            "          [ 8.1954e-08,  1.8570e-07, -2.4106e-07]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6782e-04,  8.0390e-04, -7.9841e-04],\n",
            "          [-2.8584e-04,  1.3212e-04, -1.3265e-03],\n",
            "          [-1.5385e-03, -1.5468e-03, -2.5577e-03]]],\n",
            "\n",
            "\n",
            "        [[[-8.6510e-06, -1.6845e-05, -4.0841e-06],\n",
            "          [-5.4162e-06, -7.5466e-06, -3.0573e-06],\n",
            "          [ 4.5274e-06,  8.6148e-06, -8.2704e-07]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4994e-07, -7.5392e-07,  3.6285e-08],\n",
            "          [-2.8065e-07, -1.4068e-08, -6.5415e-07],\n",
            "          [-3.1615e-07, -7.4864e-07, -1.7621e-07]]],\n",
            "\n",
            "\n",
            "        [[[-3.7953e-02, -1.2624e-01,  1.6312e-01],\n",
            "          [ 1.0537e-01, -8.3622e-01,  7.5264e-01],\n",
            "          [ 3.5270e-02, -1.9245e-01,  9.4402e-02]]],\n",
            "\n",
            "\n",
            "        [[[-3.4422e-02, -3.1016e-01,  9.9981e-02],\n",
            "          [ 5.9990e-02,  1.1761e-01,  8.9136e-01],\n",
            "          [-1.5537e-01, -3.3840e-01, -2.8331e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.0800e-05, -3.5354e-05,  2.4404e-05],\n",
            "          [-2.4813e-05, -4.9701e-05, -2.9766e-06],\n",
            "          [ 1.6277e-05,  2.2425e-05,  1.5966e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 8.7228e-12,  5.4300e-11,  2.9623e-12],\n",
            "          [ 1.5238e-11,  3.5938e-11,  1.5545e-11],\n",
            "          [ 1.4996e-11,  2.6607e-11,  6.2999e-11]]],\n",
            "\n",
            "\n",
            "        [[[-2.9054e-05, -3.2905e-06,  1.0015e-05],\n",
            "          [-2.8092e-05, -1.6985e-05,  2.4534e-05],\n",
            "          [-2.8529e-06,  1.3026e-05,  2.6998e-05]]],\n",
            "\n",
            "\n",
            "        [[[-1.7324e-01, -1.6732e-01,  3.3151e-02],\n",
            "          [ 7.8769e-03, -4.1526e-01, -3.1861e-02],\n",
            "          [ 1.0779e-01,  1.6030e-01,  1.0955e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0624e-02,  1.8447e-02, -7.5994e-03],\n",
            "          [ 2.3535e-03,  2.8674e-02, -2.3391e-02],\n",
            "          [-2.0173e-02, -1.2969e-02, -7.7802e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.6042e-10, -4.2607e-10, -1.9687e-10],\n",
            "          [ 1.1559e-10,  1.6656e-10, -1.0087e-10],\n",
            "          [ 4.2076e-10,  3.6827e-10,  7.1804e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7060e-10,  3.5616e-11, -9.9867e-12],\n",
            "          [ 5.6050e-11, -2.4515e-11, -2.7902e-11],\n",
            "          [-1.0873e-11, -3.6701e-11, -1.6846e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3131e-05,  5.3140e-05, -9.5368e-06],\n",
            "          [-1.8328e-06, -1.6742e-05,  4.0368e-07],\n",
            "          [-6.3111e-07,  3.4046e-05, -9.6784e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7098e-06,  1.9604e-05,  1.6951e-05],\n",
            "          [-4.6716e-06,  9.5290e-06,  1.0377e-05],\n",
            "          [-1.6580e-05, -1.4228e-05, -1.2013e-05]]],\n",
            "\n",
            "\n",
            "        [[[-5.9773e-10,  3.8734e-10, -9.1717e-10],\n",
            "          [-3.6604e-10, -1.3249e-09,  5.6575e-10],\n",
            "          [-2.8462e-10, -6.0897e-10,  1.0870e-10]]],\n",
            "\n",
            "\n",
            "        [[[-1.8322e-11, -3.3523e-11, -4.7521e-12],\n",
            "          [-1.5536e-11, -1.6759e-11, -1.8665e-12],\n",
            "          [ 1.8417e-11, -2.0282e-11, -2.4994e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 4.6744e-12,  5.1934e-11,  2.7790e-11],\n",
            "          [-2.8438e-11,  1.2473e-10,  9.9267e-11],\n",
            "          [-7.3068e-11,  1.4405e-11, -8.6632e-11]]],\n",
            "\n",
            "\n",
            "        [[[-3.7780e-06, -2.5795e-06, -2.7149e-05],\n",
            "          [ 3.5969e-05,  4.6734e-06, -3.5141e-05],\n",
            "          [-1.1234e-05,  3.8872e-06, -3.6072e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 2.4428e-02, -1.5910e-03,  2.4675e-02],\n",
            "          [ 3.7221e-02, -4.7482e-02,  1.1525e-02],\n",
            "          [ 5.5866e-02,  2.8334e-02,  3.9868e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7438e-09,  2.3291e-09,  6.6759e-10],\n",
            "          [ 2.0237e-09,  6.5988e-09, -1.6748e-09],\n",
            "          [-8.9963e-11, -1.5099e-09, -1.4248e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7209e-11,  1.0609e-10,  4.1373e-11],\n",
            "          [-1.3793e-11,  2.3115e-11,  2.5737e-11],\n",
            "          [ 4.0631e-11,  1.9260e-11,  4.9542e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 2.7869e-11,  3.3770e-11,  5.8234e-11],\n",
            "          [ 2.5525e-11,  4.2779e-11,  4.2109e-11],\n",
            "          [-5.0366e-12, -7.3252e-12, -6.7830e-13]]],\n",
            "\n",
            "\n",
            "        [[[-1.0094e-11, -2.5155e-12,  8.3362e-12],\n",
            "          [ 2.5620e-12, -7.3067e-12,  6.2265e-12],\n",
            "          [-1.3567e-11,  5.0694e-11,  6.3398e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5551e-11,  4.5304e-11, -1.0567e-11],\n",
            "          [ 9.3329e-11,  2.3690e-11,  3.7123e-11],\n",
            "          [-3.9173e-12,  4.9185e-11,  8.4914e-12]]],\n",
            "\n",
            "\n",
            "        [[[-4.5305e-07, -6.8431e-07, -2.5245e-07],\n",
            "          [-4.6176e-07, -4.1739e-07,  3.7362e-07],\n",
            "          [ 2.1598e-07, -1.7162e-07,  2.2387e-07]]],\n",
            "\n",
            "\n",
            "        [[[ 8.7306e-10,  2.6504e-09,  8.3419e-09],\n",
            "          [-2.9589e-09,  1.6022e-09,  4.6963e-09],\n",
            "          [-1.1664e-09, -2.3471e-09,  2.9578e-09]]],\n",
            "\n",
            "\n",
            "        [[[-5.1039e-05, -3.4307e-04, -1.1213e-04],\n",
            "          [-9.8414e-05, -4.0094e-04, -1.4169e-04],\n",
            "          [ 3.9811e-04,  3.6574e-04,  1.4992e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 3.0277e-10,  8.4183e-10, -1.4413e-10],\n",
            "          [ 3.3195e-10,  5.7174e-10, -3.8861e-10],\n",
            "          [-3.5534e-10, -2.3580e-10, -5.2090e-11]]],\n",
            "\n",
            "\n",
            "        [[[-7.2692e-11,  4.9831e-10,  6.0605e-10],\n",
            "          [ 4.2659e-11,  2.2457e-10,  2.9693e-10],\n",
            "          [-3.9398e-10, -2.2149e-10, -1.1091e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 4.9928e-12, -2.6855e-11, -2.4923e-12],\n",
            "          [-4.8138e-12, -1.9496e-11, -3.1158e-12],\n",
            "          [-1.3246e-11, -1.1049e-11, -1.1999e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7594e-11,  6.8052e-11,  3.1783e-11],\n",
            "          [-1.7735e-11, -1.2894e-11,  9.9076e-12],\n",
            "          [-2.9595e-11, -3.8256e-11, -1.4862e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 6.9318e-05,  7.3297e-04, -4.2800e-04],\n",
            "          [-3.3056e-04, -7.5748e-04, -2.5004e-04],\n",
            "          [-1.3722e-03, -1.6009e-03, -1.7724e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 3.7467e-09,  2.8912e-09,  1.2207e-09],\n",
            "          [ 2.5577e-09,  3.0806e-09, -5.0611e-10],\n",
            "          [ 1.7530e-09,  1.2330e-09,  2.3221e-09]]],\n",
            "\n",
            "\n",
            "        [[[-4.3849e-06,  1.0461e-05, -2.2064e-06],\n",
            "          [-1.2419e-05,  1.9126e-07, -4.3571e-06],\n",
            "          [-2.5896e-06,  6.6415e-06,  2.5941e-06]]],\n",
            "\n",
            "\n",
            "        [[[-1.3158e-09, -2.9717e-09,  2.6470e-09],\n",
            "          [ 2.4950e-09, -6.8569e-10, -1.3946e-08],\n",
            "          [-4.8809e-10, -2.5113e-09, -9.8084e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0753e-07, -4.4930e-08, -1.0817e-08],\n",
            "          [ 1.1849e-07, -2.9100e-07, -3.2730e-07],\n",
            "          [ 5.6292e-07,  2.4926e-07,  2.2451e-07]]],\n",
            "\n",
            "\n",
            "        [[[-7.3646e-02, -2.9576e-01,  3.0068e-01],\n",
            "          [-5.6069e-02, -3.4239e-01,  2.3139e-01],\n",
            "          [ 5.0094e-01,  1.5933e-01,  7.6377e-02]]],\n",
            "\n",
            "\n",
            "        [[[-1.4621e-05, -7.5991e-05,  4.9770e-05],\n",
            "          [-8.2583e-05, -1.4523e-04, -1.1846e-04],\n",
            "          [-9.5830e-05, -1.6461e-04,  1.3750e-06]]],\n",
            "\n",
            "\n",
            "        [[[-3.7534e-01, -2.1475e-01,  8.4023e-02],\n",
            "          [-7.7165e-02, -5.1294e-01,  1.6604e-01],\n",
            "          [ 2.1698e-01,  1.1684e-01,  1.6897e-01]]],\n",
            "\n",
            "\n",
            "        [[[-2.1645e-03, -1.1945e-03, -5.7581e-04],\n",
            "          [-5.4895e-04, -1.6748e-03, -1.2054e-03],\n",
            "          [-4.5738e-04, -4.2504e-04, -1.4984e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.2150e-09, -4.4651e-09, -1.4836e-09],\n",
            "          [-7.2292e-10, -1.1731e-09,  4.7230e-10],\n",
            "          [ 1.8530e-09,  2.8165e-09,  1.4961e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0945e-10,  3.7580e-10, -1.0477e-09],\n",
            "          [ 3.5957e-10,  5.2957e-10, -3.0966e-10],\n",
            "          [ 9.4754e-10,  1.0755e-09,  1.4315e-09]]],\n",
            "\n",
            "\n",
            "        [[[-3.4743e-04, -7.8198e-04, -3.6230e-04],\n",
            "          [-5.9613e-05,  3.5351e-04, -1.1282e-04],\n",
            "          [-5.7982e-05,  3.7462e-04,  1.0465e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 1.8033e-11, -2.3226e-11, -1.9481e-11],\n",
            "          [-2.9840e-12, -3.5711e-11, -1.6256e-11],\n",
            "          [-1.3383e-11, -2.2786e-11, -1.7310e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 4.2547e-01,  5.9155e-01, -5.0289e-01],\n",
            "          [ 6.7743e-01,  7.9516e-01, -2.8634e-01],\n",
            "          [-5.6743e-01, -3.3578e-01,  1.0274e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.2635e-10, -1.4710e-08,  1.3027e-09],\n",
            "          [-3.7842e-09, -1.7762e-08, -1.0464e-09],\n",
            "          [-5.5530e-10, -7.2689e-10, -6.1048e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 6.3969e-07, -1.6734e-07, -1.5189e-07],\n",
            "          [ 1.9860e-06,  1.4391e-06, -5.2055e-07],\n",
            "          [-2.1505e-07, -9.4533e-07, -9.1784e-07]]],\n",
            "\n",
            "\n",
            "        [[[-1.4761e-10,  1.5872e-10, -2.5450e-11],\n",
            "          [-1.2766e-10,  1.3791e-10,  7.0467e-11],\n",
            "          [-3.5898e-10,  1.1760e-10,  3.2933e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 5.1972e-11,  1.0901e-10,  2.8516e-11],\n",
            "          [ 4.5117e-11,  1.1298e-10,  6.9427e-11],\n",
            "          [ 2.0805e-11,  2.3498e-11, -2.1348e-11]]],\n",
            "\n",
            "\n",
            "        [[[-7.8585e-02,  9.9034e-02,  5.6030e-03],\n",
            "          [-1.8625e-02, -9.4752e-01, -2.1471e-01],\n",
            "          [ 1.4906e-01,  6.0885e-01,  3.3989e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.2833e-09, -2.6879e-09,  2.7084e-09],\n",
            "          [-4.7447e-09, -4.8294e-09,  5.0534e-09],\n",
            "          [-6.9383e-10, -1.0318e-10,  4.4828e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 3.5523e-10, -2.4656e-11, -8.3258e-10],\n",
            "          [ 1.7050e-09,  1.0425e-09, -5.5223e-10],\n",
            "          [-6.2309e-11,  2.2896e-10,  1.0234e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 4.7454e-06,  4.0068e-06,  3.4627e-06],\n",
            "          [-3.8518e-06, -6.5941e-06,  2.3461e-06],\n",
            "          [-1.0072e-06, -2.4174e-06,  2.5539e-06]]],\n",
            "\n",
            "\n",
            "        [[[-8.0692e-12,  1.7623e-12,  5.0023e-11],\n",
            "          [ 1.8321e-12,  1.6354e-11,  3.9440e-11],\n",
            "          [ 6.2872e-12,  5.4094e-12,  2.3271e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 9.8055e-06,  1.0364e-05, -3.4592e-06],\n",
            "          [ 4.4481e-05,  3.3365e-05,  5.2534e-06],\n",
            "          [ 5.3631e-07, -1.2423e-05, -4.1271e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 2.2030e-07, -8.1113e-07, -4.7709e-07],\n",
            "          [ 1.1623e-07, -7.2441e-07, -7.1443e-07],\n",
            "          [ 6.7458e-07,  6.6582e-07,  5.1780e-07]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1718e-06,  5.7331e-05, -2.2195e-05],\n",
            "          [ 5.3651e-05,  1.9795e-04,  3.3913e-06],\n",
            "          [-5.7296e-05, -1.2299e-05, -4.1220e-05]]],\n",
            "\n",
            "\n",
            "        [[[-1.7203e-04, -2.1494e-04,  4.7656e-05],\n",
            "          [-3.1939e-04, -3.4093e-04,  3.9314e-05],\n",
            "          [ 8.2021e-05,  5.1615e-05, -8.3729e-05]]],\n",
            "\n",
            "\n",
            "        [[[-5.1167e-07,  1.6057e-07,  3.9544e-06],\n",
            "          [ 1.6402e-07, -7.9502e-08,  5.2634e-06],\n",
            "          [ 1.7529e-06,  1.8818e-07, -5.0031e-07]]],\n",
            "\n",
            "\n",
            "        [[[-2.6916e-06, -1.8363e-06, -4.2952e-07],\n",
            "          [-7.5057e-06,  1.2673e-06,  1.3425e-05],\n",
            "          [-6.5100e-07, -1.3454e-08, -2.2226e-07]]],\n",
            "\n",
            "\n",
            "        [[[-1.2735e-06, -5.6468e-06,  4.8083e-06],\n",
            "          [-3.9151e-06, -1.2826e-06, -2.1056e-05],\n",
            "          [-6.3657e-06, -1.1040e-05, -2.0721e-06]]],\n",
            "\n",
            "\n",
            "        [[[-1.7204e-11,  1.7579e-11,  2.0569e-11],\n",
            "          [-1.4806e-11,  6.0611e-11,  4.5475e-11],\n",
            "          [-9.0026e-12, -1.9752e-11, -4.7365e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1361e-10,  1.8788e-11, -8.2237e-11],\n",
            "          [-3.4861e-11,  5.5024e-11, -6.5119e-11],\n",
            "          [ 1.3520e-10, -9.3113e-11, -8.7669e-11]]],\n",
            "\n",
            "\n",
            "        [[[-6.0500e-11,  1.0928e-10,  1.1003e-10],\n",
            "          [-6.5767e-11,  1.4064e-10,  9.7309e-11],\n",
            "          [-8.4401e-11, -7.5289e-11, -6.9086e-11]]],\n",
            "\n",
            "\n",
            "        [[[-5.2472e-09,  6.3523e-10,  5.2408e-09],\n",
            "          [-2.8808e-09,  3.4114e-09,  2.6084e-09],\n",
            "          [-1.3678e-09, -1.0759e-09, -4.5870e-09]]],\n",
            "\n",
            "\n",
            "        [[[-1.6615e-11,  6.8217e-11,  2.3292e-11],\n",
            "          [-6.2586e-12,  5.2247e-11,  5.2160e-12],\n",
            "          [ 3.7069e-12,  7.9765e-12, -3.1853e-11]]],\n",
            "\n",
            "\n",
            "        [[[-2.5662e-01, -1.5998e-01,  1.3395e-01],\n",
            "          [-3.5011e-01, -3.2668e-01,  1.0109e+00],\n",
            "          [ 2.5849e-02, -1.3721e-01,  2.0537e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.2883e-01,  3.2257e-01, -2.6285e-01],\n",
            "          [ 3.6022e-01,  4.2927e-01, -9.7043e-02],\n",
            "          [-2.8204e-01, -1.2620e-01, -1.5581e-01]]],\n",
            "\n",
            "\n",
            "        [[[-7.7096e-06,  1.2577e-05, -6.7737e-06],\n",
            "          [ 1.3382e-05,  7.5217e-05, -7.5568e-06],\n",
            "          [-2.1626e-06,  3.4419e-06, -5.4274e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3901e-07, -3.4010e-07, -1.7014e-07],\n",
            "          [ 1.7460e-07,  1.9336e-07, -1.3791e-06],\n",
            "          [-1.3156e-07, -2.0025e-07, -2.6888e-07]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5022e-11, -4.7959e-12, -4.8638e-12],\n",
            "          [ 1.9416e-11,  1.7785e-13,  1.1027e-11],\n",
            "          [ 5.7531e-11,  4.1362e-11,  4.3567e-11]]],\n",
            "\n",
            "\n",
            "        [[[-6.9535e-05, -2.1087e-04,  1.9704e-05],\n",
            "          [-1.8947e-04, -1.5131e-04, -1.0637e-04],\n",
            "          [-6.3564e-05, -2.4085e-04,  3.3575e-05]]],\n",
            "\n",
            "\n",
            "        [[[-2.3674e-11,  9.4262e-11,  2.8469e-11],\n",
            "          [-3.4874e-11,  1.7108e-10,  3.3793e-11],\n",
            "          [-3.8292e-11, -3.4708e-11, -5.3157e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5061e-07,  3.0865e-06, -2.7613e-06],\n",
            "          [ 1.5598e-06,  2.5882e-06, -5.4595e-07],\n",
            "          [-6.1975e-07, -1.3456e-06, -1.4375e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1317e-07, -4.5266e-07, -3.6401e-07],\n",
            "          [ 5.4903e-08, -1.0380e-06, -8.8516e-07],\n",
            "          [ 2.1670e-08, -5.6650e-07, -2.8447e-07]]],\n",
            "\n",
            "\n",
            "        [[[-4.8732e-05,  2.3411e-05, -7.5290e-05],\n",
            "          [ 1.3150e-04, -1.2269e-04,  1.5891e-04],\n",
            "          [ 5.1108e-05, -6.4258e-06, -1.9264e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 3.8750e-04, -1.0346e-04, -6.5349e-04],\n",
            "          [ 1.1022e-03, -3.7723e-04, -1.1141e-03],\n",
            "          [-2.8532e-04, -5.6766e-04, -1.1347e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 6.6777e-03,  1.5647e-02, -1.9838e-02],\n",
            "          [ 2.0370e-02,  2.7068e-02,  5.1341e-04],\n",
            "          [-1.2873e-02, -1.2075e-03,  8.8479e-04]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.1.bn2\n",
            "bottlenecks.1.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[-3.0086e-06]],\n",
            "\n",
            "         [[-2.3762e-05]],\n",
            "\n",
            "         [[ 3.7385e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8665e-05]],\n",
            "\n",
            "         [[-2.2094e-04]],\n",
            "\n",
            "         [[ 4.8429e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6722e-08]],\n",
            "\n",
            "         [[ 1.1443e-08]],\n",
            "\n",
            "         [[-9.4662e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.4469e-09]],\n",
            "\n",
            "         [[ 4.1501e-09]],\n",
            "\n",
            "         [[-9.8201e-10]]],\n",
            "\n",
            "\n",
            "        [[[-2.1822e-05]],\n",
            "\n",
            "         [[-1.6248e-05]],\n",
            "\n",
            "         [[-1.4606e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1051e-05]],\n",
            "\n",
            "         [[ 5.0380e-04]],\n",
            "\n",
            "         [[ 3.6476e-04]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-3.7393e-05]],\n",
            "\n",
            "         [[-1.4954e-05]],\n",
            "\n",
            "         [[-9.6059e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.6255e-06]],\n",
            "\n",
            "         [[ 5.9469e-04]],\n",
            "\n",
            "         [[-4.9922e-03]]],\n",
            "\n",
            "\n",
            "        [[[ 5.2335e-07]],\n",
            "\n",
            "         [[-4.2462e-08]],\n",
            "\n",
            "         [[-5.5636e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.6278e-08]],\n",
            "\n",
            "         [[ 2.6283e-07]],\n",
            "\n",
            "         [[-9.5597e-07]]],\n",
            "\n",
            "\n",
            "        [[[-6.7164e-09]],\n",
            "\n",
            "         [[-7.1255e-10]],\n",
            "\n",
            "         [[-3.5452e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.3854e-08]],\n",
            "\n",
            "         [[ 2.7883e-09]],\n",
            "\n",
            "         [[ 1.5396e-09]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.1.bn3\n",
            "bottlenecks.2\n",
            "bottlenecks.2.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[-1.0720e-10]],\n",
            "\n",
            "         [[ 1.2557e-11]],\n",
            "\n",
            "         [[-2.8552e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4310e-11]],\n",
            "\n",
            "         [[ 6.1941e-12]],\n",
            "\n",
            "         [[-1.5672e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 4.6261e-10]],\n",
            "\n",
            "         [[ 3.1945e-10]],\n",
            "\n",
            "         [[-5.5352e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.1319e-12]],\n",
            "\n",
            "         [[ 1.7570e-10]],\n",
            "\n",
            "         [[ 6.6693e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 3.1036e-08]],\n",
            "\n",
            "         [[ 3.0513e-09]],\n",
            "\n",
            "         [[-1.2301e-07]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.5306e-08]],\n",
            "\n",
            "         [[ 4.2703e-08]],\n",
            "\n",
            "         [[-2.7184e-09]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.0193e-08]],\n",
            "\n",
            "         [[-1.7665e-09]],\n",
            "\n",
            "         [[-4.0768e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.0188e-09]],\n",
            "\n",
            "         [[-1.1395e-09]],\n",
            "\n",
            "         [[-5.7439e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 8.5339e-12]],\n",
            "\n",
            "         [[ 1.4223e-11]],\n",
            "\n",
            "         [[-2.1519e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.9970e-11]],\n",
            "\n",
            "         [[ 1.5599e-11]],\n",
            "\n",
            "         [[ 2.3899e-11]]],\n",
            "\n",
            "\n",
            "        [[[-1.6572e-06]],\n",
            "\n",
            "         [[-6.5405e-09]],\n",
            "\n",
            "         [[-3.4741e-07]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0112e-06]],\n",
            "\n",
            "         [[ 6.2133e-08]],\n",
            "\n",
            "         [[-2.7122e-09]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.2.bn1\n",
            "bottlenecks.2.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[-2.6227e-11, -5.2187e-11,  1.3982e-10],\n",
            "          [-2.0833e-10, -4.0250e-11,  6.5985e-11],\n",
            "          [-1.8147e-10, -1.1641e-10, -8.2020e-11]]],\n",
            "\n",
            "\n",
            "        [[[-3.9072e-10, -2.3342e-10, -5.9516e-10],\n",
            "          [-3.1597e-10, -9.3704e-11, -5.4352e-10],\n",
            "          [-5.6238e-10, -7.5297e-10, -7.5560e-10]]],\n",
            "\n",
            "\n",
            "        [[[-4.3684e-08, -9.1607e-08, -9.6918e-08],\n",
            "          [ 1.0386e-07,  1.0746e-07,  1.1560e-07],\n",
            "          [-1.0495e-07, -1.8351e-07, -1.3788e-07]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-4.7472e-09, -7.3047e-09, -7.4803e-09],\n",
            "          [ 1.1190e-08, -2.8158e-09, -3.6594e-09],\n",
            "          [ 2.3846e-09, -7.5895e-09,  1.6849e-08]]],\n",
            "\n",
            "\n",
            "        [[[-6.3666e-12, -9.2494e-11, -1.2868e-10],\n",
            "          [ 2.4379e-10, -5.3042e-11,  1.3317e-11],\n",
            "          [ 8.5143e-11, -8.2668e-11, -1.2347e-10]]],\n",
            "\n",
            "\n",
            "        [[[-3.0522e-06, -5.3737e-07, -5.2557e-07],\n",
            "          [-3.4696e-06,  6.8376e-08, -6.4624e-07],\n",
            "          [-5.9422e-07,  1.5381e-06, -5.1219e-07]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.2.bn2\n",
            "bottlenecks.2.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.0817e-11]],\n",
            "\n",
            "         [[ 9.6366e-12]],\n",
            "\n",
            "         [[-3.1669e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.4614e-09]],\n",
            "\n",
            "         [[-2.4385e-11]],\n",
            "\n",
            "         [[ 7.6856e-08]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2096e-10]],\n",
            "\n",
            "         [[ 4.5437e-10]],\n",
            "\n",
            "         [[-3.6466e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.5959e-09]],\n",
            "\n",
            "         [[ 3.1658e-11]],\n",
            "\n",
            "         [[-8.6938e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 7.0478e-11]],\n",
            "\n",
            "         [[ 3.0691e-10]],\n",
            "\n",
            "         [[-1.6638e-07]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.1311e-08]],\n",
            "\n",
            "         [[-2.5596e-12]],\n",
            "\n",
            "         [[ 7.1930e-07]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-9.4685e-12]],\n",
            "\n",
            "         [[ 3.2439e-10]],\n",
            "\n",
            "         [[-3.0349e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7489e-09]],\n",
            "\n",
            "         [[ 3.4704e-12]],\n",
            "\n",
            "         [[-4.8241e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0103e-11]],\n",
            "\n",
            "         [[-1.1838e-11]],\n",
            "\n",
            "         [[-2.3839e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.4928e-09]],\n",
            "\n",
            "         [[ 3.8610e-11]],\n",
            "\n",
            "         [[ 8.0874e-09]]],\n",
            "\n",
            "\n",
            "        [[[-1.9542e-11]],\n",
            "\n",
            "         [[-1.3873e-10]],\n",
            "\n",
            "         [[-4.3131e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.0905e-09]],\n",
            "\n",
            "         [[ 8.2433e-11]],\n",
            "\n",
            "         [[ 1.6466e-08]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.2.bn3\n",
            "bottlenecks.3\n",
            "bottlenecks.3.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[-2.9954e-07]],\n",
            "\n",
            "         [[-9.4813e-08]],\n",
            "\n",
            "         [[-1.0158e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.8905e-07]],\n",
            "\n",
            "         [[ 3.5364e-07]],\n",
            "\n",
            "         [[-1.0111e-08]]],\n",
            "\n",
            "\n",
            "        [[[-3.1947e-06]],\n",
            "\n",
            "         [[ 6.7270e-09]],\n",
            "\n",
            "         [[-2.3867e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.8716e-05]],\n",
            "\n",
            "         [[-4.5043e-07]],\n",
            "\n",
            "         [[ 3.3718e-08]]],\n",
            "\n",
            "\n",
            "        [[[-4.3951e-13]],\n",
            "\n",
            "         [[ 3.5487e-11]],\n",
            "\n",
            "         [[ 4.5005e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8611e-11]],\n",
            "\n",
            "         [[ 7.8202e-12]],\n",
            "\n",
            "         [[ 1.4923e-12]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 5.5764e-07]],\n",
            "\n",
            "         [[-3.7270e-09]],\n",
            "\n",
            "         [[-4.7766e-07]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3187e-06]],\n",
            "\n",
            "         [[-4.6832e-07]],\n",
            "\n",
            "         [[-2.6329e-08]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0127e-02]],\n",
            "\n",
            "         [[-2.4983e-09]],\n",
            "\n",
            "         [[ 3.1845e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.6442e-01]],\n",
            "\n",
            "         [[-5.0829e-07]],\n",
            "\n",
            "         [[-5.5520e-08]]],\n",
            "\n",
            "\n",
            "        [[[-1.2204e-09]],\n",
            "\n",
            "         [[ 1.7098e-08]],\n",
            "\n",
            "         [[ 2.6805e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.9131e-08]],\n",
            "\n",
            "         [[-3.5327e-08]],\n",
            "\n",
            "         [[ 1.6470e-08]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.3.bn1\n",
            "bottlenecks.3.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[-4.6699e-08,  9.5323e-07, -2.6694e-07],\n",
            "          [-2.2806e-06,  1.5149e-06, -2.0392e-06],\n",
            "          [ 1.7768e-07,  8.0250e-07, -5.5906e-07]]],\n",
            "\n",
            "\n",
            "        [[[-4.6633e-05,  3.8223e-05, -8.5841e-06],\n",
            "          [ 7.4212e-05, -7.4834e-05,  2.8285e-05],\n",
            "          [-3.9224e-05,  6.0603e-05, -3.5128e-05]]],\n",
            "\n",
            "\n",
            "        [[[ 6.0837e-12,  1.3526e-11,  5.4085e-11],\n",
            "          [-1.7807e-11,  1.1499e-12,  1.7402e-11],\n",
            "          [ 1.9361e-11,  3.5434e-11, -3.7222e-11]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 7.4369e-07,  5.9325e-08, -4.2334e-07],\n",
            "          [ 1.9975e-06, -5.0838e-07, -5.8942e-07],\n",
            "          [ 7.8364e-07, -4.8384e-07, -1.1958e-06]]],\n",
            "\n",
            "\n",
            "        [[[-2.7072e-01, -5.2173e-01, -7.5713e-02],\n",
            "          [ 3.9254e-01,  5.9622e-01, -5.7383e-03],\n",
            "          [-3.5290e-02, -2.5552e-01,  3.9939e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.0022e-08, -3.5705e-08, -4.2007e-08],\n",
            "          [-5.1654e-08,  4.1170e-08, -5.1678e-09],\n",
            "          [-1.5911e-08,  5.0964e-08,  4.4954e-08]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.3.bn2\n",
            "bottlenecks.3.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[ 2.3089e-08]],\n",
            "\n",
            "         [[-3.2840e-08]],\n",
            "\n",
            "         [[ 7.5993e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1102e-08]],\n",
            "\n",
            "         [[ 3.9687e-08]],\n",
            "\n",
            "         [[-6.3215e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 2.6316e-08]],\n",
            "\n",
            "         [[ 2.1231e-05]],\n",
            "\n",
            "         [[-3.5754e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8450e-07]],\n",
            "\n",
            "         [[-2.1782e-04]],\n",
            "\n",
            "         [[ 5.1168e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 5.0408e-08]],\n",
            "\n",
            "         [[ 1.3607e-05]],\n",
            "\n",
            "         [[ 1.5102e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.2653e-07]],\n",
            "\n",
            "         [[ 4.5512e-05]],\n",
            "\n",
            "         [[ 2.4922e-08]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.0603e-06]],\n",
            "\n",
            "         [[ 9.3238e-06]],\n",
            "\n",
            "         [[ 3.1398e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.4768e-07]],\n",
            "\n",
            "         [[-1.3158e-04]],\n",
            "\n",
            "         [[-7.0116e-09]]],\n",
            "\n",
            "\n",
            "        [[[-1.1212e-08]],\n",
            "\n",
            "         [[-1.1383e-05]],\n",
            "\n",
            "         [[-1.5459e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.4701e-07]],\n",
            "\n",
            "         [[-4.8694e-02]],\n",
            "\n",
            "         [[-5.2730e-08]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2433e-07]],\n",
            "\n",
            "         [[-3.3129e-05]],\n",
            "\n",
            "         [[-1.2375e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.0958e-07]],\n",
            "\n",
            "         [[ 9.9633e-05]],\n",
            "\n",
            "         [[ 3.5077e-09]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.3.bn3\n",
            "bottlenecks.4\n",
            "bottlenecks.4.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[-6.5716e-11]],\n",
            "\n",
            "         [[ 5.7291e-11]],\n",
            "\n",
            "         [[-1.1201e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.8784e-11]],\n",
            "\n",
            "         [[ 2.7688e-11]],\n",
            "\n",
            "         [[ 2.0815e-11]]],\n",
            "\n",
            "\n",
            "        [[[-1.6433e-11]],\n",
            "\n",
            "         [[-3.7488e-11]],\n",
            "\n",
            "         [[ 9.8737e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.8818e-11]],\n",
            "\n",
            "         [[-2.2366e-11]],\n",
            "\n",
            "         [[-3.2657e-12]]],\n",
            "\n",
            "\n",
            "        [[[-1.3844e-09]],\n",
            "\n",
            "         [[ 3.3010e-04]],\n",
            "\n",
            "         [[ 3.9164e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.1112e-05]],\n",
            "\n",
            "         [[-9.6799e-02]],\n",
            "\n",
            "         [[ 1.3742e-04]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.5385e-09]],\n",
            "\n",
            "         [[ 4.5377e-05]],\n",
            "\n",
            "         [[ 1.3501e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.0013e-04]],\n",
            "\n",
            "         [[ 5.7752e-05]],\n",
            "\n",
            "         [[ 8.1032e-06]]],\n",
            "\n",
            "\n",
            "        [[[-9.6937e-09]],\n",
            "\n",
            "         [[ 1.4053e-04]],\n",
            "\n",
            "         [[ 1.4769e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.9978e-04]],\n",
            "\n",
            "         [[-9.6556e-02]],\n",
            "\n",
            "         [[ 2.5031e-04]]],\n",
            "\n",
            "\n",
            "        [[[-4.1743e-10]],\n",
            "\n",
            "         [[ 2.0515e-09]],\n",
            "\n",
            "         [[-1.8958e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.5043e-10]],\n",
            "\n",
            "         [[-2.2059e-09]],\n",
            "\n",
            "         [[-1.1213e-09]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.4.bn1\n",
            "bottlenecks.4.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[-3.9463e-11, -6.0379e-11,  3.0073e-11],\n",
            "          [-3.8608e-11, -5.7838e-11,  9.1735e-11],\n",
            "          [-3.6406e-11, -4.9965e-11,  4.9319e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7610e-12,  2.7606e-11,  1.3621e-11],\n",
            "          [-2.3441e-11,  4.2645e-11, -1.3329e-11],\n",
            "          [-1.1275e-12,  4.5882e-11,  7.8625e-12]]],\n",
            "\n",
            "\n",
            "        [[[-1.0321e-01, -2.3762e-01, -1.0971e-01],\n",
            "          [-1.6669e-01, -1.0565e-01,  4.4005e-03],\n",
            "          [ 9.7431e-02,  2.1182e-01,  3.0796e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.0033e-04, -1.1070e-06,  1.5150e-04],\n",
            "          [-6.8234e-05, -2.1421e-04, -4.0922e-05],\n",
            "          [ 1.4542e-04, -3.9206e-05, -1.3681e-04]]],\n",
            "\n",
            "\n",
            "        [[[-1.0763e-01, -1.8236e-01,  4.5637e-02],\n",
            "          [-1.8527e-01, -9.3654e-02,  2.4129e-01],\n",
            "          [-2.0786e-01, -2.0295e-02,  2.4906e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0168e-09, -1.4826e-09,  1.2753e-09],\n",
            "          [ 2.5443e-09, -1.9101e-09,  1.5999e-09],\n",
            "          [ 2.3954e-09, -1.0212e-09,  1.7164e-09]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.4.bn2\n",
            "bottlenecks.4.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[ 4.4030e-11]],\n",
            "\n",
            "         [[ 6.5289e-12]],\n",
            "\n",
            "         [[ 2.0880e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.5420e-05]],\n",
            "\n",
            "         [[-2.1607e-04]],\n",
            "\n",
            "         [[ 1.6531e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 2.5096e-11]],\n",
            "\n",
            "         [[ 2.9764e-11]],\n",
            "\n",
            "         [[ 7.0514e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.3460e-05]],\n",
            "\n",
            "         [[-3.6177e-07]],\n",
            "\n",
            "         [[-8.9541e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0304e-11]],\n",
            "\n",
            "         [[ 9.4703e-12]],\n",
            "\n",
            "         [[ 1.4120e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.7550e-10]],\n",
            "\n",
            "         [[-7.9604e-09]],\n",
            "\n",
            "         [[ 1.1771e-11]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 4.6100e-11]],\n",
            "\n",
            "         [[ 3.5578e-11]],\n",
            "\n",
            "         [[ 1.0704e-07]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3040e-06]],\n",
            "\n",
            "         [[ 9.8860e-07]],\n",
            "\n",
            "         [[ 7.3162e-10]]],\n",
            "\n",
            "\n",
            "        [[[-2.1817e-12]],\n",
            "\n",
            "         [[-2.2265e-11]],\n",
            "\n",
            "         [[-3.9381e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.3918e-09]],\n",
            "\n",
            "         [[-9.5472e-10]],\n",
            "\n",
            "         [[ 2.9994e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0112e-11]],\n",
            "\n",
            "         [[ 2.1249e-11]],\n",
            "\n",
            "         [[ 8.9559e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3094e-08]],\n",
            "\n",
            "         [[-3.1966e-09]],\n",
            "\n",
            "         [[ 1.7673e-10]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.4.bn3\n",
            "bottlenecks.5\n",
            "bottlenecks.5.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[-6.5700e-08]],\n",
            "\n",
            "         [[-9.9096e-06]],\n",
            "\n",
            "         [[ 7.9769e-07]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.7858e-08]],\n",
            "\n",
            "         [[ 1.0363e-04]],\n",
            "\n",
            "         [[-1.8325e-05]]],\n",
            "\n",
            "\n",
            "        [[[-1.2577e-07]],\n",
            "\n",
            "         [[ 1.3070e-05]],\n",
            "\n",
            "         [[ 1.1091e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6428e-08]],\n",
            "\n",
            "         [[ 5.0164e-06]],\n",
            "\n",
            "         [[-1.9088e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 4.8594e-10]],\n",
            "\n",
            "         [[ 1.3560e-11]],\n",
            "\n",
            "         [[-1.4631e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.0126e-09]],\n",
            "\n",
            "         [[-4.2433e-10]],\n",
            "\n",
            "         [[-9.1405e-10]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 2.0646e-07]],\n",
            "\n",
            "         [[-3.4874e-04]],\n",
            "\n",
            "         [[-2.3275e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.4034e-09]],\n",
            "\n",
            "         [[-2.4548e-04]],\n",
            "\n",
            "         [[-3.3253e-05]]],\n",
            "\n",
            "\n",
            "        [[[-6.6350e-11]],\n",
            "\n",
            "         [[ 8.7542e-11]],\n",
            "\n",
            "         [[-1.2979e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.4091e-10]],\n",
            "\n",
            "         [[ 1.6794e-11]],\n",
            "\n",
            "         [[-4.9719e-12]]],\n",
            "\n",
            "\n",
            "        [[[ 6.9924e-08]],\n",
            "\n",
            "         [[-2.0545e-04]],\n",
            "\n",
            "         [[ 3.9360e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.8378e-08]],\n",
            "\n",
            "         [[-1.4040e-02]],\n",
            "\n",
            "         [[ 1.7712e-05]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.5.bn1\n",
            "bottlenecks.5.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.6834e-05,  9.0416e-05,  1.3927e-04],\n",
            "          [ 1.0293e-04,  1.0173e-04,  2.0867e-06],\n",
            "          [-1.4651e-04, -1.4378e-04, -1.1889e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0220e-05,  7.9730e-06, -2.7700e-05],\n",
            "          [ 5.3116e-06,  4.5225e-06, -9.4607e-06],\n",
            "          [-2.2819e-05, -1.0026e-05,  3.5410e-05]]],\n",
            "\n",
            "\n",
            "        [[[-4.7973e-10,  1.5767e-09, -1.7969e-09],\n",
            "          [-9.2480e-10,  1.5641e-09, -1.9515e-09],\n",
            "          [-9.0448e-10,  8.5408e-10, -1.6823e-09]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-4.2975e-04,  4.6418e-04,  3.4526e-04],\n",
            "          [-4.9261e-04,  9.1926e-04,  4.6759e-04],\n",
            "          [ 5.9884e-05,  4.1669e-04,  2.1002e-04]]],\n",
            "\n",
            "\n",
            "        [[[-3.8654e-10,  2.7318e-10, -1.7888e-10],\n",
            "          [-1.8247e-10,  3.1954e-10, -1.3658e-10],\n",
            "          [-3.1479e-10, -7.8043e-11, -2.6198e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 3.0572e-03, -1.2672e-02,  6.0950e-03],\n",
            "          [ 3.8644e-03, -1.2547e-02,  4.0915e-03],\n",
            "          [-5.7001e-03,  1.4269e-02, -1.9042e-02]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.5.bn2\n",
            "bottlenecks.5.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[-1.5972e-04]],\n",
            "\n",
            "         [[-1.4030e-05]],\n",
            "\n",
            "         [[-1.6430e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.1634e-04]],\n",
            "\n",
            "         [[-2.4847e-10]],\n",
            "\n",
            "         [[ 2.4479e-04]]],\n",
            "\n",
            "\n",
            "        [[[ 3.6191e-05]],\n",
            "\n",
            "         [[-1.5360e-05]],\n",
            "\n",
            "         [[-1.8122e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.9744e-06]],\n",
            "\n",
            "         [[ 1.0178e-10]],\n",
            "\n",
            "         [[-5.0650e-05]]],\n",
            "\n",
            "\n",
            "        [[[ 3.8029e-06]],\n",
            "\n",
            "         [[-6.3480e-06]],\n",
            "\n",
            "         [[-1.2162e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.8520e-06]],\n",
            "\n",
            "         [[-7.7896e-11]],\n",
            "\n",
            "         [[-8.3251e-06]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 8.1442e-07]],\n",
            "\n",
            "         [[-7.3014e-07]],\n",
            "\n",
            "         [[-9.7363e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.3248e-06]],\n",
            "\n",
            "         [[-3.8644e-12]],\n",
            "\n",
            "         [[ 1.1374e-06]]],\n",
            "\n",
            "\n",
            "        [[[-1.6188e-09]],\n",
            "\n",
            "         [[-6.2867e-10]],\n",
            "\n",
            "         [[ 2.0124e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.4708e-09]],\n",
            "\n",
            "         [[ 5.9530e-11]],\n",
            "\n",
            "         [[ 3.3057e-09]]],\n",
            "\n",
            "\n",
            "        [[[-1.0620e-05]],\n",
            "\n",
            "         [[-1.4937e-06]],\n",
            "\n",
            "         [[-2.2615e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5190e-05]],\n",
            "\n",
            "         [[ 9.3392e-11]],\n",
            "\n",
            "         [[-8.2977e-06]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.5.bn3\n",
            "bottlenecks.6\n",
            "bottlenecks.6.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[-2.8419e-06]],\n",
            "\n",
            "         [[ 9.1227e-06]],\n",
            "\n",
            "         [[ 4.4578e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.1049e-06]],\n",
            "\n",
            "         [[ 1.8846e-06]],\n",
            "\n",
            "         [[ 1.3162e-06]]],\n",
            "\n",
            "\n",
            "        [[[-1.9942e-04]],\n",
            "\n",
            "         [[-1.8070e-04]],\n",
            "\n",
            "         [[ 4.7527e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.2316e-06]],\n",
            "\n",
            "         [[ 3.0227e-04]],\n",
            "\n",
            "         [[ 2.7559e-06]]],\n",
            "\n",
            "\n",
            "        [[[-1.1608e-04]],\n",
            "\n",
            "         [[ 1.3618e-04]],\n",
            "\n",
            "         [[-1.1497e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.8960e-07]],\n",
            "\n",
            "         [[ 3.6555e-05]],\n",
            "\n",
            "         [[-1.0566e-05]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.2041e-06]],\n",
            "\n",
            "         [[ 4.9488e-07]],\n",
            "\n",
            "         [[ 2.6895e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7147e-06]],\n",
            "\n",
            "         [[-4.5909e-05]],\n",
            "\n",
            "         [[ 5.7885e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 7.7868e-10]],\n",
            "\n",
            "         [[-2.0031e-10]],\n",
            "\n",
            "         [[ 1.3415e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.7573e-10]],\n",
            "\n",
            "         [[ 4.1242e-10]],\n",
            "\n",
            "         [[ 2.6566e-10]]],\n",
            "\n",
            "\n",
            "        [[[-1.4022e-10]],\n",
            "\n",
            "         [[ 5.5085e-11]],\n",
            "\n",
            "         [[-1.3618e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.6597e-11]],\n",
            "\n",
            "         [[-4.8666e-11]],\n",
            "\n",
            "         [[ 8.3740e-12]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.6.bn1\n",
            "bottlenecks.6.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[-1.8885e-06,  2.1046e-06,  3.3589e-06],\n",
            "          [ 2.8287e-06,  9.3338e-06,  6.1596e-06],\n",
            "          [-1.1109e-06,  7.5991e-07,  5.6189e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6698e-01,  1.9870e-01,  1.2603e-01],\n",
            "          [ 2.1978e-01,  3.2783e-01,  2.7728e-01],\n",
            "          [ 1.9249e-01,  2.7496e-01,  2.6706e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 4.2399e-05,  1.2359e-04,  4.9216e-05],\n",
            "          [ 5.0562e-05,  1.4195e-04,  9.3846e-05],\n",
            "          [ 2.9454e-05,  6.0878e-05,  2.7055e-05]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 7.3748e-06,  2.0310e-05,  1.2427e-05],\n",
            "          [ 1.5580e-06,  3.2314e-05,  2.1448e-05],\n",
            "          [ 2.6778e-07,  2.4154e-05,  1.1327e-05]]],\n",
            "\n",
            "\n",
            "        [[[-8.5111e-10, -3.0345e-10, -1.8937e-10],\n",
            "          [-1.0000e-09, -3.0187e-10, -8.3355e-10],\n",
            "          [-1.0590e-10, -8.5272e-10, -4.0746e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7609e-10,  2.4838e-11,  8.7513e-11],\n",
            "          [ 5.6181e-11,  4.0708e-11,  1.9346e-10],\n",
            "          [ 1.6783e-10,  1.4265e-10, -4.9443e-11]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.6.bn2\n",
            "bottlenecks.6.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.3910e-08]],\n",
            "\n",
            "         [[-1.1349e-08]],\n",
            "\n",
            "         [[ 2.3402e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.9255e-09]],\n",
            "\n",
            "         [[-1.3644e-10]],\n",
            "\n",
            "         [[-1.6453e-11]]],\n",
            "\n",
            "\n",
            "        [[[-2.9733e-06]],\n",
            "\n",
            "         [[ 3.1371e-03]],\n",
            "\n",
            "         [[-2.2518e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4434e-05]],\n",
            "\n",
            "         [[ 8.0652e-11]],\n",
            "\n",
            "         [[-2.6897e-12]]],\n",
            "\n",
            "\n",
            "        [[[-1.8191e-06]],\n",
            "\n",
            "         [[-7.2221e-06]],\n",
            "\n",
            "         [[-1.1059e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.2169e-06]],\n",
            "\n",
            "         [[-1.3457e-10]],\n",
            "\n",
            "         [[ 4.7856e-11]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 8.2376e-07]],\n",
            "\n",
            "         [[-1.0369e-05]],\n",
            "\n",
            "         [[ 3.1931e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.0565e-06]],\n",
            "\n",
            "         [[-3.6397e-10]],\n",
            "\n",
            "         [[ 6.1393e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.8497e-08]],\n",
            "\n",
            "         [[ 6.0949e-09]],\n",
            "\n",
            "         [[ 8.9654e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.1447e-08]],\n",
            "\n",
            "         [[ 3.6376e-11]],\n",
            "\n",
            "         [[ 7.6293e-11]]],\n",
            "\n",
            "\n",
            "        [[[-8.9343e-07]],\n",
            "\n",
            "         [[ 5.3687e-05]],\n",
            "\n",
            "         [[-2.4318e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0407e-05]],\n",
            "\n",
            "         [[-3.8374e-10]],\n",
            "\n",
            "         [[ 3.8702e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.6.bn3\n",
            "bottlenecks.7\n",
            "bottlenecks.7.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[-1.0302e-11]],\n",
            "\n",
            "         [[-2.1090e-11]],\n",
            "\n",
            "         [[ 1.8553e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3862e-11]],\n",
            "\n",
            "         [[-4.7610e-12]],\n",
            "\n",
            "         [[-1.6335e-11]]],\n",
            "\n",
            "\n",
            "        [[[-2.1315e-11]],\n",
            "\n",
            "         [[ 1.5461e-11]],\n",
            "\n",
            "         [[ 2.4097e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.8895e-12]],\n",
            "\n",
            "         [[ 3.3402e-13]],\n",
            "\n",
            "         [[ 1.1803e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 7.0451e-09]],\n",
            "\n",
            "         [[ 3.8471e-04]],\n",
            "\n",
            "         [[ 1.7843e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.8309e-06]],\n",
            "\n",
            "         [[-4.8566e-09]],\n",
            "\n",
            "         [[-7.8669e-05]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 3.5859e-11]],\n",
            "\n",
            "         [[ 8.0726e-11]],\n",
            "\n",
            "         [[ 1.6210e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.4149e-11]],\n",
            "\n",
            "         [[ 1.9214e-11]],\n",
            "\n",
            "         [[-1.2498e-11]]],\n",
            "\n",
            "\n",
            "        [[[-6.4598e-11]],\n",
            "\n",
            "         [[ 2.1156e-11]],\n",
            "\n",
            "         [[-3.9172e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.5231e-11]],\n",
            "\n",
            "         [[-2.4886e-11]],\n",
            "\n",
            "         [[ 3.1324e-11]]],\n",
            "\n",
            "\n",
            "        [[[-6.4405e-13]],\n",
            "\n",
            "         [[ 6.1002e-12]],\n",
            "\n",
            "         [[-1.0227e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0073e-11]],\n",
            "\n",
            "         [[ 1.3813e-12]],\n",
            "\n",
            "         [[ 1.4434e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.7.bn1\n",
            "bottlenecks.7.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[ 8.6382e-12,  4.3706e-11, -1.9518e-11],\n",
            "          [-8.1052e-12,  3.5000e-11,  1.8480e-12],\n",
            "          [ 1.0827e-12,  4.6753e-11, -4.2545e-12]]],\n",
            "\n",
            "\n",
            "        [[[ 3.2540e-12, -1.1429e-11, -1.5427e-11],\n",
            "          [ 9.5840e-11, -1.0526e-11,  4.1035e-11],\n",
            "          [ 4.5757e-11,  4.6852e-12,  3.3542e-11]]],\n",
            "\n",
            "\n",
            "        [[[-4.3983e-02,  8.7458e-03,  7.6820e-02],\n",
            "          [-9.0051e-02, -2.7716e-02,  1.0709e-01],\n",
            "          [ 5.1605e-02, -2.2622e-02, -1.9506e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.9974e-11, -1.1287e-11,  5.9948e-11],\n",
            "          [-1.0526e-10, -1.6575e-10, -7.6502e-11],\n",
            "          [-1.9256e-10,  4.9208e-10,  1.4461e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1285e-11,  1.9672e-10,  9.3586e-11],\n",
            "          [-1.1171e-10, -1.9902e-11,  6.7355e-12],\n",
            "          [-6.5498e-11, -4.3780e-11, -1.0731e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 6.0404e-11,  6.3352e-12, -2.2984e-11],\n",
            "          [ 6.0446e-11,  5.7634e-12, -2.1012e-11],\n",
            "          [ 2.5163e-11,  1.3012e-11, -9.5519e-12]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.7.bn2\n",
            "bottlenecks.7.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[ 2.3243e-11]],\n",
            "\n",
            "         [[ 2.4884e-11]],\n",
            "\n",
            "         [[-1.2569e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.2118e-11]],\n",
            "\n",
            "         [[ 3.6035e-11]],\n",
            "\n",
            "         [[ 2.3300e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 5.9934e-12]],\n",
            "\n",
            "         [[-5.9037e-11]],\n",
            "\n",
            "         [[ 5.4341e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.8663e-11]],\n",
            "\n",
            "         [[ 3.7338e-11]],\n",
            "\n",
            "         [[-1.9053e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 5.8317e-12]],\n",
            "\n",
            "         [[-3.1443e-11]],\n",
            "\n",
            "         [[ 1.0690e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.9743e-11]],\n",
            "\n",
            "         [[ 3.1687e-11]],\n",
            "\n",
            "         [[ 1.9343e-11]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.3114e-11]],\n",
            "\n",
            "         [[ 1.5329e-11]],\n",
            "\n",
            "         [[-4.5004e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3503e-10]],\n",
            "\n",
            "         [[ 5.8750e-13]],\n",
            "\n",
            "         [[-8.0632e-12]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7215e-11]],\n",
            "\n",
            "         [[-2.2643e-11]],\n",
            "\n",
            "         [[-2.5688e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.6857e-10]],\n",
            "\n",
            "         [[ 6.6163e-11]],\n",
            "\n",
            "         [[-1.0294e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 7.1188e-12]],\n",
            "\n",
            "         [[-1.1109e-11]],\n",
            "\n",
            "         [[-1.2639e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.1696e-11]],\n",
            "\n",
            "         [[ 2.9207e-11]],\n",
            "\n",
            "         [[-3.2504e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.7.bn3\n",
            "bottlenecks.8\n",
            "bottlenecks.8.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.9238e-10]],\n",
            "\n",
            "         [[-3.4534e-10]],\n",
            "\n",
            "         [[ 2.7436e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.6355e-10]],\n",
            "\n",
            "         [[-2.8934e-10]],\n",
            "\n",
            "         [[ 1.1778e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 1.4203e-07]],\n",
            "\n",
            "         [[ 1.7776e-03]],\n",
            "\n",
            "         [[ 3.4129e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.6456e-08]],\n",
            "\n",
            "         [[-1.9911e-09]],\n",
            "\n",
            "         [[ 8.7557e-05]]],\n",
            "\n",
            "\n",
            "        [[[-1.4618e-09]],\n",
            "\n",
            "         [[ 5.5177e-09]],\n",
            "\n",
            "         [[ 1.8045e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3515e-09]],\n",
            "\n",
            "         [[ 1.0383e-09]],\n",
            "\n",
            "         [[-1.0723e-08]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-3.5107e-10]],\n",
            "\n",
            "         [[ 7.0333e-12]],\n",
            "\n",
            "         [[-1.4497e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.9282e-10]],\n",
            "\n",
            "         [[ 1.3943e-10]],\n",
            "\n",
            "         [[ 1.4072e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 2.8841e-11]],\n",
            "\n",
            "         [[-1.0365e-10]],\n",
            "\n",
            "         [[ 1.3771e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.1108e-11]],\n",
            "\n",
            "         [[ 4.3130e-11]],\n",
            "\n",
            "         [[ 4.2659e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3424e-12]],\n",
            "\n",
            "         [[ 7.6017e-13]],\n",
            "\n",
            "         [[-4.2337e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.8432e-11]],\n",
            "\n",
            "         [[-1.9440e-11]],\n",
            "\n",
            "         [[ 3.3386e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.8.bn1\n",
            "bottlenecks.8.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.5988e-09,  3.9808e-10,  8.5218e-10],\n",
            "          [-1.2878e-10, -1.3869e-09, -7.1032e-10],\n",
            "          [ 4.8104e-10,  1.2488e-09,  3.0784e-10]]],\n",
            "\n",
            "\n",
            "        [[[-1.5326e-01, -2.4325e-02,  1.2895e-01],\n",
            "          [-2.0527e-01,  2.9320e-03,  2.6435e-01],\n",
            "          [-2.9864e-02, -5.7758e-04,  9.9257e-02]]],\n",
            "\n",
            "\n",
            "        [[[-5.8567e-09, -9.7834e-09,  3.9805e-09],\n",
            "          [-3.6387e-09, -1.0860e-08, -1.1653e-08],\n",
            "          [ 1.7475e-08,  1.6663e-08,  5.9615e-09]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.2715e-11,  5.9606e-10, -5.5793e-10],\n",
            "          [-2.4348e-10,  4.3643e-10, -1.1579e-09],\n",
            "          [-2.8462e-10,  5.7372e-10,  2.7140e-10]]],\n",
            "\n",
            "\n",
            "        [[[-2.8358e-11, -2.8604e-12, -9.9337e-11],\n",
            "          [ 2.2616e-10,  2.9614e-10,  1.7876e-10],\n",
            "          [-9.7467e-11, -7.0813e-11, -1.5065e-10]]],\n",
            "\n",
            "\n",
            "        [[[-1.6904e-12,  5.9347e-11,  3.1409e-11],\n",
            "          [-9.8810e-11, -7.7178e-11, -1.1357e-10],\n",
            "          [ 6.2504e-11,  9.8261e-11,  5.9227e-11]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.8.bn2\n",
            "bottlenecks.8.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[-1.0610e-10]],\n",
            "\n",
            "         [[-1.0226e-01]],\n",
            "\n",
            "         [[ 1.4285e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.2627e-11]],\n",
            "\n",
            "         [[ 2.8833e-11]],\n",
            "\n",
            "         [[-4.7933e-13]]],\n",
            "\n",
            "\n",
            "        [[[-8.9878e-10]],\n",
            "\n",
            "         [[ 2.4271e-04]],\n",
            "\n",
            "         [[ 1.1926e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.6592e-11]],\n",
            "\n",
            "         [[-1.5021e-10]],\n",
            "\n",
            "         [[ 1.9967e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7602e-10]],\n",
            "\n",
            "         [[ 7.6219e-08]],\n",
            "\n",
            "         [[ 1.6909e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.4716e-11]],\n",
            "\n",
            "         [[-2.2604e-10]],\n",
            "\n",
            "         [[ 3.4897e-12]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.0612e-09]],\n",
            "\n",
            "         [[-1.7828e-04]],\n",
            "\n",
            "         [[-3.0685e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.0316e-10]],\n",
            "\n",
            "         [[-2.7101e-11]],\n",
            "\n",
            "         [[-1.0483e-11]]],\n",
            "\n",
            "\n",
            "        [[[-4.9523e-10]],\n",
            "\n",
            "         [[-1.5235e-05]],\n",
            "\n",
            "         [[-5.1914e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.0006e-10]],\n",
            "\n",
            "         [[ 4.6028e-11]],\n",
            "\n",
            "         [[ 2.3758e-11]]],\n",
            "\n",
            "\n",
            "        [[[-4.2525e-11]],\n",
            "\n",
            "         [[ 2.2211e-09]],\n",
            "\n",
            "         [[ 2.7421e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.1984e-11]],\n",
            "\n",
            "         [[-2.7272e-10]],\n",
            "\n",
            "         [[ 2.9281e-13]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.8.bn3\n",
            "bottlenecks.9\n",
            "bottlenecks.9.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[-1.2385e-10]],\n",
            "\n",
            "         [[-1.0640e-10]],\n",
            "\n",
            "         [[ 4.2740e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.6733e-11]],\n",
            "\n",
            "         [[-1.7899e-10]],\n",
            "\n",
            "         [[-1.4296e-10]]],\n",
            "\n",
            "\n",
            "        [[[-1.1432e-11]],\n",
            "\n",
            "         [[ 1.1791e-10]],\n",
            "\n",
            "         [[ 2.7185e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.8239e-11]],\n",
            "\n",
            "         [[-6.5214e-13]],\n",
            "\n",
            "         [[ 8.3037e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3818e-09]],\n",
            "\n",
            "         [[-1.4966e-09]],\n",
            "\n",
            "         [[-3.6477e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.8944e-10]],\n",
            "\n",
            "         [[-1.6902e-09]],\n",
            "\n",
            "         [[ 4.0900e-10]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 4.4568e-11]],\n",
            "\n",
            "         [[-1.0375e-11]],\n",
            "\n",
            "         [[ 1.1827e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5680e-11]],\n",
            "\n",
            "         [[ 2.9023e-11]],\n",
            "\n",
            "         [[-1.6134e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0056e-11]],\n",
            "\n",
            "         [[ 3.9761e-12]],\n",
            "\n",
            "         [[ 2.1425e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8079e-11]],\n",
            "\n",
            "         [[-3.5723e-11]],\n",
            "\n",
            "         [[-5.7587e-12]]],\n",
            "\n",
            "\n",
            "        [[[-1.0660e-12]],\n",
            "\n",
            "         [[-4.3872e-10]],\n",
            "\n",
            "         [[ 3.7479e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.2642e-10]],\n",
            "\n",
            "         [[ 8.5589e-11]],\n",
            "\n",
            "         [[-2.2195e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.9.bn1\n",
            "bottlenecks.9.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[-9.6591e-11, -1.5154e-11,  3.2988e-10],\n",
            "          [-1.6266e-10, -1.1289e-10,  2.4595e-11],\n",
            "          [-9.8168e-11,  1.5011e-11, -9.0215e-11]]],\n",
            "\n",
            "\n",
            "        [[[-3.7474e-11, -6.1800e-11,  2.4258e-11],\n",
            "          [ 6.6941e-11, -1.8034e-11,  5.5924e-11],\n",
            "          [ 1.3913e-10,  1.3751e-10,  1.1480e-10]]],\n",
            "\n",
            "\n",
            "        [[[-1.6353e-10, -7.7984e-10,  9.0065e-10],\n",
            "          [-3.5724e-10,  3.3489e-10,  1.5314e-09],\n",
            "          [-1.7677e-09,  2.9515e-10,  1.6650e-09]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.0521e-11, -1.2585e-11,  1.6178e-12],\n",
            "          [ 6.7570e-11,  6.9825e-11,  6.6530e-11],\n",
            "          [-2.0308e-11,  8.3274e-11,  2.9837e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 7.7140e-11, -1.4571e-11,  1.0215e-11],\n",
            "          [ 3.6891e-11, -1.1289e-11, -1.6272e-11],\n",
            "          [ 5.7055e-12,  1.0444e-11, -5.7148e-12]]],\n",
            "\n",
            "\n",
            "        [[[-5.8975e-10, -4.8751e-10, -1.1948e-10],\n",
            "          [-2.2955e-10, -1.6447e-10, -6.2162e-10],\n",
            "          [ 1.1765e-09,  8.8788e-10, -1.9817e-10]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.9.bn2\n",
            "bottlenecks.9.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[ 2.2335e-11]],\n",
            "\n",
            "         [[-5.7664e-11]],\n",
            "\n",
            "         [[-2.7619e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3320e-11]],\n",
            "\n",
            "         [[-1.7743e-11]],\n",
            "\n",
            "         [[ 1.1274e-10]]],\n",
            "\n",
            "\n",
            "        [[[-4.2191e-11]],\n",
            "\n",
            "         [[-5.6454e-11]],\n",
            "\n",
            "         [[-1.5612e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.9570e-12]],\n",
            "\n",
            "         [[-1.4942e-11]],\n",
            "\n",
            "         [[ 3.3312e-10]]],\n",
            "\n",
            "\n",
            "        [[[-8.4103e-12]],\n",
            "\n",
            "         [[ 4.4931e-12]],\n",
            "\n",
            "         [[-6.0062e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0179e-11]],\n",
            "\n",
            "         [[ 9.8449e-13]],\n",
            "\n",
            "         [[ 3.7162e-10]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-5.8727e-11]],\n",
            "\n",
            "         [[-1.0620e-10]],\n",
            "\n",
            "         [[ 1.8620e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.9178e-11]],\n",
            "\n",
            "         [[-4.4214e-12]],\n",
            "\n",
            "         [[-7.2513e-10]]],\n",
            "\n",
            "\n",
            "        [[[-4.8629e-11]],\n",
            "\n",
            "         [[ 4.7203e-11]],\n",
            "\n",
            "         [[-2.5333e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.8060e-11]],\n",
            "\n",
            "         [[ 2.1933e-11]],\n",
            "\n",
            "         [[-2.8066e-10]]],\n",
            "\n",
            "\n",
            "        [[[-1.2023e-11]],\n",
            "\n",
            "         [[-3.3022e-11]],\n",
            "\n",
            "         [[ 7.1574e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2006e-13]],\n",
            "\n",
            "         [[ 1.3675e-12]],\n",
            "\n",
            "         [[-2.6792e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.9.bn3\n",
            "bottlenecks.10\n",
            "bottlenecks.10.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[ 7.1009e-06]],\n",
            "\n",
            "         [[-6.2517e-05]],\n",
            "\n",
            "         [[ 5.1649e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3696e-07]],\n",
            "\n",
            "         [[-3.7314e-07]],\n",
            "\n",
            "         [[ 2.5059e-05]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3346e-09]],\n",
            "\n",
            "         [[-7.0227e-09]],\n",
            "\n",
            "         [[ 8.9559e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.2188e-09]],\n",
            "\n",
            "         [[ 1.1515e-09]],\n",
            "\n",
            "         [[ 2.8735e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 3.0307e-06]],\n",
            "\n",
            "         [[ 5.4643e-04]],\n",
            "\n",
            "         [[-5.3641e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.2316e-07]],\n",
            "\n",
            "         [[-2.2559e-08]],\n",
            "\n",
            "         [[ 2.8948e-05]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-3.1153e-06]],\n",
            "\n",
            "         [[ 9.4838e-05]],\n",
            "\n",
            "         [[ 5.4376e-07]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.3553e-07]],\n",
            "\n",
            "         [[-4.4167e-08]],\n",
            "\n",
            "         [[ 4.3845e-05]]],\n",
            "\n",
            "\n",
            "        [[[-8.8118e-12]],\n",
            "\n",
            "         [[ 3.6406e-12]],\n",
            "\n",
            "         [[ 6.6472e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1942e-10]],\n",
            "\n",
            "         [[ 7.6240e-12]],\n",
            "\n",
            "         [[-4.4880e-11]]],\n",
            "\n",
            "\n",
            "        [[[-3.8479e-11]],\n",
            "\n",
            "         [[-1.1515e-12]],\n",
            "\n",
            "         [[ 4.8358e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.4850e-11]],\n",
            "\n",
            "         [[ 2.5768e-11]],\n",
            "\n",
            "         [[-2.4326e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.10.bn1\n",
            "bottlenecks.10.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[-1.9459e-01,  8.5418e-02,  1.9255e-01],\n",
            "          [-6.0029e-02, -2.6147e-02,  5.7333e-01],\n",
            "          [ 2.5459e-01,  6.0884e-02, -2.5105e-01]]],\n",
            "\n",
            "\n",
            "        [[[-9.9298e-11,  4.0277e-09,  2.2018e-09],\n",
            "          [-5.2891e-09, -6.2997e-09, -8.9084e-09],\n",
            "          [ 5.7015e-10, -5.8979e-09, -1.1039e-08]]],\n",
            "\n",
            "\n",
            "        [[[ 1.1840e-01, -5.5110e-02, -6.2235e-02],\n",
            "          [ 7.9353e-02,  3.6489e-01,  2.3430e-01],\n",
            "          [-6.6270e-02,  3.0400e-01,  2.3560e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-7.3650e-05, -6.3111e-05, -8.5364e-05],\n",
            "          [ 3.6611e-05, -9.6271e-05, -5.3011e-05],\n",
            "          [ 8.8366e-05, -1.0326e-04, -7.0883e-05]]],\n",
            "\n",
            "\n",
            "        [[[-9.7695e-12, -7.3642e-11, -6.3614e-11],\n",
            "          [-3.4053e-11, -8.4673e-11, -9.0463e-11],\n",
            "          [-3.6088e-11,  7.9420e-12, -1.3948e-11]]],\n",
            "\n",
            "\n",
            "        [[[-2.0072e-11, -2.9475e-12, -1.5703e-11],\n",
            "          [ 2.3158e-11, -1.9440e-11, -1.6884e-11],\n",
            "          [ 7.8821e-11,  9.6332e-11, -4.2348e-11]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.10.bn2\n",
            "bottlenecks.10.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[ 3.1554e-01]],\n",
            "\n",
            "         [[ 1.5922e-10]],\n",
            "\n",
            "         [[-1.8906e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.2000e-05]],\n",
            "\n",
            "         [[-4.8302e-12]],\n",
            "\n",
            "         [[-3.4974e-11]]],\n",
            "\n",
            "\n",
            "        [[[-4.0699e-02]],\n",
            "\n",
            "         [[-2.3141e-09]],\n",
            "\n",
            "         [[ 1.6548e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5780e-04]],\n",
            "\n",
            "         [[ 4.4230e-11]],\n",
            "\n",
            "         [[ 1.7196e-11]]],\n",
            "\n",
            "\n",
            "        [[[-2.1010e-08]],\n",
            "\n",
            "         [[-6.2376e-10]],\n",
            "\n",
            "         [[ 7.5487e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.6658e-09]],\n",
            "\n",
            "         [[-1.5659e-11]],\n",
            "\n",
            "         [[-1.6219e-11]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 8.4805e-05]],\n",
            "\n",
            "         [[ 5.4093e-09]],\n",
            "\n",
            "         [[-1.0790e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.0099e-05]],\n",
            "\n",
            "         [[ 6.4479e-11]],\n",
            "\n",
            "         [[-2.5841e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 3.4558e-04]],\n",
            "\n",
            "         [[ 2.9039e-10]],\n",
            "\n",
            "         [[ 2.3497e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5385e-04]],\n",
            "\n",
            "         [[ 5.2902e-11]],\n",
            "\n",
            "         [[-2.3501e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 6.2227e-06]],\n",
            "\n",
            "         [[ 5.7246e-09]],\n",
            "\n",
            "         [[ 1.2201e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.4398e-06]],\n",
            "\n",
            "         [[ 3.5579e-11]],\n",
            "\n",
            "         [[ 1.9607e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.10.bn3\n",
            "bottlenecks.11\n",
            "bottlenecks.11.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[-1.2015e-11]],\n",
            "\n",
            "         [[-7.0282e-11]],\n",
            "\n",
            "         [[ 2.5756e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.0754e-11]],\n",
            "\n",
            "         [[ 6.6356e-12]],\n",
            "\n",
            "         [[ 4.4323e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 3.8511e-12]],\n",
            "\n",
            "         [[-1.7905e-11]],\n",
            "\n",
            "         [[-6.4745e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.0737e-11]],\n",
            "\n",
            "         [[-2.8971e-11]],\n",
            "\n",
            "         [[ 3.7296e-11]]],\n",
            "\n",
            "\n",
            "        [[[-2.6874e-02]],\n",
            "\n",
            "         [[ 1.4279e-02]],\n",
            "\n",
            "         [[-5.6255e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.7630e-06]],\n",
            "\n",
            "         [[ 3.9854e-05]],\n",
            "\n",
            "         [[-4.8123e-04]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 2.0532e-09]],\n",
            "\n",
            "         [[-1.0774e-08]],\n",
            "\n",
            "         [[ 3.7217e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.7062e-09]],\n",
            "\n",
            "         [[ 9.1919e-09]],\n",
            "\n",
            "         [[ 7.6586e-09]]],\n",
            "\n",
            "\n",
            "        [[[-7.3324e-10]],\n",
            "\n",
            "         [[ 4.4320e-09]],\n",
            "\n",
            "         [[ 1.5604e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5376e-09]],\n",
            "\n",
            "         [[-3.3726e-09]],\n",
            "\n",
            "         [[-6.7812e-10]]],\n",
            "\n",
            "\n",
            "        [[[-1.6852e-06]],\n",
            "\n",
            "         [[-4.0823e-06]],\n",
            "\n",
            "         [[-1.4411e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.7832e-06]],\n",
            "\n",
            "         [[-1.8428e-06]],\n",
            "\n",
            "         [[ 2.4205e-06]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.11.bn1\n",
            "bottlenecks.11.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.7569e-11,  5.8411e-12, -3.6140e-11],\n",
            "          [ 7.6296e-11,  5.5042e-11,  2.8195e-11],\n",
            "          [ 8.9262e-11,  2.7931e-11,  5.0887e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 2.9002e-11,  7.8013e-12, -3.7607e-11],\n",
            "          [ 9.1520e-11, -1.7411e-11, -2.1725e-11],\n",
            "          [ 3.6115e-11, -5.9215e-11, -1.6221e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 3.2017e-02, -1.4369e-02,  3.0383e-02],\n",
            "          [ 2.4325e-02, -4.2861e-02,  2.1061e-02],\n",
            "          [-1.3282e-02, -1.6492e-02,  1.3093e-03]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-4.8071e-09, -3.8659e-09, -6.9332e-09],\n",
            "          [-1.6370e-09,  1.9372e-09, -5.6584e-09],\n",
            "          [-7.0440e-09, -5.7383e-09, -1.0312e-08]]],\n",
            "\n",
            "\n",
            "        [[[-1.8299e-09, -2.2319e-09, -1.5887e-09],\n",
            "          [-1.6212e-09, -3.3264e-09, -1.7817e-09],\n",
            "          [ 2.6630e-09,  5.4555e-09,  2.8782e-09]]],\n",
            "\n",
            "\n",
            "        [[[-4.8033e-06, -2.3542e-06, -3.2742e-07],\n",
            "          [-5.4290e-06,  2.3456e-07,  2.6201e-06],\n",
            "          [-3.4211e-06,  5.4953e-07,  6.0590e-06]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.11.bn2\n",
            "bottlenecks.11.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[-2.4081e-11]],\n",
            "\n",
            "         [[ 2.2581e-11]],\n",
            "\n",
            "         [[-8.3894e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.2378e-09]],\n",
            "\n",
            "         [[-7.6557e-11]],\n",
            "\n",
            "         [[ 8.4689e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 3.9280e-11]],\n",
            "\n",
            "         [[ 2.0469e-13]],\n",
            "\n",
            "         [[-1.9877e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.0070e-11]],\n",
            "\n",
            "         [[ 1.0125e-11]],\n",
            "\n",
            "         [[ 1.9531e-10]]],\n",
            "\n",
            "\n",
            "        [[[-5.1166e-11]],\n",
            "\n",
            "         [[ 1.7962e-11]],\n",
            "\n",
            "         [[-2.8314e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.4099e-10]],\n",
            "\n",
            "         [[ 2.0432e-09]],\n",
            "\n",
            "         [[-6.5028e-09]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 4.6629e-12]],\n",
            "\n",
            "         [[ 9.0551e-13]],\n",
            "\n",
            "         [[ 4.8825e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.6989e-10]],\n",
            "\n",
            "         [[-6.3641e-10]],\n",
            "\n",
            "         [[ 1.3471e-09]]],\n",
            "\n",
            "\n",
            "        [[[-2.1312e-11]],\n",
            "\n",
            "         [[-1.1005e-11]],\n",
            "\n",
            "         [[ 1.1344e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.7158e-09]],\n",
            "\n",
            "         [[ 2.1694e-09]],\n",
            "\n",
            "         [[ 1.9615e-06]]],\n",
            "\n",
            "\n",
            "        [[[ 2.0161e-11]],\n",
            "\n",
            "         [[ 2.3119e-12]],\n",
            "\n",
            "         [[-9.2976e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.6913e-09]],\n",
            "\n",
            "         [[-2.6395e-10]],\n",
            "\n",
            "         [[ 7.7659e-07]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.11.bn3\n",
            "bottlenecks.12\n",
            "bottlenecks.12.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[-1.4114e-10]],\n",
            "\n",
            "         [[ 1.5683e-09]],\n",
            "\n",
            "         [[-5.0688e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5075e-09]],\n",
            "\n",
            "         [[-4.3197e-10]],\n",
            "\n",
            "         [[-4.1068e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6821e-10]],\n",
            "\n",
            "         [[-6.4933e-11]],\n",
            "\n",
            "         [[-9.0231e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.7228e-11]],\n",
            "\n",
            "         [[ 2.3387e-10]],\n",
            "\n",
            "         [[ 9.4396e-11]]],\n",
            "\n",
            "\n",
            "        [[[-3.4887e-11]],\n",
            "\n",
            "         [[-3.0800e-11]],\n",
            "\n",
            "         [[ 6.8693e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.8907e-11]],\n",
            "\n",
            "         [[ 3.2875e-11]],\n",
            "\n",
            "         [[-1.6879e-11]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-9.4825e-12]],\n",
            "\n",
            "         [[ 5.7062e-11]],\n",
            "\n",
            "         [[ 4.1662e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.4102e-11]],\n",
            "\n",
            "         [[-1.3625e-10]],\n",
            "\n",
            "         [[-5.2822e-11]]],\n",
            "\n",
            "\n",
            "        [[[-8.5036e-03]],\n",
            "\n",
            "         [[-2.3646e-03]],\n",
            "\n",
            "         [[ 1.2988e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.8056e-07]],\n",
            "\n",
            "         [[-2.0586e-04]],\n",
            "\n",
            "         [[ 4.3634e-05]]],\n",
            "\n",
            "\n",
            "        [[[-3.1207e-12]],\n",
            "\n",
            "         [[-1.8354e-11]],\n",
            "\n",
            "         [[ 5.3753e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.6087e-11]],\n",
            "\n",
            "         [[-1.5800e-11]],\n",
            "\n",
            "         [[-7.8105e-13]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.12.bn1\n",
            "bottlenecks.12.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[-9.3400e-10,  6.4263e-09, -2.8724e-09],\n",
            "          [-1.1693e-09,  1.0234e-09, -2.5469e-09],\n",
            "          [-1.5334e-09,  2.0455e-09, -2.3240e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 9.8069e-11,  2.9248e-10,  3.7761e-10],\n",
            "          [-2.5263e-10, -1.1887e-10, -7.2276e-11],\n",
            "          [-4.1759e-11, -7.9536e-11, -1.1930e-10]]],\n",
            "\n",
            "\n",
            "        [[[-1.0076e-10, -7.8265e-11, -3.7618e-11],\n",
            "          [-8.4755e-11, -8.4173e-12, -5.4285e-12],\n",
            "          [-6.4808e-11,  7.1360e-11,  1.4973e-10]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-6.3461e-11, -6.9719e-11, -6.9868e-11],\n",
            "          [-2.4075e-11, -4.5368e-11, -8.0735e-11],\n",
            "          [ 9.8125e-11,  2.0876e-10,  1.2965e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0625e-03,  1.3757e-02, -1.2020e-02],\n",
            "          [-1.4541e-02,  3.5541e-02,  1.0568e-02],\n",
            "          [-1.2270e-02,  5.4339e-03, -8.1710e-03]]],\n",
            "\n",
            "\n",
            "        [[[-2.4279e-11, -7.9772e-12, -1.7765e-11],\n",
            "          [-2.1718e-11, -8.5755e-12, -1.8268e-11],\n",
            "          [-2.1459e-11,  1.4009e-12, -2.5516e-11]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.12.bn2\n",
            "bottlenecks.12.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[-1.6696e-10]],\n",
            "\n",
            "         [[ 3.9507e-11]],\n",
            "\n",
            "         [[ 3.8835e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.5530e-11]],\n",
            "\n",
            "         [[-1.7570e-09]],\n",
            "\n",
            "         [[-7.8251e-12]]],\n",
            "\n",
            "\n",
            "        [[[-3.8733e-10]],\n",
            "\n",
            "         [[ 6.0936e-12]],\n",
            "\n",
            "         [[ 1.4546e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.4956e-13]],\n",
            "\n",
            "         [[-1.9510e-04]],\n",
            "\n",
            "         [[-1.9032e-13]]],\n",
            "\n",
            "\n",
            "        [[[ 4.6805e-10]],\n",
            "\n",
            "         [[ 3.0980e-11]],\n",
            "\n",
            "         [[-3.6559e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.7605e-11]],\n",
            "\n",
            "         [[-1.8457e-09]],\n",
            "\n",
            "         [[ 1.0426e-11]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-3.3658e-10]],\n",
            "\n",
            "         [[-1.9696e-11]],\n",
            "\n",
            "         [[-3.1271e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.8732e-11]],\n",
            "\n",
            "         [[-8.8351e-09]],\n",
            "\n",
            "         [[ 9.9319e-12]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5388e-09]],\n",
            "\n",
            "         [[-4.1253e-12]],\n",
            "\n",
            "         [[ 8.8693e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.6031e-11]],\n",
            "\n",
            "         [[-1.3418e-03]],\n",
            "\n",
            "         [[ 9.1449e-12]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3249e-09]],\n",
            "\n",
            "         [[-2.8324e-10]],\n",
            "\n",
            "         [[-2.5923e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.9576e-12]],\n",
            "\n",
            "         [[-2.9512e-07]],\n",
            "\n",
            "         [[ 2.9880e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.12.bn3\n",
            "bottlenecks.13\n",
            "bottlenecks.13.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[ 2.9165e-11]],\n",
            "\n",
            "         [[-3.6668e-12]],\n",
            "\n",
            "         [[ 1.1260e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.4189e-11]],\n",
            "\n",
            "         [[-5.7268e-12]],\n",
            "\n",
            "         [[-2.4395e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5336e-09]],\n",
            "\n",
            "         [[-2.5251e-09]],\n",
            "\n",
            "         [[ 5.5213e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4018e-09]],\n",
            "\n",
            "         [[-9.2736e-10]],\n",
            "\n",
            "         [[ 1.8043e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 4.4042e-11]],\n",
            "\n",
            "         [[-1.2891e-10]],\n",
            "\n",
            "         [[-5.0711e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.4159e-11]],\n",
            "\n",
            "         [[ 7.9634e-11]],\n",
            "\n",
            "         [[ 5.5441e-11]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-3.0077e-08]],\n",
            "\n",
            "         [[-4.6576e-09]],\n",
            "\n",
            "         [[-1.8366e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2757e-08]],\n",
            "\n",
            "         [[ 7.8683e-08]],\n",
            "\n",
            "         [[-1.8974e-07]]],\n",
            "\n",
            "\n",
            "        [[[ 1.2754e-11]],\n",
            "\n",
            "         [[-1.0168e-10]],\n",
            "\n",
            "         [[ 4.3819e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2141e-11]],\n",
            "\n",
            "         [[-1.5778e-11]],\n",
            "\n",
            "         [[-5.2761e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 7.9928e-11]],\n",
            "\n",
            "         [[-2.6928e-11]],\n",
            "\n",
            "         [[ 6.4859e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.3407e-11]],\n",
            "\n",
            "         [[-4.6896e-11]],\n",
            "\n",
            "         [[-3.5816e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.13.bn1\n",
            "bottlenecks.13.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[-2.5573e-11, -1.3288e-11,  9.5503e-12],\n",
            "          [ 3.8599e-12,  1.5681e-11,  5.5658e-11],\n",
            "          [ 2.1332e-11,  2.9021e-11,  6.8821e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.9354e-09,  2.9433e-09, -8.9307e-11],\n",
            "          [ 2.6674e-09,  3.4584e-09, -6.0788e-10],\n",
            "          [ 4.6271e-09, -1.9708e-11,  3.8059e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 2.7769e-11,  1.1240e-10, -5.0840e-11],\n",
            "          [-9.9348e-11,  1.9990e-10,  2.3837e-10],\n",
            "          [-1.2015e-10, -1.3832e-11,  2.1151e-10]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-5.9082e-08, -2.1348e-07, -1.4786e-07],\n",
            "          [-2.8024e-07,  6.2708e-08, -3.3034e-07],\n",
            "          [-1.2417e-07, -8.7832e-08, -2.6446e-07]]],\n",
            "\n",
            "\n",
            "        [[[-7.0488e-11, -4.1273e-11, -4.1527e-11],\n",
            "          [-1.1404e-10, -8.8958e-11, -3.4447e-11],\n",
            "          [-4.1158e-11, -5.4978e-11, -6.8669e-11]]],\n",
            "\n",
            "\n",
            "        [[[-9.8578e-11, -5.2246e-11, -8.9205e-12],\n",
            "          [ 1.7153e-10,  1.6269e-10,  1.2958e-10],\n",
            "          [ 1.8876e-10,  1.7219e-10,  1.2709e-10]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.13.bn2\n",
            "bottlenecks.13.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[-1.5435e-11]],\n",
            "\n",
            "         [[ 6.4985e-10]],\n",
            "\n",
            "         [[ 1.1249e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.9956e-09]],\n",
            "\n",
            "         [[ 1.9566e-12]],\n",
            "\n",
            "         [[-2.4966e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 9.4444e-12]],\n",
            "\n",
            "         [[-5.9735e-10]],\n",
            "\n",
            "         [[-1.3344e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-7.3513e-08]],\n",
            "\n",
            "         [[-5.8589e-11]],\n",
            "\n",
            "         [[ 6.8268e-11]]],\n",
            "\n",
            "\n",
            "        [[[-2.1792e-12]],\n",
            "\n",
            "         [[-3.9990e-10]],\n",
            "\n",
            "         [[ 4.8330e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.8044e-07]],\n",
            "\n",
            "         [[-6.5015e-12]],\n",
            "\n",
            "         [[ 6.3296e-11]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.2227e-11]],\n",
            "\n",
            "         [[-8.7083e-11]],\n",
            "\n",
            "         [[ 1.0191e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.5378e-08]],\n",
            "\n",
            "         [[-8.7932e-12]],\n",
            "\n",
            "         [[-5.7077e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 2.4157e-11]],\n",
            "\n",
            "         [[-1.0606e-09]],\n",
            "\n",
            "         [[ 7.8313e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-9.5287e-08]],\n",
            "\n",
            "         [[-3.4872e-11]],\n",
            "\n",
            "         [[ 2.9341e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 5.2715e-12]],\n",
            "\n",
            "         [[ 2.4271e-10]],\n",
            "\n",
            "         [[ 5.5708e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.1374e-09]],\n",
            "\n",
            "         [[ 3.0519e-11]],\n",
            "\n",
            "         [[ 1.6288e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.13.bn3\n",
            "bottlenecks.14\n",
            "bottlenecks.14.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[ 8.9062e-12]],\n",
            "\n",
            "         [[-1.0796e-11]],\n",
            "\n",
            "         [[ 4.4112e-13]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.8393e-13]],\n",
            "\n",
            "         [[-5.2267e-12]],\n",
            "\n",
            "         [[ 9.7725e-12]]],\n",
            "\n",
            "\n",
            "        [[[-2.7173e-03]],\n",
            "\n",
            "         [[ 4.8750e-06]],\n",
            "\n",
            "         [[-3.3157e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.1434e-02]],\n",
            "\n",
            "         [[-3.8886e-06]],\n",
            "\n",
            "         [[-5.3461e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 4.2414e-09]],\n",
            "\n",
            "         [[ 1.0272e-09]],\n",
            "\n",
            "         [[-5.6639e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.3726e-09]],\n",
            "\n",
            "         [[ 5.9746e-10]],\n",
            "\n",
            "         [[-1.2916e-09]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.3866e-11]],\n",
            "\n",
            "         [[-1.8811e-11]],\n",
            "\n",
            "         [[ 1.3880e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3449e-11]],\n",
            "\n",
            "         [[-6.9584e-11]],\n",
            "\n",
            "         [[-2.6103e-11]]],\n",
            "\n",
            "\n",
            "        [[[-2.4406e-11]],\n",
            "\n",
            "         [[ 4.2802e-11]],\n",
            "\n",
            "         [[ 8.5346e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.7383e-12]],\n",
            "\n",
            "         [[ 6.0837e-11]],\n",
            "\n",
            "         [[-5.8464e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 3.0221e-09]],\n",
            "\n",
            "         [[-8.9368e-10]],\n",
            "\n",
            "         [[ 4.4551e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.0899e-09]],\n",
            "\n",
            "         [[ 1.3323e-09]],\n",
            "\n",
            "         [[-6.9101e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.14.bn1\n",
            "bottlenecks.14.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[-3.4115e-12,  4.4718e-12, -1.5989e-12],\n",
            "          [-1.2185e-11, -4.2854e-11, -2.1147e-11],\n",
            "          [ 1.6940e-11,  4.3599e-11,  4.2011e-11]]],\n",
            "\n",
            "\n",
            "        [[[-9.6153e-02, -3.0977e-01, -1.9760e-01],\n",
            "          [ 1.1928e-01,  7.1086e-02,  1.4284e-01],\n",
            "          [-5.3059e-02,  2.8099e-01,  1.2034e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.5352e-09, -1.7649e-09, -4.2834e-09],\n",
            "          [-5.6537e-09, -4.9801e-09, -2.1456e-09],\n",
            "          [ 1.2664e-08,  7.9490e-09, -1.0182e-09]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.0918e-10, -1.3647e-10,  3.8869e-10],\n",
            "          [-9.4221e-11, -1.5108e-10,  1.1257e-10],\n",
            "          [-8.7812e-11,  4.3919e-11,  4.3232e-11]]],\n",
            "\n",
            "\n",
            "        [[[-2.5543e-10, -2.5611e-10,  2.1206e-10],\n",
            "          [ 4.0244e-10, -1.2151e-10, -1.9124e-10],\n",
            "          [ 1.7061e-10, -1.6888e-10, -2.1588e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 1.9782e-09,  1.5382e-08, -5.1105e-09],\n",
            "          [-5.1255e-09, -2.9910e-09, -4.2645e-09],\n",
            "          [-2.4108e-09,  6.8209e-09,  3.0967e-09]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.14.bn2\n",
            "bottlenecks.14.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[ 7.2741e-12]],\n",
            "\n",
            "         [[ 3.0671e-04]],\n",
            "\n",
            "         [[-2.7955e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-5.9781e-11]],\n",
            "\n",
            "         [[ 3.5451e-11]],\n",
            "\n",
            "         [[ 1.2062e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5034e-11]],\n",
            "\n",
            "         [[ 7.7048e-07]],\n",
            "\n",
            "         [[-2.5033e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2998e-11]],\n",
            "\n",
            "         [[-3.3796e-11]],\n",
            "\n",
            "         [[-3.2224e-09]]],\n",
            "\n",
            "\n",
            "        [[[-4.9563e-12]],\n",
            "\n",
            "         [[-8.6915e-10]],\n",
            "\n",
            "         [[ 8.4316e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.5414e-11]],\n",
            "\n",
            "         [[ 5.3046e-11]],\n",
            "\n",
            "         [[-5.6107e-10]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-6.2245e-12]],\n",
            "\n",
            "         [[-3.6249e-05]],\n",
            "\n",
            "         [[ 2.5244e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 6.3462e-11]],\n",
            "\n",
            "         [[-7.6879e-11]],\n",
            "\n",
            "         [[-9.0907e-10]]],\n",
            "\n",
            "\n",
            "        [[[-1.7452e-11]],\n",
            "\n",
            "         [[-6.5805e-09]],\n",
            "\n",
            "         [[ 2.4835e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.9506e-11]],\n",
            "\n",
            "         [[ 6.8383e-11]],\n",
            "\n",
            "         [[ 1.1229e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 2.5666e-13]],\n",
            "\n",
            "         [[-1.8654e-01]],\n",
            "\n",
            "         [[ 1.3421e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.9726e-11]],\n",
            "\n",
            "         [[ 8.0619e-13]],\n",
            "\n",
            "         [[ 1.8708e-09]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.14.bn3\n",
            "bottlenecks.15\n",
            "bottlenecks.15.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[ 5.7452e-11]],\n",
            "\n",
            "         [[ 9.0890e-11]],\n",
            "\n",
            "         [[-1.6727e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.2581e-11]],\n",
            "\n",
            "         [[-1.1692e-11]],\n",
            "\n",
            "         [[-3.3949e-11]]],\n",
            "\n",
            "\n",
            "        [[[-1.9145e-09]],\n",
            "\n",
            "         [[-2.4172e-10]],\n",
            "\n",
            "         [[ 1.0447e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.9437e-09]],\n",
            "\n",
            "         [[ 3.8029e-10]],\n",
            "\n",
            "         [[ 3.8913e-09]]],\n",
            "\n",
            "\n",
            "        [[[-7.9458e-06]],\n",
            "\n",
            "         [[-7.7569e-06]],\n",
            "\n",
            "         [[ 3.8832e-08]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-8.9920e-05]],\n",
            "\n",
            "         [[-2.9772e-08]],\n",
            "\n",
            "         [[ 1.0034e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 6.3704e-11]],\n",
            "\n",
            "         [[-6.2396e-10]],\n",
            "\n",
            "         [[ 1.2595e-10]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.8599e-10]],\n",
            "\n",
            "         [[-6.5259e-10]],\n",
            "\n",
            "         [[-3.7469e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 5.2850e-10]],\n",
            "\n",
            "         [[ 1.3347e-09]],\n",
            "\n",
            "         [[ 3.0782e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.5773e-09]],\n",
            "\n",
            "         [[ 4.6271e-10]],\n",
            "\n",
            "         [[-7.2569e-10]]],\n",
            "\n",
            "\n",
            "        [[[-1.1176e-11]],\n",
            "\n",
            "         [[-1.5540e-12]],\n",
            "\n",
            "         [[-1.8193e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.0549e-12]],\n",
            "\n",
            "         [[ 3.1457e-11]],\n",
            "\n",
            "         [[ 2.7859e-11]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.15.bn1\n",
            "bottlenecks.15.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[-3.3638e-11, -4.5525e-12, -2.6060e-12],\n",
            "          [-2.0581e-11, -1.7372e-11, -3.4612e-11],\n",
            "          [ 1.5551e-10,  3.0196e-11,  1.6350e-10]]],\n",
            "\n",
            "\n",
            "        [[[-4.2010e-10,  2.3207e-09,  7.8703e-09],\n",
            "          [-6.4146e-10,  1.9695e-09,  6.6567e-09],\n",
            "          [-3.5883e-09, -1.5746e-09,  4.8798e-09]]],\n",
            "\n",
            "\n",
            "        [[[-5.2505e-02, -5.2400e-02, -7.4094e-02],\n",
            "          [-1.1239e-01, -1.1552e-01, -8.1611e-02],\n",
            "          [-1.1773e-01, -8.9051e-02, -1.6370e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.5492e-09, -5.1393e-10, -1.6345e-09],\n",
            "          [-1.5901e-09, -1.1389e-09, -3.0104e-10],\n",
            "          [-5.9335e-10, -1.8206e-10, -5.9324e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3008e-09,  6.3513e-10, -1.8318e-09],\n",
            "          [ 1.6053e-09, -1.1927e-09, -5.0670e-10],\n",
            "          [ 4.4817e-09,  1.5030e-09, -3.9272e-10]]],\n",
            "\n",
            "\n",
            "        [[[ 7.0088e-11,  4.7645e-11,  4.8512e-11],\n",
            "          [ 8.5289e-11, -1.7823e-11,  8.9170e-11],\n",
            "          [ 1.7631e-11, -6.1434e-11, -1.4424e-11]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.15.bn2\n",
            "bottlenecks.15.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[-5.6810e-11]],\n",
            "\n",
            "         [[ 1.9290e-10]],\n",
            "\n",
            "         [[-2.6925e-04]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.2247e-10]],\n",
            "\n",
            "         [[-2.6461e-10]],\n",
            "\n",
            "         [[-6.5486e-11]]],\n",
            "\n",
            "\n",
            "        [[[-4.9703e-12]],\n",
            "\n",
            "         [[-5.1389e-10]],\n",
            "\n",
            "         [[ 9.8683e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.7750e-10]],\n",
            "\n",
            "         [[ 4.5139e-12]],\n",
            "\n",
            "         [[ 9.0237e-12]]],\n",
            "\n",
            "\n",
            "        [[[ 4.5056e-11]],\n",
            "\n",
            "         [[ 6.0564e-10]],\n",
            "\n",
            "         [[-4.0810e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-4.6524e-10]],\n",
            "\n",
            "         [[ 4.3079e-10]],\n",
            "\n",
            "         [[ 1.5007e-11]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.0246e-11]],\n",
            "\n",
            "         [[ 4.2241e-09]],\n",
            "\n",
            "         [[-1.8901e-05]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.1054e-10]],\n",
            "\n",
            "         [[-1.4678e-09]],\n",
            "\n",
            "         [[ 1.9895e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.8158e-12]],\n",
            "\n",
            "         [[-1.1554e-09]],\n",
            "\n",
            "         [[ 7.9534e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.5255e-11]],\n",
            "\n",
            "         [[-2.1893e-10]],\n",
            "\n",
            "         [[ 4.0256e-12]]],\n",
            "\n",
            "\n",
            "        [[[-3.7306e-11]],\n",
            "\n",
            "         [[ 9.9531e-10]],\n",
            "\n",
            "         [[ 4.0194e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-6.6323e-10]],\n",
            "\n",
            "         [[ 7.7396e-10]],\n",
            "\n",
            "         [[ 5.3159e-13]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.15.bn3\n",
            "bottlenecks.16\n",
            "bottlenecks.16.conv1\n",
            "Parameter containing:\n",
            "tensor([[[[ 9.4484e-08]],\n",
            "\n",
            "         [[ 3.3801e-07]],\n",
            "\n",
            "         [[-5.7737e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.0803e-06]],\n",
            "\n",
            "         [[-1.4325e-07]],\n",
            "\n",
            "         [[-6.4389e-08]]],\n",
            "\n",
            "\n",
            "        [[[ 2.7510e-11]],\n",
            "\n",
            "         [[-1.2772e-12]],\n",
            "\n",
            "         [[-1.8597e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.5871e-11]],\n",
            "\n",
            "         [[-7.8181e-12]],\n",
            "\n",
            "         [[ 1.3470e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.6467e-13]],\n",
            "\n",
            "         [[ 2.9510e-11]],\n",
            "\n",
            "         [[-5.4060e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.2302e-11]],\n",
            "\n",
            "         [[-2.5180e-11]],\n",
            "\n",
            "         [[-5.7199e-11]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 5.4541e-11]],\n",
            "\n",
            "         [[ 3.9040e-11]],\n",
            "\n",
            "         [[-1.2465e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.5549e-12]],\n",
            "\n",
            "         [[ 4.4647e-11]],\n",
            "\n",
            "         [[ 6.4252e-11]]],\n",
            "\n",
            "\n",
            "        [[[-6.2702e-11]],\n",
            "\n",
            "         [[ 9.3499e-12]],\n",
            "\n",
            "         [[-1.8429e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.2494e-11]],\n",
            "\n",
            "         [[ 2.4161e-11]],\n",
            "\n",
            "         [[-1.4091e-11]]],\n",
            "\n",
            "\n",
            "        [[[ 1.5204e-11]],\n",
            "\n",
            "         [[ 2.3079e-11]],\n",
            "\n",
            "         [[ 2.1883e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 5.4178e-12]],\n",
            "\n",
            "         [[ 3.5391e-11]],\n",
            "\n",
            "         [[ 6.6894e-12]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.16.bn1\n",
            "bottlenecks.16.conv2\n",
            "Parameter containing:\n",
            "tensor([[[[ 5.9956e-07,  1.0395e-06,  8.5624e-07],\n",
            "          [ 6.0362e-07,  1.4951e-07,  4.7149e-07],\n",
            "          [ 9.7957e-07,  9.4500e-07,  6.5879e-07]]],\n",
            "\n",
            "\n",
            "        [[[ 2.1676e-11,  2.4338e-11,  2.2785e-11],\n",
            "          [ 1.9063e-11,  3.6706e-11,  4.4362e-11],\n",
            "          [-1.6623e-11,  4.1295e-12,  1.4186e-11]]],\n",
            "\n",
            "\n",
            "        [[[-4.0228e-11, -4.9082e-11, -5.1024e-11],\n",
            "          [-4.2612e-11, -5.0125e-11, -2.6356e-11],\n",
            "          [-2.5477e-11, -6.7591e-11, -3.4926e-11]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 1.3594e-10,  8.2350e-11,  2.0605e-10],\n",
            "          [-9.6380e-12, -7.0645e-11,  6.7306e-11],\n",
            "          [-6.9496e-11, -1.1204e-10, -1.0701e-10]]],\n",
            "\n",
            "\n",
            "        [[[-4.1638e-11, -2.1751e-11, -3.5779e-11],\n",
            "          [-5.5601e-12, -3.0747e-11, -1.4248e-11],\n",
            "          [ 8.7610e-11,  3.9578e-11,  8.4576e-11]]],\n",
            "\n",
            "\n",
            "        [[[-2.8807e-11, -1.6166e-11,  5.5255e-11],\n",
            "          [-8.2656e-12, -1.2979e-12,  6.7335e-11],\n",
            "          [ 1.0340e-11,  5.6108e-11, -2.4229e-11]]]], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "bottlenecks.16.bn2\n",
            "bottlenecks.16.conv3\n",
            "Parameter containing:\n",
            "tensor([[[[-5.5512e-07]],\n",
            "\n",
            "         [[-1.3791e-11]],\n",
            "\n",
            "         [[-4.8839e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.1896e-11]],\n",
            "\n",
            "         [[ 1.2490e-11]],\n",
            "\n",
            "         [[-9.8357e-12]]],\n",
            "\n",
            "\n",
            "        [[[ 4.2300e-09]],\n",
            "\n",
            "         [[ 3.0921e-12]],\n",
            "\n",
            "         [[-1.2154e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.3258e-11]],\n",
            "\n",
            "         [[ 3.6768e-12]],\n",
            "\n",
            "         [[-1.1178e-11]]],\n",
            "\n",
            "\n",
            "        [[[-4.3687e-07]],\n",
            "\n",
            "         [[ 1.4735e-11]],\n",
            "\n",
            "         [[-1.5368e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 4.6757e-11]],\n",
            "\n",
            "         [[-6.4835e-12]],\n",
            "\n",
            "         [[-7.5897e-13]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.0375e-07]],\n",
            "\n",
            "         [[ 1.1983e-11]],\n",
            "\n",
            "         [[-1.0053e-11]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4801e-11]],\n",
            "\n",
            "         [[-3.2336e-12]],\n",
            "\n",
            "         [[ 1.2502e-11]]],\n",
            "\n",
            "\n",
            "        [[[-4.3426e-09]],\n",
            "\n",
            "         [[-3.6022e-12]],\n",
            "\n",
            "         [[ 1.6822e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.8782e-11]],\n",
            "\n",
            "         [[ 6.8870e-12]],\n",
            "\n",
            "         [[ 4.5572e-13]]],\n",
            "\n",
            "\n",
            "        [[[ 9.4635e-09]],\n",
            "\n",
            "         [[-6.6313e-12]],\n",
            "\n",
            "         [[ 8.7744e-12]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 8.4937e-12]],\n",
            "\n",
            "         [[ 1.8603e-12]],\n",
            "\n",
            "         [[-1.7425e-12]]]], device='cuda:0', requires_grad=True)\n",
            "bottlenecks.16.bn3\n",
            "conv1\n",
            "Parameter containing:\n",
            "tensor([[[[ 1.0711e-09]],\n",
            "\n",
            "         [[-2.1269e-09]],\n",
            "\n",
            "         [[-1.2475e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.3363e-09]],\n",
            "\n",
            "         [[ 1.0612e-09]],\n",
            "\n",
            "         [[-2.1772e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 3.5849e-03]],\n",
            "\n",
            "         [[ 1.0985e-08]],\n",
            "\n",
            "         [[-5.1743e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.7937e-07]],\n",
            "\n",
            "         [[ 1.9946e-09]],\n",
            "\n",
            "         [[ 2.1377e-09]]],\n",
            "\n",
            "\n",
            "        [[[ 1.0818e-03]],\n",
            "\n",
            "         [[-5.5062e-08]],\n",
            "\n",
            "         [[-1.5353e-06]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7495e-07]],\n",
            "\n",
            "         [[ 6.8028e-09]],\n",
            "\n",
            "         [[-9.7311e-10]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-2.6085e-09]],\n",
            "\n",
            "         [[ 5.5563e-12]],\n",
            "\n",
            "         [[-2.4266e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.3928e-10]],\n",
            "\n",
            "         [[ 1.2015e-09]],\n",
            "\n",
            "         [[-5.3677e-11]]],\n",
            "\n",
            "\n",
            "        [[[-3.0994e-03]],\n",
            "\n",
            "         [[-1.9369e-08]],\n",
            "\n",
            "         [[-3.6530e-07]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 9.8584e-08]],\n",
            "\n",
            "         [[ 5.8130e-10]],\n",
            "\n",
            "         [[-1.3525e-09]]],\n",
            "\n",
            "\n",
            "        [[[-5.0684e-09]],\n",
            "\n",
            "         [[-2.4329e-09]],\n",
            "\n",
            "         [[ 1.1517e-09]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.4592e-11]],\n",
            "\n",
            "         [[ 3.7398e-10]],\n",
            "\n",
            "         [[ 3.0847e-09]]]], device='cuda:0', requires_grad=True)\n",
            "bn1\n",
            "fc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPEjDyn7CySR"
      },
      "source": [
        "Bien que les résultats du Binary COnnect appliqué à mobile net semble intéressants, après étude des poids nous avons vu qu'ils n'étaient pas binarisés..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qzgFXYXLJY5"
      },
      "source": [
        "## half sur nos réseaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO3n147fDffU"
      },
      "source": [
        "On applique ici une quantization 32 bits aux réseaux qui présente de l'intérêt à notre avis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXYmQlhEL6aQ"
      },
      "source": [
        "### 45k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1WqJQyxL80v",
        "outputId": "e3d3145c-8d6b-4ee8-8f0f-152600f60481"
      },
      "source": [
        "data_int=False\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('mnet_cifar10_kd_45k.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "\n",
        "mymodel.remove_prune()\n",
        "mesure_operation(mymodel.model,\"resnet\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "tensor([0.0833])\n",
            "Conv2d: S_c=3, F_in=3, F_out=32, P=1024, params=36, operations=107861\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.0156])\n",
            "Conv2d: S_c=1, F_in=32, F_out=32, P=1024, params=8, operations=24064\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.1250])\n",
            "Conv2d: S_c=3, F_in=32, F_out=32, P=1024, params=18, operations=1765376\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.0156])\n",
            "Conv2d: S_c=1, F_in=32, F_out=16, P=1024, params=4, operations=12032\n",
            "Batch norm: F_in=16 P=1024, params=16, operations=49152\n",
            "tensor([0.0156])\n",
            "Conv2d: S_c=1, F_in=16, F_out=96, P=1024, params=12, operations=35328\n",
            "Batch norm: F_in=96 P=1024, params=96, operations=294912\n",
            "tensor([0.1250])\n",
            "Conv2d: S_c=3, F_in=96, F_out=96, P=1024, params=54, operations=15912960\n",
            "Batch norm: F_in=96 P=1024, params=96, operations=294912\n",
            "tensor([0.0156])\n",
            "Conv2d: S_c=1, F_in=96, F_out=24, P=1024, params=18, operations=54912\n",
            "Batch norm: F_in=24 P=1024, params=24, operations=73728\n",
            "tensor([0.0162])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=28, operations=83626\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.1312])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=85, operations=37581940\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.0162])\n",
            "Conv2d: S_c=1, F_in=144, F_out=24, P=1024, params=28, operations=85617\n",
            "Batch norm: F_in=24 P=1024, params=24, operations=73728\n",
            "tensor([0.0162])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=28, operations=83626\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.1312])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=85, operations=37581940\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.0165])\n",
            "Conv2d: S_c=1, F_in=144, F_out=32, P=1024, params=38, operations=116195\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.0169])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=52, operations=156416\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.1354])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=117, operations=68982784\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.0169])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=52, operations=159189\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.0169])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=52, operations=156416\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.1354])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=117, operations=68982784\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.0169])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=52, operations=159189\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.0169])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=52, operations=156416\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.1354])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=117, operations=17245696\n",
            "Batch norm: F_in=192 P=256, params=192, operations=147456\n",
            "tensor([0.0190])\n",
            "Conv2d: S_c=1, F_in=192, F_out=64, P=256, params=117, operations=89544\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.0183])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=225, operations=171000\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.1302])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=225, operations=66342400\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.0183])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=225, operations=172500\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.0183])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=225, operations=171000\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.1302])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=225, operations=66342400\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.0183])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=225, operations=172500\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.0183])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=225, operations=171000\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.1302])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=225, operations=66342400\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.0183])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=225, operations=172500\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.0183])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=225, operations=171000\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.1302])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=225, operations=66342400\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.0163])\n",
            "Conv2d: S_c=1, F_in=384, F_out=96, P=256, params=300, operations=230000\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([0.0165])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=456, operations=347776\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.1319])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=342, operations=151270400\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.0165])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=456, operations=349802\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([0.0165])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=456, operations=347776\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.1319])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=342, operations=151270400\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.0165])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=456, operations=349802\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([0.0165])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=456, operations=347776\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.1319])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=342, operations=37817600\n",
            "Batch norm: F_in=576 P=64, params=576, operations=110592\n",
            "tensor([0.0181])\n",
            "Conv2d: S_c=1, F_in=576, F_out=160, P=64, params=836, operations=160326\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([0.0180])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=1386, operations=265003\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.1312])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=567, operations=104501368\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.0180])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=1386, operations=265927\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([0.0180])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=1386, operations=265003\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.1312])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=567, operations=104501368\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.0180])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=1386, operations=265927\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([0.0180])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=1386, operations=265003\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.1312])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=567, operations=104501368\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.0172])\n",
            "Conv2d: S_c=1, F_in=960, F_out=320, P=64, params=2646, operations=507679\n",
            "Batch norm: F_in=320 P=64, params=320, operations=61440\n",
            "tensor([0.0173])\n",
            "Conv2d: S_c=1, F_in=320, F_out=1280, P=64, params=3549, operations=679988\n",
            "Batch norm: F_in=1280 P=64, params=1280, operations=245760\n",
            "1280\n",
            "Linear: F_in=1280, F_out=10, params=1290, operations=1932\n",
            "Flops: 1187464320.0, Params: 41293.5\n",
            "Score flops: 1.423198884399076 Score Params: 0.007391022092253402\n",
            "Final score: 1.4305899064913294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiJlzV9tLRtd",
        "outputId": "f40f45cf-e076-4921-a778-bd9a11d7aaf0"
      },
      "source": [
        "data_int=True\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('mnet_cifar10_kd_45k.pt',map_location=torch.device('cpu')))\n",
        "if data_int :\n",
        "  mymodel.model.half()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "valid_loss,training_loss,test_accuracy=[],[],[]\n",
        "\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 1,criterion,optimizer,mymodel,scheduler=False ) \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "epoch  0\n",
            "saving weights.... \n",
            "67.58  % ,  0.951640625  ,  0.99979091796875\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohX6DosDMr70",
        "outputId": "9eb078f2-5515-4748-df2d-bc56592c11ca"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 20,criterion,optimizer,mymodel,scheduler=False ) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "saving weights.... \n",
            "74.58  % ,  0.749175390625  ,  0.7881087890625\n",
            "epoch  1\n",
            "saving weights.... \n",
            "74.66  % ,  0.74453984375  ,  0.732587109375\n",
            "epoch  2\n",
            "73.32  % ,  0.7542609375  ,  0.69352216796875\n",
            "epoch  3\n",
            "saving weights.... \n",
            "76.67  % ,  0.67958984375  ,  0.686780078125\n",
            "epoch  4\n",
            "saving weights.... \n",
            "77.9  % ,  0.66290703125  ,  0.6719482421875\n",
            "epoch  5\n",
            "saving weights.... \n",
            "78.6  % ,  0.6343125  ,  0.6599123046875\n",
            "epoch  6\n",
            "saving weights.... \n",
            "79.24  % ,  0.629033984375  ,  0.64786826171875\n",
            "epoch  7\n",
            "72.94  % ,  0.8068328125  ,  0.644325390625\n",
            "epoch  8\n",
            "75.88  % ,  0.703375  ,  0.63085458984375\n",
            "epoch  9\n",
            "77.33  % ,  0.65828671875  ,  0.6273140625\n",
            "epoch  10\n",
            "76.21  % ,  0.69695859375  ,  0.616440625\n",
            "epoch  11\n",
            "78.62  % ,  0.651365625  ,  0.61372666015625\n",
            "epoch  12\n",
            "75.78  % ,  0.70234609375  ,  0.61296455078125\n",
            "epoch  13\n",
            "77.58  % ,  0.663975  ,  0.5972353515625\n",
            "epoch  14\n",
            "77.06  % ,  0.673258203125  ,  0.60660205078125\n",
            "epoch  15\n",
            "saving weights.... \n",
            "79.1  % ,  0.62759453125  ,  0.5942736328125\n",
            "epoch  16\n",
            "saving weights.... \n",
            "78.77  % ,  0.621643359375  ,  0.6006318359375\n",
            "epoch  17\n",
            "75.0  % ,  0.738749609375  ,  0.59690556640625\n",
            "epoch  18\n",
            "saving weights.... \n",
            "79.12  % ,  0.600945703125  ,  0.5936736328125\n",
            "epoch  19\n",
            "74.83  % ,  0.7369734375  ,  0.58940224609375\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if5SpDYDMotl",
        "outputId": "c742b112-31cd-4c1b-96a5-4798630394dc"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,10,criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy,scheduler=False ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "saving weights.... \n",
            "84.77  % ,  0.453483203125  ,  0.47350751953125\n",
            "epoch  1\n",
            "saving weights.... \n",
            "84.95  % ,  0.434588671875  ,  0.430129150390625\n",
            "epoch  2\n",
            "saving weights.... \n",
            "85.2  % ,  0.43351796875  ,  0.417379052734375\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.63  % ,  0.4293400390625  ,  0.410043798828125\n",
            "epoch  4\n",
            "saving weights.... \n",
            "85.62  % ,  0.4196998046875  ,  0.401387548828125\n",
            "epoch  5\n",
            "86.25  % ,  0.4243568359375  ,  0.39577939453125\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.85  % ,  0.4167611328125  ,  0.391538916015625\n",
            "epoch  7\n",
            "saving weights.... \n",
            "85.94  % ,  0.414596875  ,  0.388394189453125\n",
            "epoch  8\n",
            "86.66  % ,  0.41995703125  ,  0.382406884765625\n",
            "epoch  9\n",
            "saving weights.... \n",
            "86.31  % ,  0.411021875  ,  0.38247509765625\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF2jRoQEL2S3"
      },
      "source": [
        "### 68k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y6xtPoJLTvQ",
        "outputId": "6324f9e5-7b8c-4631-b2b5-587fcbe0e5fb"
      },
      "source": [
        "data_int=False\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('checkpointmnet_cifar10_kd_68k.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "\n",
        "mymodel.remove_prune()\n",
        "mesure_operation(mymodel.model,\"resnet\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "tensor([0.1042])\n",
            "Conv2d: S_c=3, F_in=3, F_out=32, P=1024, params=45, operations=134826\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.0234])\n",
            "Conv2d: S_c=1, F_in=32, F_out=32, P=1024, params=12, operations=36096\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.1528])\n",
            "Conv2d: S_c=3, F_in=32, F_out=32, P=1024, params=22, operations=2157681\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.0195])\n",
            "Conv2d: S_c=1, F_in=32, F_out=16, P=1024, params=5, operations=15040\n",
            "Batch norm: F_in=16 P=1024, params=16, operations=49152\n",
            "tensor([0.0195])\n",
            "Conv2d: S_c=1, F_in=16, F_out=96, P=1024, params=15, operations=44160\n",
            "Batch norm: F_in=96 P=1024, params=96, operations=294912\n",
            "tensor([0.1551])\n",
            "Conv2d: S_c=3, F_in=96, F_out=96, P=1024, params=67, operations=19743858\n",
            "Batch norm: F_in=96 P=1024, params=96, operations=294912\n",
            "tensor([0.0260])\n",
            "Conv2d: S_c=1, F_in=96, F_out=24, P=1024, params=30, operations=91520\n",
            "Batch norm: F_in=24 P=1024, params=24, operations=73728\n",
            "tensor([0.0278])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=48, operations=143360\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.1667])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=108, operations=47751168\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.0278])\n",
            "Conv2d: S_c=1, F_in=144, F_out=24, P=1024, params=48, operations=146773\n",
            "Batch norm: F_in=24 P=1024, params=24, operations=73728\n",
            "tensor([0.0278])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=48, operations=143360\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.1667])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=108, operations=47751168\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.0260])\n",
            "Conv2d: S_c=1, F_in=144, F_out=32, P=1024, params=60, operations=183466\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.0260])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=80, operations=240640\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.1667])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=144, operations=84901888\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.0260])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=80, operations=244906\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.0260])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=80, operations=240640\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.1667])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=144, operations=84901888\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.0260])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=80, operations=244906\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.0260])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=80, operations=240640\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.1667])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=144, operations=21225472\n",
            "Batch norm: F_in=192 P=256, params=192, operations=147456\n",
            "tensor([0.0286])\n",
            "Conv2d: S_c=1, F_in=192, F_out=64, P=256, params=176, operations=134698\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.0282])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=346, operations=262960\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.1638])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=283, operations=83444000\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.0282])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=346, operations=265266\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.0282])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=346, operations=262960\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.1638])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=283, operations=83444000\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.0282])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=346, operations=265266\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.0282])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=346, operations=262960\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.1638])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=283, operations=83444000\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.0282])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=346, operations=265266\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.0282])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=346, operations=262960\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.1638])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=283, operations=83444000\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.0256])\n",
            "Conv2d: S_c=1, F_in=384, F_out=96, P=256, params=472, operations=361866\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([0.0258])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=712, operations=543018\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.1647])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=427, operations=188866848\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.0258])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=712, operations=546183\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([0.0258])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=712, operations=543018\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.1647])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=427, operations=188866848\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.0258])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=712, operations=546183\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([0.0258])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=712, operations=543018\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.1647])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=427, operations=47216712\n",
            "Batch norm: F_in=576 P=64, params=576, operations=110592\n",
            "tensor([0.0278])\n",
            "Conv2d: S_c=1, F_in=576, F_out=160, P=64, params=1282, operations=245859\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([0.0278])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=2133, operations=407829\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.1646])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=711, operations=131041408\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.0278])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=2133, operations=409251\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([0.0278])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=2133, operations=407829\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.1646])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=711, operations=131041408\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.0278])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=2133, operations=409251\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([0.0278])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=2133, operations=407829\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.1646])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=711, operations=131041408\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.0273])\n",
            "Conv2d: S_c=1, F_in=960, F_out=320, P=64, params=4187, operations=803345\n",
            "Batch norm: F_in=320 P=64, params=320, operations=61440\n",
            "tensor([0.0273])\n",
            "Conv2d: S_c=1, F_in=320, F_out=1280, P=64, params=5591, operations=1071235\n",
            "Batch norm: F_in=1280 P=64, params=1280, operations=245760\n",
            "1280\n",
            "Linear: F_in=1280, F_out=10, params=1290, operations=1932\n",
            "Flops: 1484511104.0, Params: 52739.5\n",
            "Score flops: 1.7792151827272085 Score Params: 0.009439713505379739\n",
            "Final score: 1.788654896232588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py2_ujOZP6pp",
        "outputId": "6ef62732-99c1-4aee-88c7-34635c815a5f"
      },
      "source": [
        "data_int=False\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('checkpointmnet_cifar10_kd_68k.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(eval_accuracy(mymodel.model,c10testloader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "89.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAfwSXwAnw3Z",
        "outputId": "c86b391b-082a-4388-b099-4655918909ed"
      },
      "source": [
        "data_int=True\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('mnet_cifar10_kd_68k.pt',map_location=torch.device('cpu')))\n",
        "if data_int :\n",
        "  mymodel.model.half()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "valid_loss,training_loss,test_accuracy=[],[],[]\n",
        "\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 1,criterion,optimizer,mymodel,scheduler=False ) \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "epoch  0\n",
            "68.14  % ,  0.929415625  ,  0.9984505859375\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfLa0vWcnw3k",
        "outputId": "a40bab75-659d-404b-db9f-dee29bca59d9"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 20,criterion,optimizer,mymodel,scheduler=False ) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "71.64  % ,  0.8596  ,  0.789081640625\n",
            "epoch  1\n",
            "75.14  % ,  0.722784375  ,  0.7319400390625\n",
            "epoch  2\n",
            "75.61  % ,  0.724984375  ,  0.7066099609375\n",
            "epoch  3\n",
            "74.76  % ,  0.73442890625  ,  0.67573310546875\n",
            "epoch  4\n",
            "74.81  % ,  0.72003515625  ,  0.66013564453125\n",
            "epoch  5\n",
            "75.06  % ,  0.71742578125  ,  0.6479560546875\n",
            "epoch  6\n",
            "76.71  % ,  0.715137109375  ,  0.63591474609375\n",
            "epoch  7\n",
            "76.04  % ,  0.68206796875  ,  0.63283828125\n",
            "epoch  8\n",
            "78.48  % ,  0.654242578125  ,  0.625721875\n",
            "epoch  9\n",
            "76.83  % ,  0.68983125  ,  0.62082421875\n",
            "epoch  10\n",
            "77.17  % ,  0.641733203125  ,  0.61616171875\n",
            "epoch  11\n",
            "77.41  % ,  0.6840390625  ,  0.6149458984375\n",
            "epoch  12\n",
            "77.45  % ,  0.6428265625  ,  0.6082552734375\n",
            "epoch  13\n",
            "77.95  % ,  0.63481171875  ,  0.60747138671875\n",
            "epoch  14\n",
            "79.69  % ,  0.589803515625  ,  0.60443486328125\n",
            "epoch  15\n",
            "76.78  % ,  0.67206328125  ,  0.5999828125\n",
            "epoch  16\n",
            "80.35  % ,  0.59741640625  ,  0.6020232421875\n",
            "epoch  17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-bS2eT5uT6A",
        "outputId": "b83ecff0-493f-4d03-d21c-59003dfff03c"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 8,criterion,optimizer,mymodel,scheduler=False ) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "78.04  % ,  0.656537890625  ,  0.5959181640625\n",
            "epoch  1\n",
            "79.09  % ,  0.622998828125  ,  0.59561494140625\n",
            "epoch  2\n",
            "79.68  % ,  0.610316015625  ,  0.59540224609375\n",
            "epoch  3\n",
            "79.56  % ,  0.6109103515625  ,  0.597985546875\n",
            "epoch  4\n",
            "80.47  % ,  0.591781640625  ,  0.596171484375\n",
            "epoch  5\n",
            "79.19  % ,  0.61143515625  ,  0.58944013671875\n",
            "epoch  6\n",
            "78.54  % ,  0.622975390625  ,  0.5920076171875\n",
            "epoch  7\n",
            "78.75  % ,  0.634009375  ,  0.59277392578125\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlC-jHQ5nw3k",
        "outputId": "bc760b92-1f02-4d4b-cd17-a33b3e0410d6"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,10,criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy,scheduler=False ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "84.36  % ,  0.465015625  ,  0.4789638671875\n",
            "epoch  1\n",
            "84.8  % ,  0.4531296875  ,  0.432490234375\n",
            "epoch  2\n",
            "85.3  % ,  0.439956640625  ,  0.420766796875\n",
            "epoch  3\n",
            "85.4  % ,  0.4329140625  ,  0.4096037109375\n",
            "epoch  4\n",
            "85.3  % ,  0.42796171875  ,  0.404459619140625\n",
            "epoch  5\n",
            "85.54  % ,  0.433471875  ,  0.39996181640625\n",
            "epoch  6\n",
            "85.54  % ,  0.419629296875  ,  0.3944962890625\n",
            "epoch  7\n",
            "85.58  % ,  0.421043359375  ,  0.385657080078125\n",
            "epoch  8\n",
            "85.82  % ,  0.4197306640625  ,  0.38346376953125\n",
            "epoch  9\n",
            "85.47  % ,  0.4260857421875  ,  0.38232138671875\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjOWvVpFLR02",
        "outputId": "50631eda-0af5-4229-b1f3-bcebe819ffc2"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,10,criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy,scheduler=False ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "85.94  % ,  0.4172732421875  ,  0.380199951171875\n",
            "epoch  1\n",
            "85.47  % ,  0.4169791015625  ,  0.376739892578125\n",
            "epoch  2\n",
            "86.11  % ,  0.420416796875  ,  0.373740869140625\n",
            "epoch  3\n",
            "saving weights.... \n",
            "85.7  % ,  0.410644921875  ,  0.37342724609375\n",
            "epoch  4\n",
            "85.99  % ,  0.4152974609375  ,  0.370627880859375\n",
            "epoch  5\n",
            "saving weights.... \n",
            "85.74  % ,  0.410621875  ,  0.368991943359375\n",
            "epoch  6\n",
            "saving weights.... \n",
            "85.96  % ,  0.4055537109375  ,  0.369993798828125\n",
            "epoch  7\n",
            "86.02  % ,  0.41183515625  ,  0.3633412109375\n",
            "epoch  8\n",
            "86.01  % ,  0.4155869140625  ,  0.365323486328125\n",
            "epoch  9\n",
            "85.87  % ,  0.4148583984375  ,  0.36306025390625\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vduMpcmO22MT",
        "outputId": "b1ff1999-39d1-4fc9-ea78-6ed7044aac09"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.0001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,10,criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy,scheduler=False ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "86.25  % ,  0.409061328125  ,  0.355794189453125\n",
            "epoch  1\n",
            "85.9  % ,  0.4117482421875  ,  0.352028515625\n",
            "epoch  2\n",
            "86.22  % ,  0.407715234375  ,  0.352715576171875\n",
            "epoch  3\n",
            "saving weights.... \n",
            "86.22  % ,  0.4053765625  ,  0.3458421875\n",
            "epoch  4\n",
            "saving weights.... \n",
            "86.49  % ,  0.4047259765625  ,  0.3476559814453125\n",
            "epoch  5\n",
            "saving weights.... \n",
            "86.29  % ,  0.3983556640625  ,  0.343079150390625\n",
            "epoch  6\n",
            "86.3  % ,  0.401123046875  ,  0.346611083984375\n",
            "epoch  7\n",
            "86.3  % ,  0.405167578125  ,  0.346972705078125\n",
            "epoch  8\n",
            "86.37  % ,  0.399871875  ,  0.33999609375\n",
            "epoch  9\n",
            "86.43  % ,  0.405915625  ,  0.344748046875\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypf0e5uv22Pq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GTicuQD22bV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4ZilD5YLuZv"
      },
      "source": [
        "### 250k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApgXbq7lLR4X",
        "outputId": "bfcb3ec6-948c-43e6-e184-cf740ccba38c"
      },
      "source": [
        "data_int=False\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('mnet_cifar10_250k.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "\n",
        "mymodel.remove_prune()\n",
        "mesure_operation(mymodel.model,\"resnet\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "tensor([0.2083])\n",
            "Conv2d: S_c=3, F_in=3, F_out=32, P=1024, params=90, operations=269653\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.0977])\n",
            "Conv2d: S_c=1, F_in=32, F_out=32, P=1024, params=50, operations=150400\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.3125])\n",
            "Conv2d: S_c=3, F_in=32, F_out=32, P=1024, params=45, operations=4413440\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.0977])\n",
            "Conv2d: S_c=1, F_in=32, F_out=16, P=1024, params=25, operations=75200\n",
            "Batch norm: F_in=16 P=1024, params=16, operations=49152\n",
            "tensor([0.0977])\n",
            "Conv2d: S_c=1, F_in=16, F_out=96, P=1024, params=75, operations=220800\n",
            "Batch norm: F_in=96 P=1024, params=96, operations=294912\n",
            "tensor([0.3125])\n",
            "Conv2d: S_c=3, F_in=96, F_out=96, P=1024, params=135, operations=39782400\n",
            "Batch norm: F_in=96 P=1024, params=96, operations=294912\n",
            "tensor([0.1042])\n",
            "Conv2d: S_c=1, F_in=96, F_out=24, P=1024, params=120, operations=366080\n",
            "Batch norm: F_in=24 P=1024, params=24, operations=73728\n",
            "tensor([0.1065])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=184, operations=549546\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.3194])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=207, operations=91523072\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.1065])\n",
            "Conv2d: S_c=1, F_in=144, F_out=24, P=1024, params=184, operations=562631\n",
            "Batch norm: F_in=24 P=1024, params=24, operations=73728\n",
            "tensor([0.1065])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=184, operations=549546\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.3194])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=207, operations=91523072\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.0998])\n",
            "Conv2d: S_c=1, F_in=144, F_out=32, P=1024, params=230, operations=703288\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.1009])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=310, operations=932480\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=279, operations=164497408\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.1009])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=310, operations=949013\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.1009])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=310, operations=932480\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=279, operations=164497408\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.1009])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=310, operations=949013\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.1009])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=310, operations=932480\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=279, operations=41124352\n",
            "Batch norm: F_in=192 P=256, params=192, operations=147456\n",
            "tensor([0.1060])\n",
            "Conv2d: S_c=1, F_in=192, F_out=64, P=256, params=651, operations=498232\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.1060])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=1302, operations=989520\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=558, operations=164529152\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.1060])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=1302, operations=998200\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.1060])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=1302, operations=989520\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=558, operations=164529152\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.1060])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=1302, operations=998200\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.1060])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=1302, operations=989520\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=558, operations=164529152\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.1060])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=1302, operations=998200\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.1060])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=1302, operations=989520\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=558, operations=164529152\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.1009])\n",
            "Conv2d: S_c=1, F_in=384, F_out=96, P=256, params=1860, operations=1426000\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([0.1009])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=2790, operations=2127840\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=837, operations=370214400\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.1009])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=2790, operations=2140240\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([0.1009])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=2790, operations=2127840\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=837, operations=370214400\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.1009])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=2790, operations=2140240\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([0.1009])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=2790, operations=2127840\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=837, operations=92553600\n",
            "Batch norm: F_in=576 P=64, params=576, operations=110592\n",
            "tensor([0.1049])\n",
            "Conv2d: S_c=1, F_in=576, F_out=160, P=64, params=4836, operations=927437\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([0.1049])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=8060, operations=1541072\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=1395, operations=257106560\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.1049])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=8060, operations=1546445\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([0.1049])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=8060, operations=1541072\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=1395, operations=257106560\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.1049])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=8060, operations=1546445\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([0.1049])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=8060, operations=1541072\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.3229])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=1395, operations=257106560\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.1049])\n",
            "Conv2d: S_c=1, F_in=960, F_out=320, P=64, params=16120, operations=3092890\n",
            "Batch norm: F_in=320 P=64, params=320, operations=61440\n",
            "tensor([0.1049])\n",
            "Conv2d: S_c=1, F_in=320, F_out=1280, P=64, params=21476, operations=4114801\n",
            "Batch norm: F_in=1280 P=64, params=1280, operations=245760\n",
            "1280\n",
            "Linear: F_in=1280, F_out=10, params=1290, operations=1932\n",
            "Flops: 2916164352.0, Params: 139736.0\n",
            "Score flops: 3.4950792058246885 Score Params: 0.025011003259184164\n",
            "Final score: 3.5200902090838726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5dNLf30SrMT",
        "outputId": "b40d81eb-b8f2-4ce8-b860-9f51bae19ce6"
      },
      "source": [
        "data_int=False\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('mnet_cifar10_250k.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "print(eval_accuracy(mymodel.model,c10testloader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "91.14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urfsMjVqvYiu",
        "outputId": "798ece5a-d82d-4724-9a8e-a6fa57aeadc6"
      },
      "source": [
        "data_int=True\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('mnet_cifar10_250k.pt',map_location=torch.device('cpu')))\n",
        "if data_int :\n",
        "  mymodel.model.half()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "valid_loss,training_loss,test_accuracy=[],[],[]\n",
        "\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 1,criterion,optimizer,mymodel,scheduler=False ) \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "epoch  0\n",
            "83.97  % ,  0.4252779296875  ,  0.457237841796875\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07A_UlDfvYiy",
        "outputId": "f7e01aec-9ba4-4469-a37d-9dd751c09871"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 20,criterion,optimizer,mymodel,scheduler=False ) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "84.2  % ,  0.448318359375  ,  0.439951953125\n",
            "epoch  1\n",
            "84.3  % ,  0.4407103515625  ,  0.43315615234375\n",
            "epoch  2\n",
            "83.45  % ,  0.465434765625  ,  0.419251416015625\n",
            "epoch  3\n",
            "83.09  % ,  0.4650162109375  ,  0.42665068359375\n",
            "epoch  4\n",
            "85.1  % ,  0.425803125  ,  0.423358251953125\n",
            "epoch  5\n",
            "85.26  % ,  0.403140234375  ,  0.4287947509765625\n",
            "epoch  6\n",
            "84.68  % ,  0.4421681640625  ,  0.425832080078125\n",
            "epoch  7\n",
            "84.13  % ,  0.4629896484375  ,  0.428233642578125\n",
            "epoch  8\n",
            "83.01  % ,  0.48621552734375  ,  0.421339794921875\n",
            "epoch  9\n",
            "81.9  % ,  0.5007568359375  ,  0.425131298828125\n",
            "epoch  10\n",
            "85.07  % ,  0.4381736328125  ,  0.422677587890625\n",
            "epoch  11\n",
            "84.53  % ,  0.434758984375  ,  0.4167341796875\n",
            "epoch  12\n",
            "83.33  % ,  0.47806953125  ,  0.424689892578125\n",
            "epoch  13\n",
            "84.89  % ,  0.4280552734375  ,  0.4198611328125\n",
            "epoch  14\n",
            "85.26  % ,  0.438326953125  ,  0.422596240234375\n",
            "epoch  15\n",
            "84.57  % ,  0.4497509765625  ,  0.421431005859375\n",
            "epoch  16\n",
            "84.07  % ,  0.446935546875  ,  0.428239013671875\n",
            "epoch  17\n",
            "85.32  % ,  0.4429849609375  ,  0.421679052734375\n",
            "epoch  18\n",
            "83.45  % ,  0.4776203125  ,  0.425072412109375\n",
            "epoch  19\n",
            "84.02  % ,  0.4556654296875  ,  0.425585107421875\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBVB6EeQvYi0",
        "outputId": "7b6772af-4d51-407a-b820-a1fa6b3370db"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,10,criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy,scheduler=False ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "saving weights.... \n",
            "89.62  % ,  0.30145107421875  ,  0.30467763671875\n",
            "epoch  1\n",
            "saving weights.... \n",
            "90.03  % ,  0.285808203125  ,  0.258798828125\n",
            "epoch  2\n",
            "saving weights.... \n",
            "90.39  % ,  0.27697437744140624  ,  0.247108984375\n",
            "epoch  3\n",
            "saving weights.... \n",
            "90.03  % ,  0.27376337890625  ,  0.233851611328125\n",
            "epoch  4\n",
            "saving weights.... \n",
            "90.17  % ,  0.27175400390625  ,  0.2245684326171875\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.5  % ,  0.265438671875  ,  0.2229976806640625\n",
            "epoch  6\n",
            "saving weights.... \n",
            "90.61  % ,  0.26212060546875  ,  0.2125527099609375\n",
            "epoch  7\n",
            "90.42  % ,  0.262145263671875  ,  0.210363134765625\n",
            "epoch  8\n",
            "saving weights.... \n",
            "90.75  % ,  0.26116435546875  ,  0.20753099365234376\n",
            "epoch  9\n",
            "saving weights.... \n",
            "90.87  % ,  0.252710546875  ,  0.2019739501953125\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox_LTIHlvede",
        "outputId": "03dc7fdb-e814-4a90-d5c7-e32d15640343"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,10,criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy,scheduler=False ) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "90.72  % ,  0.25936923828125  ,  0.19746005859375\n",
            "epoch  1\n",
            "90.78  % ,  0.256189013671875  ,  0.19698758544921874\n",
            "epoch  2\n",
            "90.74  % ,  0.2542048828125  ,  0.1984523681640625\n",
            "epoch  3\n",
            "91.04  % ,  0.25916318359375  ,  0.1914765380859375\n",
            "epoch  4\n",
            "90.94  % ,  0.26081494140625  ,  0.18764398193359375\n",
            "epoch  5\n",
            "saving weights.... \n",
            "90.99  % ,  0.24860859375  ,  0.18425377197265624\n",
            "epoch  6\n",
            "90.92  % ,  0.24914287109375  ,  0.18383046875\n",
            "epoch  7\n",
            "90.93  % ,  0.252965771484375  ,  0.17895667724609374\n",
            "epoch  8\n",
            "90.99  % ,  0.2503494140625  ,  0.18241988525390626\n",
            "epoch  9\n",
            "91.05  % ,  0.2547103515625  ,  0.1785185791015625\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POimVhvJLizZ"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.0001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader,10,criterion,optimizer,mymodel,valid_loss,training_loss,test_accuracy,scheduler=False ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y7fNF0ZLi1-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IzA3s6mLi5x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfwDO0s7LqVH"
      },
      "source": [
        "### 500k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-YCkhSSvehl",
        "outputId": "4534e0cb-35f5-4e83-bc08-fbd306679624"
      },
      "source": [
        "data_int=False\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('mnet_cifar10_500k.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "\n",
        "mymodel.remove_prune()\n",
        "mesure_operation(mymodel.model,\"resnet\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "tensor([0.3333])\n",
            "Conv2d: S_c=3, F_in=3, F_out=32, P=1024, params=144, operations=431445\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.2500])\n",
            "Conv2d: S_c=1, F_in=32, F_out=32, P=1024, params=128, operations=385024\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.5000])\n",
            "Conv2d: S_c=3, F_in=32, F_out=32, P=1024, params=72, operations=7061504\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.2500])\n",
            "Conv2d: S_c=1, F_in=32, F_out=16, P=1024, params=64, operations=192512\n",
            "Batch norm: F_in=16 P=1024, params=16, operations=49152\n",
            "tensor([0.2500])\n",
            "Conv2d: S_c=1, F_in=16, F_out=96, P=1024, params=192, operations=565248\n",
            "Batch norm: F_in=96 P=1024, params=96, operations=294912\n",
            "tensor([0.5000])\n",
            "Conv2d: S_c=3, F_in=96, F_out=96, P=1024, params=216, operations=63651840\n",
            "Batch norm: F_in=96 P=1024, params=96, operations=294912\n",
            "tensor([0.2500])\n",
            "Conv2d: S_c=1, F_in=96, F_out=24, P=1024, params=288, operations=878592\n",
            "Batch norm: F_in=24 P=1024, params=24, operations=73728\n",
            "tensor([0.2535])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=438, operations=1308160\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.5062])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=328, operations=145022064\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.2535])\n",
            "Conv2d: S_c=1, F_in=144, F_out=24, P=1024, params=438, operations=1339306\n",
            "Batch norm: F_in=24 P=1024, params=24, operations=73728\n",
            "tensor([0.2535])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=438, operations=1308160\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.5062])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=328, operations=145022064\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([0.2535])\n",
            "Conv2d: S_c=1, F_in=144, F_out=32, P=1024, params=584, operations=1785742\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.2526])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=776, operations=2334208\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.5046])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=436, operations=257064048\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.2526])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=776, operations=2375594\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.2526])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=776, operations=2334208\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.5046])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=436, operations=257064048\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.2526])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=776, operations=2375594\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([0.2526])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=776, operations=2334208\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([0.5046])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=436, operations=64266012\n",
            "Batch norm: F_in=192 P=256, params=192, operations=147456\n",
            "tensor([0.2604])\n",
            "Conv2d: S_c=1, F_in=192, F_out=64, P=256, params=1600, operations=1224533\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=3201, operations=2432760\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.5052])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=873, operations=257408512\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=3201, operations=2454100\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=3201, operations=2432760\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.5052])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=873, operations=257408512\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=3201, operations=2454100\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=3201, operations=2432760\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.5052])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=873, operations=257408512\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=3201, operations=2454100\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=3201, operations=2432760\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.5052])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=873, operations=257408512\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([0.2526])\n",
            "Conv2d: S_c=1, F_in=384, F_out=96, P=256, params=4656, operations=3569600\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([0.2517])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=6960, operations=5308160\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.5035])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=1305, operations=577216000\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.2517])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=6960, operations=5339093\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([0.2517])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=6960, operations=5308160\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.5035])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=1305, operations=577216000\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.2517])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=6960, operations=5339093\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([0.2517])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=6960, operations=5308160\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([0.5035])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=1305, operations=144304000\n",
            "Batch norm: F_in=576 P=64, params=576, operations=110592\n",
            "tensor([0.2549])\n",
            "Conv2d: S_c=1, F_in=576, F_out=160, P=64, params=11745, operations=2252430\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([0.2552])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=19602, operations=3747902\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.5042])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=2178, operations=401417984\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.2552])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=19602, operations=3760970\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([0.2552])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=19602, operations=3747902\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.5042])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=2178, operations=401417984\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.2552])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=19602, operations=3760970\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([0.2552])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=19602, operations=3747902\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.5042])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=2178, operations=401417984\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([0.2552])\n",
            "Conv2d: S_c=1, F_in=960, F_out=320, P=64, params=39204, operations=7521941\n",
            "Batch norm: F_in=320 P=64, params=320, operations=61440\n",
            "tensor([0.2551])\n",
            "Conv2d: S_c=1, F_in=320, F_out=1280, P=64, params=52245, operations=10010142\n",
            "Batch norm: F_in=1280 P=64, params=1280, operations=245760\n",
            "1280\n",
            "Linear: F_in=1280, F_out=10, params=1290, operations=1932\n",
            "Flops: 4591612928.0, Params: 305835.0\n",
            "Score flops: 5.503136630431114 Score Params: 0.054740655105145336\n",
            "Final score: 5.557877285536259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNmKVsvKS5lB",
        "outputId": "892b2e3e-7ec9-4185-ed6b-ebbb837d351c"
      },
      "source": [
        "data_int=False\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('mnet_cifar10_500k.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "print(eval_accuracy(mymodel.model,c10testloader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "91.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIm5BIcbvewt",
        "outputId": "46d341a0-92aa-4a1e-a740-27e9f152be22"
      },
      "source": [
        "data_int=True\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('mnet_cifar10_500k.pt',map_location=torch.device('cpu')))\n",
        "if data_int :\n",
        "  mymodel.model.half()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "valid_loss,training_loss,test_accuracy=[],[],[]\n",
        "\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 1,criterion,optimizer,mymodel,scheduler=False ) \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "epoch  0\n",
            "83.26  % ,  0.436076953125  ,  0.428305859375\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TUDBIHWvewu",
        "outputId": "331f8a3a-782a-4932-86fe-818751105ff4"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 20,criterion,optimizer,mymodel,scheduler=False ) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "84.24  % ,  0.43582578125  ,  0.40821025390625\n",
            "epoch  1\n",
            "85.53  % ,  0.4098771484375  ,  0.402265087890625\n",
            "epoch  2\n",
            "84.76  % ,  0.4551380859375  ,  0.4013587890625\n",
            "epoch  3\n",
            "84.69  % ,  0.4414552734375  ,  0.403194482421875\n",
            "epoch  4\n",
            "85.75  % ,  0.3937130859375  ,  0.394665576171875\n",
            "epoch  5\n",
            "86.76  % ,  0.384089453125  ,  0.390714404296875\n",
            "epoch  6\n",
            "86.47  % ,  0.403701171875  ,  0.39056334228515627\n",
            "epoch  7\n",
            "85.55  % ,  0.39702724609375  ,  0.3927271484375\n",
            "epoch  8\n",
            "85.14  % ,  0.41624140625  ,  0.3889236328125\n",
            "epoch  9\n",
            "85.32  % ,  0.4236015625  ,  0.39325390625\n",
            "epoch  10\n",
            "85.23  % ,  0.427569921875  ,  0.3911814453125\n",
            "epoch  11\n",
            "85.3  % ,  0.416136328125  ,  0.387389208984375\n",
            "epoch  12\n",
            "85.17  % ,  0.4395765625  ,  0.383902490234375\n",
            "epoch  13\n",
            "85.63  % ,  0.404353515625  ,  0.383990380859375\n",
            "epoch  14\n",
            "84.39  % ,  0.4083197265625  ,  0.392638818359375\n",
            "epoch  15\n",
            "86.16  % ,  0.4046498046875  ,  0.3920455078125\n",
            "epoch  16\n",
            "81.7  % ,  0.50402265625  ,  0.390272509765625\n",
            "epoch  17\n",
            "85.03  % ,  0.42030625  ,  0.387670166015625\n",
            "epoch  18\n",
            "86.12  % ,  0.4071501953125  ,  0.38681943359375\n",
            "epoch  19\n",
            "84.1  % ,  0.44271455078125  ,  0.38002177734375\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhtJcj2vvewv",
        "outputId": "3849e14a-4d42-4427-f6b2-72dd9cfbe241"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 15,criterion,optimizer,mymodel,scheduler=False ) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "90.24  % ,  0.275444140625  ,  0.2657078125\n",
            "epoch  1\n",
            "90.75  % ,  0.2619583984375  ,  0.220584326171875\n",
            "epoch  2\n",
            "90.92  % ,  0.25004404296875  ,  0.2090855224609375\n",
            "epoch  3\n",
            "90.99  % ,  0.2489447265625  ,  0.198292431640625\n",
            "epoch  4\n",
            "saving weights.... \n",
            "91.11  % ,  0.23826337890625  ,  0.19119903564453125\n",
            "epoch  5\n",
            "91.2  % ,  0.2414296875  ,  0.18436785888671875\n",
            "epoch  6\n",
            "saving weights.... \n",
            "91.54  % ,  0.23666787109375  ,  0.18117882080078124\n",
            "epoch  7\n",
            "saving weights.... \n",
            "91.68  % ,  0.235828271484375  ,  0.172925439453125\n",
            "epoch  8\n",
            "91.52  % ,  0.2359166015625  ,  0.16814989013671874\n",
            "epoch  9\n",
            "saving weights.... \n",
            "91.6  % ,  0.23331572265625  ,  0.165990283203125\n",
            "epoch  10\n",
            "91.43  % ,  0.236915625  ,  0.16245931396484375\n",
            "epoch  11\n",
            "saving weights.... \n",
            "91.52  % ,  0.226235546875  ,  0.16195107421875\n",
            "epoch  12\n",
            "91.59  % ,  0.232803662109375  ,  0.15408988037109375\n",
            "epoch  13\n",
            "91.71  % ,  0.22919580078125  ,  0.1552059326171875\n",
            "epoch  14\n",
            "saving weights.... \n",
            "91.54  % ,  0.226234765625  ,  0.1528412109375\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4bBnPz4l9lg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XteNNo6Rl-OC"
      },
      "source": [
        "### Mobile net de base"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaMkTp3yl-OD",
        "outputId": "7c8b4962-28c9-42b6-8dad-0bb13c50b818"
      },
      "source": [
        "data_int=False\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "\n",
        "#mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('mnetKd_cifar10.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "\n",
        "#mymodel.remove_prune()\n",
        "mesure_operation(mymodel.model,\"resnet\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=3, F_out=32, P=1024, params=432, operations=1294336\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=32, F_out=32, P=1024, params=512, operations=1540096\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=32, F_out=32, P=1024, params=144, operations=14123008\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=32, F_out=16, P=1024, params=256, operations=770048\n",
            "Batch norm: F_in=16 P=1024, params=16, operations=49152\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=16, F_out=96, P=1024, params=768, operations=2260992\n",
            "Batch norm: F_in=96 P=1024, params=96, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=96, F_out=96, P=1024, params=432, operations=127303680\n",
            "Batch norm: F_in=96 P=1024, params=96, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=96, F_out=24, P=1024, params=1152, operations=3514368\n",
            "Batch norm: F_in=24 P=1024, params=24, operations=73728\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=1728, operations=5160960\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=648, operations=286507008\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=144, F_out=24, P=1024, params=1728, operations=5283840\n",
            "Batch norm: F_in=24 P=1024, params=24, operations=73728\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=1728, operations=5160960\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=648, operations=286507008\n",
            "Batch norm: F_in=144 P=1024, params=144, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=144, F_out=32, P=1024, params=2304, operations=7045120\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=3072, operations=9240576\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=864, operations=509411328\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=3072, operations=9404416\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=3072, operations=9240576\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=864, operations=509411328\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=3072, operations=9404416\n",
            "Batch norm: F_in=32 P=1024, params=32, operations=98304\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=3072, operations=9240576\n",
            "Batch norm: F_in=192 P=1024, params=192, operations=589824\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=864, operations=127352832\n",
            "Batch norm: F_in=192 P=256, params=192, operations=147456\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=192, F_out=64, P=256, params=6144, operations=4702208\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=12288, operations=9338880\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=1728, operations=509509632\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=12288, operations=9420800\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=12288, operations=9338880\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=1728, operations=509509632\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=12288, operations=9420800\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=12288, operations=9338880\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=1728, operations=509509632\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=12288, operations=9420800\n",
            "Batch norm: F_in=64 P=256, params=64, operations=49152\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=12288, operations=9338880\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=1728, operations=509509632\n",
            "Batch norm: F_in=384 P=256, params=384, operations=294912\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=384, F_out=96, P=256, params=18432, operations=14131200\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=27648, operations=21086208\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=2592, operations=1146470400\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=27648, operations=21209088\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=27648, operations=21086208\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=2592, operations=1146470400\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=27648, operations=21209088\n",
            "Batch norm: F_in=96 P=256, params=96, operations=73728\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=27648, operations=21086208\n",
            "Batch norm: F_in=576 P=256, params=576, operations=442368\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=2592, operations=286617600\n",
            "Batch norm: F_in=576 P=64, params=576, operations=110592\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=576, F_out=160, P=64, params=46080, operations=8837120\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=76800, operations=14684160\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=4320, operations=796200960\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=76800, operations=14735360\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=76800, operations=14684160\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=4320, operations=796200960\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=76800, operations=14735360\n",
            "Batch norm: F_in=160 P=64, params=160, operations=30720\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=76800, operations=14684160\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([1.])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=4320, operations=796200960\n",
            "Batch norm: F_in=960 P=64, params=960, operations=184320\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=960, F_out=320, P=64, params=153600, operations=29470720\n",
            "Batch norm: F_in=320 P=64, params=320, operations=61440\n",
            "tensor([1.])\n",
            "Conv2d: S_c=1, F_in=320, F_out=1280, P=64, params=204800, operations=39239680\n",
            "Batch norm: F_in=1280 P=64, params=1280, operations=245760\n",
            "1280\n",
            "Linear: F_in=1280, F_out=10, params=12810, operations=19190\n",
            "Flops: 9299442688.0, Params: 1125290.0\n",
            "Score flops: 11.145561374925979 Score Params: 0.20141289186413916\n",
            "Final score: 11.346974266790118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boKI9FFPl-OE",
        "outputId": "fbc5c6c8-7667-4880-b21f-4e30c2620858"
      },
      "source": [
        "data_int=True\n",
        "net = MobileNetV2(10, alpha=1)\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('mnet_cifar10_500k.pt',map_location=torch.device('cpu')))\n",
        "if data_int :\n",
        "  mymodel.model.half()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "valid_loss,training_loss,test_accuracy=[],[],[]\n",
        "\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 1,criterion,optimizer,mymodel,scheduler=False ) \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "epoch  0\n",
            "saving weights.... \n",
            "83.6  % ,  0.42083984375  ,  0.4364951171875\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HliQSG9l-OE",
        "outputId": "2f187187-a5bd-4b2e-f4e2-b9a9dd610326"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 20,criterion,optimizer,mymodel,scheduler=False ) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "84.48  % ,  0.423961328125  ,  0.418491845703125\n",
            "epoch  1\n",
            "saving weights.... \n",
            "84.62  % ,  0.41185703125  ,  0.403348876953125\n",
            "epoch  2\n",
            "83.78  % ,  0.431893359375  ,  0.398962255859375\n",
            "epoch  3\n",
            "saving weights.... \n",
            "84.75  % ,  0.3962326171875  ,  0.39889296875\n",
            "epoch  4\n",
            "saving weights.... \n",
            "86.15  % ,  0.3813916015625  ,  0.392094384765625\n",
            "epoch  5\n",
            "saving weights.... \n",
            "85.82  % ,  0.3733484375  ,  0.392160302734375\n",
            "epoch  6\n",
            "84.31  % ,  0.4355515625  ,  0.394030419921875\n",
            "epoch  7\n",
            "85.21  % ,  0.4021943359375  ,  0.388679736328125\n",
            "epoch  8\n",
            "84.54  % ,  0.4140033203125  ,  0.40240595703125\n",
            "epoch  9\n",
            "85.74  % ,  0.3930517578125  ,  0.396212109375\n",
            "epoch  10\n",
            "83.48  % ,  0.46876796875  ,  0.400312744140625\n",
            "epoch  11\n",
            "85.83  % ,  0.384796484375  ,  0.3906533203125\n",
            "epoch  12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUFsjv1zl-OF",
        "outputId": "3849e14a-4d42-4427-f6b2-72dd9cfbe241"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 15,criterion,optimizer,mymodel,scheduler=False ) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "90.24  % ,  0.275444140625  ,  0.2657078125\n",
            "epoch  1\n",
            "90.75  % ,  0.2619583984375  ,  0.220584326171875\n",
            "epoch  2\n",
            "90.92  % ,  0.25004404296875  ,  0.2090855224609375\n",
            "epoch  3\n",
            "90.99  % ,  0.2489447265625  ,  0.198292431640625\n",
            "epoch  4\n",
            "saving weights.... \n",
            "91.11  % ,  0.23826337890625  ,  0.19119903564453125\n",
            "epoch  5\n",
            "91.2  % ,  0.2414296875  ,  0.18436785888671875\n",
            "epoch  6\n",
            "saving weights.... \n",
            "91.54  % ,  0.23666787109375  ,  0.18117882080078124\n",
            "epoch  7\n",
            "saving weights.... \n",
            "91.68  % ,  0.235828271484375  ,  0.172925439453125\n",
            "epoch  8\n",
            "91.52  % ,  0.2359166015625  ,  0.16814989013671874\n",
            "epoch  9\n",
            "saving weights.... \n",
            "91.6  % ,  0.23331572265625  ,  0.165990283203125\n",
            "epoch  10\n",
            "91.43  % ,  0.236915625  ,  0.16245931396484375\n",
            "epoch  11\n",
            "saving weights.... \n",
            "91.52  % ,  0.226235546875  ,  0.16195107421875\n",
            "epoch  12\n",
            "91.59  % ,  0.232803662109375  ,  0.15408988037109375\n",
            "epoch  13\n",
            "91.71  % ,  0.22919580078125  ,  0.1552059326171875\n",
            "epoch  14\n",
            "saving weights.... \n",
            "91.54  % ,  0.226234765625  ,  0.1528412109375\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcXvv4GdmDqk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFe3zDQ3mEDA"
      },
      "source": [
        "### VGG pruned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I58llVYXmEDA",
        "outputId": "d76af65c-c00f-4fe7-faa5-42bce204eca0"
      },
      "source": [
        "data_int=False\n",
        "net = VGG('VGG16')\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('checkpoint_prdim2_temp.pt',map_location=torch.device('cpu')))\n",
        "\n",
        "\n",
        "mymodel.remove_prune()\n",
        "mesure_operation(mymodel.model,\"resnet\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "tensor([0.3333])\n",
            "Conv2d: S_c=3, F_in=3, F_out=32, P=1024, params=288, operations=578901\n",
            "Batch norm: F_in=32 P=1024, params=64, operations=98304\n",
            "tensor([0.2500])\n",
            "Conv2d: S_c=1, F_in=32, F_out=32, P=1024, params=256, operations=516096\n",
            "Batch norm: F_in=32 P=1024, params=64, operations=98304\n",
            "tensor([0.5000])\n",
            "Conv2d: S_c=3, F_in=32, F_out=32, P=1024, params=144, operations=9420800\n",
            "Batch norm: F_in=32 P=1024, params=64, operations=98304\n",
            "tensor([0.2500])\n",
            "Conv2d: S_c=1, F_in=32, F_out=16, P=1024, params=128, operations=258048\n",
            "Batch norm: F_in=16 P=1024, params=32, operations=49152\n",
            "tensor([0.2500])\n",
            "Conv2d: S_c=1, F_in=16, F_out=96, P=1024, params=384, operations=761856\n",
            "Batch norm: F_in=96 P=1024, params=192, operations=294912\n",
            "tensor([0.5000])\n",
            "Conv2d: S_c=3, F_in=96, F_out=96, P=1024, params=432, operations=84885504\n",
            "Batch norm: F_in=96 P=1024, params=192, operations=294912\n",
            "tensor([0.2500])\n",
            "Conv2d: S_c=1, F_in=96, F_out=24, P=1024, params=576, operations=1173504\n",
            "Batch norm: F_in=24 P=1024, params=48, operations=73728\n",
            "tensor([0.2535])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=876, operations=1756672\n",
            "Batch norm: F_in=144 P=1024, params=288, operations=442368\n",
            "tensor([0.5069])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=657, operations=193682448\n",
            "Batch norm: F_in=144 P=1024, params=288, operations=442368\n",
            "tensor([0.2535])\n",
            "Conv2d: S_c=1, F_in=144, F_out=24, P=1024, params=876, operations=1787818\n",
            "Batch norm: F_in=24 P=1024, params=48, operations=73728\n",
            "tensor([0.2535])\n",
            "Conv2d: S_c=1, F_in=24, F_out=144, P=1024, params=876, operations=1756672\n",
            "Batch norm: F_in=144 P=1024, params=288, operations=442368\n",
            "tensor([0.5069])\n",
            "Conv2d: S_c=3, F_in=144, F_out=144, P=1024, params=657, operations=193682448\n",
            "Batch norm: F_in=144 P=1024, params=288, operations=442368\n",
            "tensor([0.2535])\n",
            "Conv2d: S_c=1, F_in=144, F_out=32, P=1024, params=1168, operations=2383758\n",
            "Batch norm: F_in=32 P=1024, params=64, operations=98304\n",
            "tensor([0.2526])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=1552, operations=3128832\n",
            "Batch norm: F_in=192 P=1024, params=384, operations=589824\n",
            "tensor([0.5052])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=873, operations=343178240\n",
            "Batch norm: F_in=192 P=1024, params=384, operations=589824\n",
            "tensor([0.2526])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=1552, operations=3170219\n",
            "Batch norm: F_in=32 P=1024, params=64, operations=98304\n",
            "tensor([0.2526])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=1552, operations=3128832\n",
            "Batch norm: F_in=192 P=1024, params=384, operations=589824\n",
            "tensor([0.5052])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=873, operations=343178240\n",
            "Batch norm: F_in=192 P=1024, params=384, operations=589824\n",
            "tensor([0.2526])\n",
            "Conv2d: S_c=1, F_in=192, F_out=32, P=1024, params=1552, operations=3170219\n",
            "Batch norm: F_in=32 P=1024, params=64, operations=98304\n",
            "tensor([0.2526])\n",
            "Conv2d: S_c=1, F_in=32, F_out=192, P=1024, params=1552, operations=3128832\n",
            "Batch norm: F_in=192 P=1024, params=384, operations=589824\n",
            "tensor([0.5052])\n",
            "Conv2d: S_c=3, F_in=192, F_out=192, P=1024, params=873, operations=85794560\n",
            "Batch norm: F_in=192 P=256, params=384, operations=147456\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=192, F_out=64, P=256, params=3201, operations=1634644\n",
            "Batch norm: F_in=64 P=256, params=128, operations=49152\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=6402, operations=3252216\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([0.5052])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=1746, operations=343227904\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=6402, operations=3273556\n",
            "Batch norm: F_in=64 P=256, params=128, operations=49152\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=6402, operations=3252216\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([0.5052])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=1746, operations=343227904\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=6402, operations=3273556\n",
            "Batch norm: F_in=64 P=256, params=128, operations=49152\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=6402, operations=3252216\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([0.5052])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=1746, operations=343227904\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=384, F_out=64, P=256, params=6402, operations=3273556\n",
            "Batch norm: F_in=64 P=256, params=128, operations=49152\n",
            "tensor([0.2605])\n",
            "Conv2d: S_c=1, F_in=64, F_out=384, P=256, params=6402, operations=3252216\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([0.5052])\n",
            "Conv2d: S_c=3, F_in=384, F_out=384, P=256, params=1746, operations=343227904\n",
            "Batch norm: F_in=384 P=256, params=768, operations=294912\n",
            "tensor([0.2526])\n",
            "Conv2d: S_c=1, F_in=384, F_out=96, P=256, params=9312, operations=4761536\n",
            "Batch norm: F_in=96 P=256, params=192, operations=73728\n",
            "tensor([0.2517])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=13920, operations=7089920\n",
            "Batch norm: F_in=576 P=256, params=1152, operations=442368\n",
            "tensor([0.5035])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=2610, operations=769646080\n",
            "Batch norm: F_in=576 P=256, params=1152, operations=442368\n",
            "tensor([0.2517])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=13920, operations=7120853\n",
            "Batch norm: F_in=96 P=256, params=192, operations=73728\n",
            "tensor([0.2517])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=13920, operations=7089920\n",
            "Batch norm: F_in=576 P=256, params=1152, operations=442368\n",
            "tensor([0.5035])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=2610, operations=769646080\n",
            "Batch norm: F_in=576 P=256, params=1152, operations=442368\n",
            "tensor([0.2517])\n",
            "Conv2d: S_c=1, F_in=576, F_out=96, P=256, params=13920, operations=7120853\n",
            "Batch norm: F_in=96 P=256, params=192, operations=73728\n",
            "tensor([0.2517])\n",
            "Conv2d: S_c=1, F_in=96, F_out=576, P=256, params=13920, operations=7089920\n",
            "Batch norm: F_in=576 P=256, params=1152, operations=442368\n",
            "tensor([0.5035])\n",
            "Conv2d: S_c=3, F_in=576, F_out=576, P=256, params=2610, operations=192411520\n",
            "Batch norm: F_in=576 P=64, params=1152, operations=110592\n",
            "tensor([0.2549])\n",
            "Conv2d: S_c=1, F_in=576, F_out=160, P=64, params=23490, operations=3004110\n",
            "Batch norm: F_in=160 P=64, params=320, operations=30720\n",
            "tensor([0.2552])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=39204, operations=5002430\n",
            "Batch norm: F_in=960 P=64, params=1920, operations=184320\n",
            "tensor([0.5042])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=4356, operations=535234304\n",
            "Batch norm: F_in=960 P=64, params=1920, operations=184320\n",
            "tensor([0.2552])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=39204, operations=5015498\n",
            "Batch norm: F_in=160 P=64, params=320, operations=30720\n",
            "tensor([0.2552])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=39204, operations=5002430\n",
            "Batch norm: F_in=960 P=64, params=1920, operations=184320\n",
            "tensor([0.5042])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=4356, operations=535234304\n",
            "Batch norm: F_in=960 P=64, params=1920, operations=184320\n",
            "tensor([0.2552])\n",
            "Conv2d: S_c=1, F_in=960, F_out=160, P=64, params=39204, operations=5015498\n",
            "Batch norm: F_in=160 P=64, params=320, operations=30720\n",
            "tensor([0.2552])\n",
            "Conv2d: S_c=1, F_in=160, F_out=960, P=64, params=39204, operations=5002430\n",
            "Batch norm: F_in=960 P=64, params=1920, operations=184320\n",
            "tensor([0.5042])\n",
            "Conv2d: S_c=3, F_in=960, F_out=960, P=64, params=4356, operations=535234304\n",
            "Batch norm: F_in=960 P=64, params=1920, operations=184320\n",
            "tensor([0.2552])\n",
            "Conv2d: S_c=1, F_in=960, F_out=320, P=64, params=78408, operations=10030997\n",
            "Batch norm: F_in=320 P=64, params=640, operations=61440\n",
            "tensor([0.2551])\n",
            "Conv2d: S_c=1, F_in=320, F_out=1280, P=64, params=104490, operations=13353822\n",
            "Batch norm: F_in=1280 P=64, params=2560, operations=245760\n",
            "1280\n",
            "Linear: F_in=1280, F_out=10, params=2580, operations=3864\n",
            "Flops: 6119856128.0, Params: 611670.0\n",
            "Score flops: 7.334765573463671 Score Params: 0.10948131021029067\n",
            "Final score: 7.444246883673962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKMZjAbJmEDB",
        "outputId": "4f00a8e0-d68c-4c76-9ccc-84f7265b98f9"
      },
      "source": [
        "data_int=True\n",
        "net = VGG('VGG16')\n",
        "\n",
        "\n",
        "mymodel = my_network_with_trous(net)\n",
        "mymodel.model = mymodel.model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "mymodel.prune_all_layers({\"fc\":0 , \"conv\":0,\"dim\":0})\n",
        "mymodel.model.load_state_dict(torch.load('checkpoint_prdim2_temp.pt',map_location=torch.device('cpu')))\n",
        "if data_int :\n",
        "  mymodel.model.half()\n",
        "\n",
        "net.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "\n",
        "valid_loss,training_loss,test_accuracy=[],[],[]\n",
        "\n",
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 1,criterion,optimizer,mymodel,scheduler=False ) \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pruning....\n",
            "epoch  0\n",
            "saving weights.... \n",
            "84.49  % ,  0.40348359375  ,  0.425525048828125\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmg9mzCRmEDB",
        "outputId": "2668c867-bf0c-4e3d-91a1-8f979c9015b2"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.01, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 20,criterion,optimizer,mymodel,scheduler=False ) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "saving weights.... \n",
            "86.09  % ,  0.38214765625  ,  0.411821240234375\n",
            "epoch  1\n",
            "85.53  % ,  0.4088943359375  ,  0.3993401123046875\n",
            "epoch  2\n",
            "85.98  % ,  0.396128515625  ,  0.397957568359375\n",
            "epoch  3\n",
            "85.38  % ,  0.407944140625  ,  0.39186533203125\n",
            "epoch  4\n",
            "85.54  % ,  0.4048671875  ,  0.393453662109375\n",
            "epoch  5\n",
            "83.71  % ,  0.45586484375  ,  0.392194091796875\n",
            "epoch  6\n",
            "84.03  % ,  0.4460841796875  ,  0.39105625\n",
            "epoch  7\n",
            "86.11  % ,  0.4093078125  ,  0.382208203125\n",
            "epoch  8\n",
            "saving weights.... \n",
            "86.69  % ,  0.369654296875  ,  0.3827322265625\n",
            "epoch  9\n",
            "84.97  % ,  0.4204810546875  ,  0.38502177734375\n",
            "epoch  10\n",
            "85.93  % ,  0.394250390625  ,  0.393476611328125\n",
            "epoch  11\n",
            "86.42  % ,  0.396379296875  ,  0.3942763671875\n",
            "epoch  12\n",
            "86.47  % ,  0.3870154296875  ,  0.392476171875\n",
            "epoch  13\n",
            "85.22  % ,  0.4254609375  ,  0.39171064453125\n",
            "epoch  14\n",
            "83.97  % ,  0.4733318359375  ,  0.39555400390625\n",
            "epoch  15\n",
            "84.09  % ,  0.434648828125  ,  0.39032041015625\n",
            "epoch  16\n",
            "82.61  % ,  0.50474375  ,  0.391720556640625\n",
            "epoch  17\n",
            "86.21  % ,  0.37662353515625  ,  0.39499951171875\n",
            "epoch  18\n",
            "85.04  % ,  0.434215625  ,  0.3869185546875\n",
            "epoch  19\n",
            "84.69  % ,  0.4477875  ,  0.3858048828125\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE1i-15imEDC",
        "outputId": "3849e14a-4d42-4427-f6b2-72dd9cfbe241"
      },
      "source": [
        "optimizer = optim.SGD(mymodel.model.parameters(), lr=0.001, momentum=0.9,weight_decay=5e-4)\n",
        "valid_loss,training_loss,test_accuracy = trainingwithPrunning(c10trainloader,c10validloader,c10testloader, 15,criterion,optimizer,mymodel,scheduler=False ) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "90.24  % ,  0.275444140625  ,  0.2657078125\n",
            "epoch  1\n",
            "90.75  % ,  0.2619583984375  ,  0.220584326171875\n",
            "epoch  2\n",
            "90.92  % ,  0.25004404296875  ,  0.2090855224609375\n",
            "epoch  3\n",
            "90.99  % ,  0.2489447265625  ,  0.198292431640625\n",
            "epoch  4\n",
            "saving weights.... \n",
            "91.11  % ,  0.23826337890625  ,  0.19119903564453125\n",
            "epoch  5\n",
            "91.2  % ,  0.2414296875  ,  0.18436785888671875\n",
            "epoch  6\n",
            "saving weights.... \n",
            "91.54  % ,  0.23666787109375  ,  0.18117882080078124\n",
            "epoch  7\n",
            "saving weights.... \n",
            "91.68  % ,  0.235828271484375  ,  0.172925439453125\n",
            "epoch  8\n",
            "91.52  % ,  0.2359166015625  ,  0.16814989013671874\n",
            "epoch  9\n",
            "saving weights.... \n",
            "91.6  % ,  0.23331572265625  ,  0.165990283203125\n",
            "epoch  10\n",
            "91.43  % ,  0.236915625  ,  0.16245931396484375\n",
            "epoch  11\n",
            "saving weights.... \n",
            "91.52  % ,  0.226235546875  ,  0.16195107421875\n",
            "epoch  12\n",
            "91.59  % ,  0.232803662109375  ,  0.15408988037109375\n",
            "epoch  13\n",
            "91.71  % ,  0.22919580078125  ,  0.1552059326171875\n",
            "epoch  14\n",
            "saving weights.... \n",
            "91.54  % ,  0.226234765625  ,  0.1528412109375\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}